[{"title":"Apollo安装配置","url":"/20241121/CentOS/22e869879d7c/","content":"Apollo安装配置1.Apollo简介Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 服务端基于Spring Boot和Spring Cloud开发，打包后可以直接运行，不需要额外安装Tomcat等应用容器。 Java客户端不依赖任何框架，能够运行于所有Java运行时环境，同时对Spring/Spring Boot环境也有较好的支持。 .Net客户端不依赖任何框架，能够运行于所有.Net运行时环境。 详情参考https://github.com/ctripcorp/apollo 根据官方描述apollo可以用在：开发环境（dev）、生产环境（pro）、以及测试和集成环境（fat和uat）\n\n在部署方式上：\n\n其实这些环境在部署方面都是一样的，都分别部署了离线包中的apollo-adminservice和apollo-configservice然后再找一台服务器部署apollo-portal来管理这些环境同时根据官方描述同一个环境（例如PRO）可以进行高可用部署，也就是在多个服务器上部署多个副本\n2.环境准备\n本次部署的架构\n\n 为了适应未来项目架构，我们需要部署两个Apollo配置中心，其中Apollo1提供读写服务，Apollo2仅提供读服务；通过Mysql的主从模式，中心2的Mysql会一直同步中心1的Mysql数据，作为中心1的备份 \n\n部署规划如下：\n\n\nserver1 apollo1 + mysql主\n\nserver2 apollo2 + mysql从\n\n\n3.开始部署3.1  jdk 1.8离线部署查询本服务器预装的openjdk版本\nrpm -qa |grep jdk\n卸载openjdk\nrpm -e --nodeps java-11-openjdk-11.0.ea.28-7.el7.x86_64rpm -e --nodeps java-11-openjdk-headless-11.0.ea.28-7.el7.x86_64\n查看java版本报错，已经卸载成功\njava -version\n下载jdk-8u161-linux-x64.tar.gz，放到/java目录下（如果没有请新建）解压缩jdk-8u161-linux-x64.tar.gz\ncd  /javatar -zxvf jdk-8u161-linux-x64.tar.gz\n编辑/etc/profile\nvim  /etc/profileJAVA_HOME=/java/jdk1.8.0_161PATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOMEexport PATHexport CLASSPATH\n重新加载环境变量\nsource  /etc/profilejava -version\n\n3.2  jdk 1.8在线部署查看yum库中jdk的版本,选择java-1.8.0安装\nyum search java|grep jdkyum install java-1.8.0-openjdk* -y\n配置环境变量\nvi /etc/profile# set java environment# 根据自己的路径设置(带bin目录的)JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH# 环境变量生效source /etc/profilejava -version\n\n3.3  使用apollo用户创建数据库#注意：由于架构图中的两个Mysql已经设置为主从模式，下面数据库操作仅在主数据库上进行，从数据库会自动同步 #下载数据库sql文件压缩包Apollo_db_sql.zip，并解压 #执行2个.sql文件 #给apollo用户授权ApolloConfigDB、ApolloPortalDB两个库的使用权限\nCREATE USER &#x27;apollo&#x27;@&#x27;%&#x27; IDENTIFIED WITH &#x27;mysql_native_password&#x27; BY &#x27;Apollo@147258&#x27;;# 给apollo用户授权ApolloConfigDB、ApolloPortalDB两个库的使用权限use ApolloConfigDB;grant ALL PRIVILEGES on ApolloConfigDB.* to &#x27;apollo&#x27;;FLUSH PRIVILEGES;use ApolloPortalDB;grant ALL PRIVILEGES on ApolloPortalDB.* to &#x27;apollo&#x27;;FLUSH PRIVILEGES;\n\n3.4  配置Apollo解压三个zip文件到/data/apollo\nunzip  apollo-adminservice-1.6.2-github.zip  -d /data/apollo/apollo-adminserviceunzip  apollo-configservice-1.6.2-github.zip  -d /data/apollo/apollo-configserviceunzip  apollo-portal-1.6.2-github.zip  -d /data/apollo/apollo-portal\n\n3.4.1  部署adminservice修改application-github.properties中的mysql信息后启动\nvi apollo-adminservice/config/application-github.propertiesspring.datasource.url = jdbc:mysql://192.168.150.11:3306/ApolloConfigDB?useSSL=false&amp;characterEncoding=utf8spring.datasource.username = apollospring.datasource.password = Apollo@147258\n在apollo-adminservice/scripts中执行starup.sh启动服务\ncd /data/apollo/apollo-adminservice/scripts./startup.sh\n\n\n更改日志文件位置(可不更改)\nvi /data/apollo/apollo-adminservice/apollo-adminservice.confMODE=servicePID_FOLDER=.LOG_FOLDER=/data/apollo/logs/100003172/\n\n3.4.2   部署configservice修改application-github.properties中的mysql信息后启动\nvi /data/apollo/apollo-configservice/config/application-github.propertiesspring.datasource.url = jdbc:mysql://192.168.150.11:3306/ApolloConfigDB?useSSL=false&amp;characterEncoding=utf8spring.datasource.username = apollospring.datasource.password = Apollo@147258\n在apollo-configservice/scripts中执行starup.sh启动服务\ncd /data/apollo/apollo-configservice/scripts./startup.sh\n\n\n更改日志文件位置(可不更改)\nvi /data/apollo/apollo-configservice/apollo-configservice.confMODE=servicePID_FOLDER=.LOG_FOLDER=/data/apollo/logs/100003171/\n\n3.4.3   部署portal修改apollo-env.properties更改环境信息\nvi /data/apollo/apollo-portal/config/apollo-env.properties#local.meta=http://localhost:8080dev.meta=http://192.168.150.11:8080#fat.meta=http://fill-in-fat-meta-server:8080#uat.meta=http://fill-in-uat-meta-server:8080#lpt.meta=$&#123;lpt_meta&#125;#pro.meta=http://fill-in-pro-meta-server:8080\n\n\n修改application-github.properties中的mysql信息后启动\nDataSource\nspring.datasource.url = jdbc:mysql://192.168.150.11:3306/ApolloPortalDB?characterEncoding=utf8spring.datasource.username = apollospring.datasource.password = Apollo@147258\n在apollo-portal/scripts中执行starup.sh启动服务\ncd /data/apollo/apollo-portal/scripts./startup.sh\n\n\n更改日志文件位置(可不更改)\nvi /data/apollo/apollo-portal/apollo-configservice.confMODE=servicePID_FOLDER=.LOG_FOLDER=/data/apollo/logs/100003173/\n\n4.  编辑Apollo服务并开机自启4.1  编辑apollo-config.servicecat &gt;/usr/lib/systemd/system/apollo-config.service &lt;&lt;EOF[Unit]Description=Apollo ServiceAfter=network.target[Service]Type=simpleEnvironment=JAVA_HOME=/data/java/jdk1.8.0_161PIDFile=/data/apollo/apollo-configservice/apollo-configservice_dataapolloapollo-configservice.pidExecStart=/data/apollo/apollo-configservice/scripts/startup.shExecStop=/data/apollo/apollo-configservice/scripts/shutdown.shRestart=no[Install]WantedBy=multi-user.targetEOF\n\n4.2  编辑apollo-admin.servicecat &gt;/usr/lib/systemd/system/apollo-admin.service &lt;&lt;EOF[Unit]Description=Apollo ServiceAfter=network.target[Service]Type=simpleEnvironment=JAVA_HOME=/data/java/jdk1.8.0_161PIDFile=/data/apollo/apollo-adminservice/apollo-adminservice_dataapolloapollo-adminservice.pidExecStart=/data/apollo/apollo-adminservice/scripts/startup.shExecStop=/data/apollo/apollo-adminservice/scripts/shutdown.shRestart=no[Install]WantedBy=multi-user.targetEOF\n\n4.3  编辑apollo-portal.servicecat &gt;/usr/lib/systemd/system/apollo-portal.service &lt;&lt;EOF[Unit]Description=Apollo ServiceAfter=network.target[Service]Type=simpleEnvironment=JAVA_HOME=/data/java/jdk1.8.0_161PIDFile=/data/apollo/apollo-portal/apollo-portal_dataapolloapollo-portal.pidExecStart=/data/apollo/apollo-portal/scripts/startup.shExecStop=/data/apollo/apollo-portal/scripts/shutdown.shRestart=no[Install]WantedBy=multi-user.targetEOF\n\n4.4  设置开机自启# 启动systemctl start apollo-config.servicesystemctl start apollo-admin.servicesystemctl start apollo-portal.service\n# 停止systemctl stop apollo-config.servicesystemctl stop apollo-admin.servicesystemctl stop apollo-portal.service\n# 开机自启systemctl enable apollo-config.servicesystemctl enable apollo-admin.servicesystemctl enable apollo-portal.service\n\n5. 验证打开浏览器，输入前台界面的登陆地址http://192.168.6.60:8070，注意前台登陆的端口默认是8070默认账号apollo 密码 admin可以尝试创建一个项目\n\n与USE_START_DAEMON相关的错误在apollo-adminservice.conf开头加上一行USE_START_DAEMON&#x3D;false就好了\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CDH6部署文档","url":"/20241121/Hadoop/843aa697d2b2/","content":"CDH6部署文档1.虚拟机准备\n 设置hadoop101、hadoop102、hadoop103的主机对应内存分别是：8G、8G、8G (看条件分配) \n\n\n设置主机名并添加到&#x2F;etc&#x2F;hosts文件内\n\nhostnamectl set-hostname hadoop1hostnamectl set-hostname hadoop2hostnamectl set-hostname hadoop3\n\n\n2.SSH免密登录\n1.生成公钥和私钥\n\nssh-keygen -t rsa\n\n\n然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）*\n\n\n2.将公钥拷贝到要免密登录的目标机器上\n\nssh-copy-id hadoop1ssh-copy-id hadoop2ssh-copy-id hadoop3\n\n\n3.重复1和2的操作，配置hadoop1、hadoop2、hadoop3三台服务器相互免密登录\n\n3.关闭防火墙\n1.查看防火墙状态\n\nsystemctl status firewalld\n\n\n2.关闭防火墙\n\nsystemctl stop firewalldsystemctl disable firewalld\n\n4.安装JDK(重要)\n1.导入rpm包安装\n\n\n所有服务器都安装\n\n# 安装jdkrpm -ivh oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm # 添加环境变量echo &#x27;# jdk&#x27; &gt;&gt; /etc/profileecho &#x27;export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera&#x27; &gt;&gt; /etc/profileecho &#x27;export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib&#x27; &gt;&gt; /etc/profileecho &#x27;export PATH=$PATH:$JAVA_HOME/bin&#x27; &gt;&gt; /etc/profile# 加载环境变量source /etc/profilejava -version\n\n\n5.安装MySQL\n1.查看MySQL是否安装\n\nrpm -qa|grep mysql# mysql-libs-5.1.73-7.el6.x86_64\n\n\n2.若有安装将其卸载\n\nrpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64\n\n\n3.删除原有MySql依赖(也适用安装出错干掉原来的)\n\nyum remove mysql-libs\n\n\n4.安装mysql初始化数据\n\n\n上传rpm包并安装（8.0或者5.7）\n\nls# mysql-community-client-8.0.22-1.el7.x86_64.rpm# mysql-community-client-plugins-8.0.22-1.el7.x86_64.rpm# mysql-community-common-8.0.22-1.el7.x86_64.rpm# mysql-community-libs-8.0.22-1.el7.x86_64.rpm# mysql-community-libs-compat-8.0.22-1.el7.x86_64.rpm# mysql-community-server-8.0.22-1.el7.x86_64.rpmyum localinstall *.rpm -y\n\n\n&#x2F;etc&#x2F;my.cnf样例\n\n[mysqld]datadir=/var/lib/mysql/data/socket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:#lower_case_table_names=1symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64#query_cache_limit = 8M#query_cache_size = 64M#query_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace &#x27;/var/lib/mysql/mysql_binary_log&#x27; with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/binlog/mysql-bin#In later versions of MySQL, if you enable the binary log and do not set#a server_id, MySQL will not start. The server_id must be unique within#the replicating group.server_id=11binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit  = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]prompt=mysql5729_db01 [\\\\d]&gt;sql_mode=STRICT_ALL_TABLESsocket=/var/lib/mysql/mysql.sock\n\n\n初始化mysql,初始化完成后注意查看密码\n\nmysqld --defaults-file=/etc/my.cnf --initialize# 设置忽略大小写,mysql8.0需要在初始化时设置（安装cdh不需要忽略）mysqld --defaults-file=/etc/my.cnf --initialize --lower-case-table-names=1\n\n\n初始化后修改密码(以下方式选用一个)\n\nset password=&#x27;Root@123&#x27;;FLUSH PRIVILEGES;# 或者alter user &#x27;root&#x27;@&#x27;%&#x27; identified with mysql_native_password by &#x27;密码&#x27;;UPDATE user SET Password=PASSWORD(&#x27;root@123&#x27;) where USER=&#x27;root&#x27;;-- 8.0版本直接mysql -uroot -p连接UPDATE user SET authentication_string=PASSWORD(&#x27;root@123&#x27;) where USER=&#x27;root&#x27;;alter user &#x27;boer&#x27;@&#x27;%&#x27;  IDENTIFIED BY &#x27;Boer@123&#x27;;  alter user &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;Root@123&#x27;; FLUSH PRIVILEGES;\n\n\n更改权限(以下方式选用一个)\n\n-- Mysql默认不允许远程登录，所以需要开启远程访问权限update user set host = &#x27;%&#x27; where user = &#x27;root&#x27;;\tFLUSH PRIVILEGES;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Root@123&#x27; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; WITH GRANT OPTION; GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;localhost&#x27; WITH GRANT OPTION; FLUSH PRIVILEGES;\n\n\n主从设置(可选)\n\n略过\n\n\n5.创建相关数据库\n\n\n创建库语句\n\nmysql5.7\n-- mysql5.7-- scmCREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON scm.* TO &#x27;scm&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Scm@147258&#x27;;-- amonCREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON amon.* TO &#x27;amon&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Amon@147258&#x27;;-- rmanCREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON rman.* TO &#x27;rman&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Rman@147258&#x27;;-- hueCREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON hue.* TO &#x27;hue&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Hue@147258&#x27;;-- hiveCREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO &#x27;hive&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Hive@147258&#x27;;-- sentryCREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;   GRANT ALL ON sentry.* TO &#x27;sentry&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Sentry@147258&#x27;;-- navCREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;      GRANT ALL ON nav.* TO &#x27;nav&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Nav@147258&#x27;;-- navmsCREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON navms.* TO &#x27;navms&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Navms@147258&#x27;;-- oozieCREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON oozie.* TO &#x27;oozie&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Oozie@147258&#x27;;-- flushFLUSH PRIVILEGES;\n\nmysql8.0\n-- mysql8.0-- scmCREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;scm&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Scm@147258&#x27;;grant all privileges on scm.*  to &#x27;scm&#x27;@&#x27;%&#x27; ;-- amonCREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;amon&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Amon@147258&#x27;;grant all privileges on amon.*  to &#x27;amon&#x27;@&#x27;%&#x27; ;-- rmanCREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;rman&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Rman@147258&#x27;;grant all privileges on rman.*  to &#x27;rman&#x27;@&#x27;%&#x27; ;-- hueCREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE USER &#x27;hue&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Hue@147258&#x27;;grant all privileges on hue.*  to &#x27;hue&#x27;@&#x27;%&#x27; ;-- hiveCREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;hive&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Hive@147258&#x27;;grant all privileges on metastore.*  to &#x27;hive&#x27;@&#x27;%&#x27; ;-- sentryCREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;sentry&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Sentry@147258&#x27;;grant all privileges on sentry.*  to &#x27;sentry&#x27;@&#x27;%&#x27; ;   -- navCREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; CREATE USER &#x27;nav&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Nav@147258&#x27;;grant all privileges on nav.*  to &#x27;nav&#x27;@&#x27;%&#x27; ;     -- navmsCREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;navms&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Navms@147258&#x27;;grant all privileges on navms.*  to &#x27;navms&#x27;@&#x27;%&#x27; ;-- oozieCREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER &#x27;oozie&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Oozie@147258&#x27;;grant all privileges on oozie.*  to &#x27;oozie&#x27;@&#x27;%&#x27; ;-- flushFLUSH PRIVILEGES;\n\n6.上传连接mysql的jar包\n上传到服务器上并更改名称\n\nls# mysql-connector-java-8.0.16.jarcp mysql-connector-java-8.0.16.jar  /usr/share/java/mysql-connector-java.jar\n\n7.安装 cloudera-manager\n创建cloudera-manager目录,存放cdh安装文件。解压压缩包\n\nmkdir /opt/cloudera-managertar -zxvf cm6.3.1-redhat7.tar.gzcd cm6.3.1/RPMS/x86_64/mv cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm /opt/cloudera-manager/mv cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm /opt/cloudera-manager/mv cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm /opt/cloudera-manager/\n\n\nrpm安装,所有节点都安装cloudera-manager-daemons和cloudera-manager-agent\n\nrpm -ivh cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm # 安装agent依赖yum install -y perl bind-utils psmisc cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs /lib/lsb/init-functions httpd mod_ssl openssl-devel python-psycopg2 MySQL-python libxsltrpm -ivh cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm\n\n\n修改agent配置文件\n\nvim /etc/cloudera-scm-agent/config.iniserver_host=hadoop101\n\n\n主节点安装cloudera-manager-server\n\nrpm -ivh cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm \n\n\n修改 server的db.properties \n\nvim /etc/cloudera-scm-server/db.propertiescom.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=hadoop101:3306com.cloudera.cmf.db.name=scmcom.cloudera.cmf.db.user=scmcom.cloudera.cmf.db.password=Scm@147258com.cloudera.cmf.db.setupType=EXTERNAL\n\n\n上传parcel包\n\nls /opt/cloudera/parcel-repo# CDH-6.3.2-1.cdh6.3.2.p0.1605554-el7.parcel# CDH-6.3.2-1.cdh6.3.2.p0.1605554-el7.parcel.sha# manifest.json\n\n8.初始化cm库/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -h hadoop101 scm scm Scm@147258\n\n9.启动Server与Agent\n启动主节点的Server和所有节点的Agent\n\n启动Server命令\nservice cloudera-scm-server.service start\n\n启动Agent命令\nsystemctl start cloudera-scm-agent\n\n\n启动Server查看日志，等待几分钟后无问题登录\n\ntail -f /var/log/cloudera-scm-server/cloudera-scm-server.log\n\n10.登录界面默认端口号：7180默认用户名：admin默认密码：admin\n\n安装完成\n11.安装前后遇到的问题\n问题1\n\n问题描述\nFailed to add storage directory [DISK]file:/data1/dfs/dnjava.io.IOException: Incompatible clusterIDs in /data1/dfs/dn: namenode clusterID = cluster8; datanode clusterID = cluster7\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:722)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:286)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:399)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:379)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:544)\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1740)\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1676)\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)\tat java.lang.Thread.run(Thread.java:748)\n\n解决方式:\nvi  /data/dfs/dn/current/VERSION# 修改相对应的ID#Wed Feb 03 16:14:04 CST 2021storageID=DS-2f4324ff-a104-46ef-a966-c908e19dda2cclusterID=cluster7cTime=0datanodeUuid=20e21b1d-02ee-4f13-bc3a-630eec78580astorageType=DATA_NODElayoutVersion=-57\n\n\n问题2\n\n问题描述\n安装过程界面提示警告\n已启用透明大页面压缩，可能会导致重大性能问题。请运行“echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag”和“echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled”以禁用此设置，然后将同一命令添加到 /etc/rc.local 等初始化脚本中，以便在系统重启时予以设置。以下主机将受到影响:\n\n解决方式\necho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled\n\n\n问题3\n\n问题描述\n主机 hadoop4 上的内存被调拨过度。总内存分配额是 210.0 GiB 个字节，但是 RAM 只有 251.7 GiB 个字节（其中的 50.3 GiB 个字节是保留给系统使用的）。如需获得分配详细信息，请访问“主机”页面上的“资源”选项卡。重新配置主机上的角色以降低总内存分配额。请注意：Java 最大堆大小乘以 1.3 等于近似的 JVM 开销。\n\n解决方式\n内存调拨过度验证阈值改成 0.9\n\n\n问题4\n\n问题描述\nFatal error during KafkaServer startup. Prepare to shutdownkafka.common.InconsistentBrokerIdException: Configured broker.id 56 doesn&#x27;t match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).\tat kafka.server.KafkaServer.getBrokerIdAndOfflineDirs(KafkaServer.scala:707)\tat kafka.server.KafkaServer.startup(KafkaServer.scala:212)\tat kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:42)\tat kafka.Kafka$.main(Kafka.scala:75)\tat com.cloudera.kafka.wrap.Kafka$$anonfun$1.apply(Kafka.scala:92)\tat com.cloudera.kafka.wrap.Kafka$$anonfun$1.apply(Kafka.scala:92)\tat com.cloudera.kafka.wrap.Kafka$.runMain(Kafka.scala:103)\tat com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:95)\tat com.cloudera.kafka.wrap.Kafka.main(Kafka.scala)\n\n解决方式\n# 修改meta.properties文件的相应ID值find / -name meta.properties# /var/local/kafka/data/meta.propertiesvi /var/local/kafka/data/meta.properties## Sat Jan 30 13:34:53 CST 2021# version=0# broker.id=65\n\n\n问题5\n\n问题描述\n\narchive.cloudera.com主机名未配置【此报错，可忽略】\n\n2021-01-22 14:42:14,126 ERROR ParcelUpdateService:com.cloudera.parcel.components.ParcelDownloaderImpl: (11 skipped) Unable to retrieve remote parcel repository manifestjava.util.concurrent.ExecutionException: java.net.UnknownHostException: archive.cloudera.com: 未知的名称或服务\tat com.ning.http.client.providers.netty.future.NettyResponseFuture.abort(NettyResponseFuture.java:231)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.abort(NettyRequestSender.java:422)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:290)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:142)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:117)\tat com.ning.http.client.providers.netty.NettyAsyncHttpProvider.execute(NettyAsyncHttpProvider.java:87)\tat com.ning.http.client.AsyncHttpClient.executeRequest(AsyncHttpClient.java:506)\tat com.ning.http.client.AsyncHttpClient$BoundRequestBuilder.execute(AsyncHttpClient.java:229)\tat com.cloudera.parcel.components.ParcelDownloaderImpl.getRepositoryInfoFuture(ParcelDownloaderImpl.java:592)\tat com.cloudera.parcel.components.ParcelDownloaderImpl.getRepositoryInfo(ParcelDownloaderImpl.java:544)\tat com.cloudera.parcel.components.ParcelDownloaderImpl.syncRemoteRepos(ParcelDownloaderImpl.java:357)\tat com.cloudera.parcel.components.ParcelDownloaderImpl$1.run(ParcelDownloaderImpl.java:464)\tat com.cloudera.parcel.components.ParcelDownloaderImpl$1.run(ParcelDownloaderImpl.java:459)\tat com.cloudera.cmf.persist.ReadWriteDatabaseTaskCallable.call(ReadWriteDatabaseTaskCallable.java:36)\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:748)Caused by: java.net.UnknownHostException: archive.cloudera.com: 未知的名称或服务\tat java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)\tat java.net.InetAddress.getAllByName0(InetAddress.java:1276)\tat java.net.InetAddress.getAllByName(InetAddress.java:1192)\tat java.net.InetAddress.getAllByName(InetAddress.java:1126)\tat java.net.InetAddress.getByName(InetAddress.java:1076)\tat com.ning.http.client.NameResolver$JdkNameResolver.resolve(NameResolver.java:28)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.remoteAddress(NettyRequestSender.java:358)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.connect(NettyRequestSender.java:369)\tat com.ning.http.client.providers.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:283)\t... 15 more\n\n解决方式\n\n向hosts文件添加127.0.0.1 archive.cloudera.com，重启 cloudera-scm-server\n\ncat &gt;&gt; /etc/hosts/ &lt;&lt;EOF127.0.0.1 archive.cloudera.comEOFsystemctl restart cloudera-scm-servertailf /var/log/cloudera-scm-server/cloudera-scm-server.log\n\n\n问题6\n\n问题描述\n\n问题分析:提示datanode的clusterID()和namenode的clusterID不匹配;\n\n产生原因:可能是在安装CDH的时候,第一次格式化dfs后,启动了hadoop,中途又重新执行了某些步骤导致重复dfs格式化命令（hdfs namenode -format),这时namenode的clusterID会重新生成,而datanode的clusterID保持不变\nFailed to add storage directory [DISK]file:/data1/dfs/dnjava.io.IOException: Incompatible clusterIDs in /data1/dfs/dn: namenode clusterID = cluster8; datanode clusterID = cluster7\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:722)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:286)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:399)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:379)\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:544)\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1740)\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1676)\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)\tat java.lang.Thread.run(Thread.java:748)\n\n解决方式\n\n根据报错地址去查看current下的VERSION文件，更改clusterID\n\ncat /data1/dfs/dn/current/VERSION#Sat Jan 30 11:15:36 CST 2021storageID=DS-f9072fbc-3fb9-4600-a3c3-35e465a5cd45clusterID=cluster8cTime=0datanodeUuid=b34f6c62-9f50-488e-a5b4-85046595bcb7storageType=DATA_NODElayoutVersion=-57\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"CDH集群相关命令","url":"/20241121/Hadoop/1ca0264eaaf9/","content":"CDH集群相关命令操作集群命令\n查看dfs空间sudo -u hdfs hadoop fs -ls /hadoop fs -ls -R /aa\n\n创建文件夹hadoop fs -mkdir /ahadoop fs -mkdir -p /aa/bb/cc\n\n查看hsdf系统根目录下的所有文件包括子文件夹里面的文件hadoop fs -ls -R /aa\n\n上传文件hadoop fs -put words.txt /aahadoop fs -copyFromLocal words.txt /aa/bb\n\n下载文件hadoop fs -get /aa/words.txt ~/newwords.txthadoop fs -copyToLocal /aa/words.txt ~/newwords1.txt\n\n合并下载hadoop fs -getmerge /aa/words.txt /aa/bb/words.txt ~/2words.txt\n\n复制\n从HDFS一个路径拷贝到HDFS另一个路径\n\nhadoop fs -cp /aa/words.txt /a\n\n移动\n在HDFS目录中移动文件\n\nhadoop fs -mv /a/words.txt /aa/bb/cc\n\n删除\n删除文件或文件夹\n\nhadoop fs -rm /aa/bb/cc/words.txt\n\n\n删除空目录 \n\nhadoop fs -rmdir /aa/bb/cc/\n\n\n强制删除 \n\nhadoop fs -rm -r /aa/bb/\n\n从本地剪切文件到HDFS上hadoop fs -moveFromLocal ~/hello.txt /aa\n\n追加文件\n追加之前hello.txt到words.txt之前 \n\nhadoop fs -appendToFile ~/hello.txt /aa/words.txt\n\n查看文件内容hadoop fs -cat /aa/hello.txt\n\n查看集群的工作状态hadoop dfsadmin -report\n\nnamenode格式化hadoop namenode -format\n\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"CentOS7.6制作升级内核后ISO镜像文件","url":"/20241121/CentOS/02a0437bb3ba/","content":"CentOS7.6制作升级内核后ISO镜像文件1.安装官方OS下载Centos官方包，我这里下载的是CentOS-7.4-x86_64-DVD-1810.iso然后安装在服务器上（用来定制ISO的服务器）根据自己的需求选择安装，我这里选择的是minimal安装 。\n 注：制作出来的ISO会沿用现在的选择，也就是说制作出来的ISO只要没问题，就会在选择”install centos”后自动检查软件依赖并安装和设置用户（是的，账户&amp;密码也会和现在安装所选择&#x2F;设置的一样）。 \n2.安装制作ISO的工具yum  -y install createrepo mkisofs isomd5sum rsync\n\n3.建立 image-making-directory（1）创建 ISO制作目录mkdir -p /root/iso/CentOS7.6-5.16\n\n（2）挂载官方ISO启动盘# 方式1 软盘挂载(假设是/dev.sr1)mkdir /mnt/usbmount /dev/sr1 /mnt/usb# 方式2 上传iso后挂载mkdir /mnt/usbmount -o loop /mnt/CentOS-7-x86_64-DVD-1810.iso /mnt/usb\n\n（3）把官方镜像里的文件同步到image-making-directoryrsync -a /mnt/usb/ /root/iso/CentOS7.6-5.16/或scp -r /mnt/usb/ /root/iso/CentOS7.6-5.16/\n\n 可以使用 ll -a查看目录，其中 \ntotal 336-rw-rw-r--. 1 root root     14 Nov 26  2018 CentOS_BuildTagdrwxr-xr-x. 3 root root     35 Nov 26  2018 EFI-rw-rw-r--. 1 root root    227 Aug 30  2017 EULA-rw-rw-r--. 1 root root  18009 Dec 10  2015 GPLdrwxr-xr-x. 3 root root     57 Nov 26  2018 imagesdrwxr-xr-x. 2 root root    212 Jan 17 19:00 isolinuxdrwxr-xr-x. 2 root root     43 Nov 26  2018 LiveOSdrwxrwxr-x. 2 root root 233472 Jan 14 22:56 Packagesdrwxr-xr-x. 2 root root   4096 Jan 17 16:43 repodata-rw-rw-r--. 1 root root   1690 Dec 10  2015 RPM-GPG-KEY-CentOS-7-rw-rw-r--. 1 root root   1690 Dec 10  2015 RPM-GPG-KEY-CentOS-Testing-7-r--r--r--. 1 root root   2883 Nov 26  2018 TRANS.TBL\n\n4.内核升级与选择自己想安装的工具包（1）配置yum主要是为了保存rpm包（要放进我们定制的ISO里面）。\nvi  /etc/yum.conf# keepcache=0改为keepcache=1\n\n 保存缓存的rpm包 \n（2）安装下载工具包# 方式1 yum想要安装的包,例如:yum install -y net-tools.x86_64 vim i2c-tools gcc# 方式2 也可以手动安装自己制作下载的rpm离线包,例如:rpm -ivh myPackage-1.0.0-1.x86_64.rpm# 或yum install -y myPackage-1.0.0-1.x86_64.rpm\n\n（3）内核升级使用rpm命令添加新的ELRepo存储库。\nrpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm\n\n (从ELRepo信息库安装一个新的最新内核) \n# --enablerepo是在CentOS系统上启用特定存储库的选项。默认情况下，启 用“ elrepo”存储库，但不启用“ elrepo-kernel”存储库yum --enablerepo=elrepo-kernel install kernel-ml\n\n5.替换Packages&#x2F;里的文件 官方包有4G多大，主要是里面存放了很多rpm包,这里可以选择删除掉也可以不删除。\n（1）生成当前的rpm列表rpm -qa &gt;&gt; /root/install.log\n\n（2）选择性删除包(不删除此步骤略过)需要其他rpm包不删除,建议不进行删除,可以用作本地源, 删除后的依赖问题也是比较头疼\n添加的包不要太多，包太多ISO太大DVD可能刻录不下\nrm -f /root/iso/CentOS7.6-5.16/Packages/*\n\n执行以下会提示一些包没有，这些是我们用yum安装的以及自己本地的rpm包。\ncat /root/install.log|awk &#x27;&#123;print $0&#125;&#x27; |xargs -i cp /mnt/usb/Packages/&#123;&#125;.rpm /root/iso/CentOS7.6-5.16/Packages/\n\n原包Packages内可能是其他包的依赖包,所以此次都复制过去,避免依赖问题。\n\\cp /mnt/usb/Packages/lib* /root/iso/CentOS7.6-5.16/Packages/\\cp /mnt/usb/Packages/kernel* /root/iso/CentOS7.6-5.16/Packages/\\cp /mnt/usb/Packages/python* /root/iso/CentOS7.6-5.16/Packages/\n\n（3）拷贝安装下载后的包文件到image-making-directory目录下\\cp /var/cache/yum/x86_64/7/base/packages/* /root/iso/CentOS7.6-5.16/Packages/\\cp /var/cache/yum/x86_64/7/updates/packages/* /root/iso/CentOS7.6-5.16/Packages/# 升级内核的包不在base目录下,在其他源elrepo-kernel的目录下\\cp /var/cache/yum/x86_64/7/elrepo-kernel/packages/* /root/iso/CentOS7.6-5.16/Packages/\n\n 当然不是所有yum的包都在这里，比如centos-release-gcc和devtoolset-7-gcc*这些包是在其他目录的,使用find命令查找，别忘了把自己制作的rpm也放进去（如果有的话）。\nfind / -name devtoolset-7*.rpm\n\n6.重新生成依赖文件（非常重要） *comps.xml文件包含所有与RPM相关的内容。 它检查“软件包”下的RPM软件包依赖性。 安装时如果依赖包丢失，它将提示哪个RPM包需要哪个依赖。 \nISO_DIR=/root/iso/CentOS7.6-5.16createrepo -g $&#123;ISO_DIR&#125;/repodata/*-comps.xml  $&#123;ISO_DIR&#125;\n\n*-coms.xml只有一个 \naced7d22b338fdf7c0a71ffcf32614e058f4422c42476d1f4b9e9364d567702f-c7-x86_64-comps.xml\n\n 注：如果后面加了RPM包一定要重新生成\n7.编辑 ks.cfg文件 通常，安装系统后，&#x2F; root目录中将存在anaconda-ks.cfg文件，记录系统安装过程中的配置。 将其复制到镜像下的isolinux目录中 。\ncp /root/anaconda-ks.cfg /root/iso/CentOS7.6-5.16/isolinux/ks.cfg\n\n 可以根据这个ks.cfg来定制安装需求，与rpmbuild配置类似。\n%pre表示系统安装前运行的内容\n%post表示系统安装后执行的脚本%packages表示要安装的包，我们主要是在这里加上需要安装的rpm \nks.cfg文件内容:\n(此文件root密码默认为Root@123)\n#version=DEVEL# System authorization informationauth --enableshadow --passalgo=sha512# Use CDROM installation mediacdrom# Use graphical installgraphical# Run the Setup Agent on first bootfirstboot --enableignoredisk --only-use=sda# Keyboard layoutskeyboard --vckeymap=us --xlayouts=&#x27;us&#x27;# System languagelang en_US.UTF-8# Network informationnetwork  --bootproto=dhcp --device=enp1s0 --onboot=off --ipv6=auto --no-activatenetwork  --hostname=localhost.localdomain# Root passwordrootpw --iscrypted $6$/cak3hUuBmFNqaMn$0Ug37Q5q5l8Jx/aegQ7G/Ksz6pe8jrSyBthtmuD5xBA1X7G4kjqVh0ABYilcteihLt4x3HVGFHIPVUkPyv34W.# System servicesservices --disabled=&quot;chronyd&quot;# System timezonetimezone Asia/Shanghai --isUtc --nontp# System bootloader configurationbootloader --append=&quot; crashkernel=auto&quot; --location=mbr --boot-drive=sda# Partition clearing informationclearpart --all --initlabel --list=sda%packages@^minimal@corekexec-toolswgetnet-toolscurltelnetexpectunzipbash-completionvim-enhancedvimgcckernel-ml%end%addon com_redhat_kdump --enable --reserve-mb=&#x27;auto&#x27;%end%anacondapwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notemptypwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyokpwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty%end\n\n注1：加上的是RPM的NAME而不是rpm文件的名字，如我定制的rpm包，文件名叫evglow-1.0.0-1.x86_64.rpm，但是RPM的NAME是evglow-tool，所以%packages下面写的是evglow-tool，\n即yum包时你写什么文件内就写什么。\n8.编辑启动文件(1)修改isolinux.cfg文件 修改在label linux这个item下的配置 \n\n如果有“menu default”这一行，删掉。\n把” append initrd&#x3D;initrd.img inst.stage2&#x3D;hd:LABEL&#x3D;CentOS\\x207\\x20x86_64 quiet”这一行改为”append initrd&#x3D;initrd.img inst.stage2&#x3D;hd:LABEL&#x3D;CentOS7 inst.ks&#x3D;hd:LABEL&#x3D;CentOS7:&#x2F;isolinux&#x2F;ks.cfg“这个“CentOS7”可以任意命名，记得保持一致就好（后面刻盘时候，USB启动盘的名字要改成这个。不然找不到U盘的话需要先在install centos那里按”E”,然后使用linux dd 查看U盘设备名，假如是&#x2F;dev&#x2F;sda4，确认设备名后重启，再次按E编辑，把”LABEL&#x3D;CentOS7”改为&#x2F;dev&#x2F;sda4,两边都要改,然后ctrl+X进入安装界面）。\n\nvim isolinux/isolinux.cfg \n\ndefault vesamenu.c32timeout 600display boot.msg# Clear the screen when exiting the menu, instead of leaving the menu displayed.# For vesamenu, this means the graphical background is still displayed without# the menu itself for as long as the screen remains in graphics mode.menu clearmenu background splash.pngmenu title CentOS 7menu vshift 8menu rows 18menu margin 8#menu hiddenmenu helpmsgrow 15menu tabmsgrow 13# Border Areamenu color border * #00000000 #00000000 none# Selected itemmenu color sel 0 #ffffffff #00000000 none# Title barmenu color title 0 #ff7ba3d0 #00000000 none# Press [Tab] messagemenu color tabmsg 0 #ff3a6496 #00000000 none# Unselected menu itemmenu color unsel 0 #84b8ffff #00000000 none# Selected hotkeymenu color hotsel 0 #84b8ffff #00000000 none# Unselected hotkeymenu color hotkey 0 #ffffffff #00000000 none# Help textmenu color help 0 #ffffffff #00000000 none# A scrollbar of some type? Not sure.menu color scrollbar 0 #ffffffff #ff355594 none# Timeout msgmenu color timeout 0 #ffffffff #00000000 nonemenu color timeout_msg 0 #ffffffff #00000000 none# Command prompt textmenu color cmdmark 0 #84b8ffff #00000000 nonemenu color cmdline 0 #ffffffff #00000000 none# Do not display the actual menu unless the user presses a key. All that is displayed is a timeout message.menu tabmsg Press Tab for full configuration options on menu items.menu separator # insert an empty linemenu separator # insert an empty linelabel linux  menu label ^Install CentOS 7  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS7 inst.ks=hd:LABEL=CentOS7:/isolinux/ks.cfg quietlabel check  menu label Test this ^media &amp; install CentOS 7  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS7 inst.ks=hd:LABEL=CentOS7:/isolinux/ks.cfg rd.live.check quietmenu separator # insert an empty line# utilities submenumenu begin ^Troubleshooting  menu title Troubleshootinglabel vesa  menu indent count 5  menu label Install CentOS 7 in ^basic graphics mode  text help\tTry this option out if you&#x27;re having trouble installing\tCentOS 7.  endtext  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 xdriver=vesa nomodeset quietlabel rescue  menu indent count 5  menu label ^Rescue a CentOS system  text help\tIf the system will not boot, this lets you access files\tand edit config files to try to get it booting again.  endtext  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 rescue quietlabel memtest  menu label Run a ^memory test  text help\tIf your system is having issues, a problem with your\tsystem&#x27;s memory may be the cause. Use this utility to\tsee if the memory is working correctly.  endtext  kernel memtestmenu separator # insert an empty linelabel local  menu label Boot from ^local drive  localboot 0xffffmenu separator # insert an empty linemenu separator # insert an empty linelabel returntomain  menu label Return to ^main menu  menu exitmenu end\n\n(2)修改 grub.cfg 把menuentry ‘Install CentOS 7’ –class fedora –class gnu-linux –class gnu –class os {undefined下面那行的后半部分也改为:\ninst.stage2=hd:LABEL=CentOS7 inst.ks=hd:LABEL=CentOS7:/isolinux/ks.cfg效果如下 :\nvim /root/iso/CentOS7.6-5.16/EFI/BOOT/grub.cfg\n\nset default=&quot;1&quot;function load_video &#123;  insmod efi_gop  insmod efi_uga  insmod video_bochs  insmod video_cirrus  insmod all_video&#125;load_videoset gfxpayload=keepinsmod gzioinsmod part_gptinsmod ext2set timeout=60### END /etc/grub.d/00_header ###search --no-floppy --set=root -l &#x27;CentOS 7 x86_64&#x27;### BEGIN /etc/grub.d/10_linux ###menuentry &#x27;Install CentOS 7&#x27; --class fedora --class gnu-linux --class gnu --class os &#123;\tlinuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS7 inst.stage2=hd:LABEL=CentOS7:/isolinux/ks.cfg quiet\tinitrdefi /images/pxeboot/initrd.img&#125;menuentry &#x27;Test this media &amp; install CentOS 7&#x27; --class fedora --class gnu-linux --class gnu --class os &#123;\tlinuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS7 inst.stage2=hd:LABEL=CentOS7:/isolinux/ks.cfg rd.live.check quiet\tinitrdefi /images/pxeboot/initrd.img&#125;submenu &#x27;Troubleshooting --&gt;&#x27; &#123;\tmenuentry &#x27;Install CentOS 7 in basic graphics mode&#x27; --class fedora --class gnu-linux --class gnu --class os &#123;\t\tlinuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 xdriver=vesa nomodeset quiet\t\tinitrdefi /images/pxeboot/initrd.img\t&#125;\tmenuentry &#x27;Rescue a CentOS system&#x27; --class fedora --class gnu-linux --class gnu --class os &#123;\t\tlinuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 rescue quiet\t\tinitrdefi /images/pxeboot/initrd.img\t&#125;&#125;\n\n9.制作ISO（1）制作镜像mkisofs -o CentOS7.6-5.16-V1.2.iso -input-charset utf-8 -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -R -J -v -T -joliet-long -V CentOS7 /root/iso/CentOS7.6-5.16/\n\n 此时会生成CentOS7.6-5.16-V1.2.iso文件\n（2） 生产Md5(可以略过)implantisomd5 CentOS7.4-wedge400.iso\n\n10.目录展示[root@localhost ~]# ll /roottotal 4639200-rw-------. 1 root root       1592 Jan 14 23:30 anaconda-ks.cfg-rw-r--r--. 1 root root 4750512128 Jan 17 17:46 CentOS7.6-boer-v1.2.iso-rw-r--r--. 1 root root      21153 Jan 14 22:31 install.logdrwxr-xr-x. 3 root root         28 Jan 17 22:09 iso\n\n\n\n本文借鉴 evglow\n完\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS7.6升级内核","url":"/20241121/CentOS/1154457a90e2/","content":"CentOS7.6升级内核1、更新系统和安装包yum -y update# 安装yum插件，以在安装和更新软件包时更快yum -y install yum-plugin-fastestmirror \n\n2、查看内核版本uname -r\n\n3、添加ELrepo存储库(在安装新的内核版本之前，我们需要添加一个新的存储 库-ELRepo存储库）将ELRepo gpg密钥添加到系统 \n# 使用rpm命令添加新的ELRepo存储库rpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm# 检查系统上启用的所有存储库，并确保ELRepo在列表中、yum repolist\n\n4、安装最新版本的内核\n在线升级\n\n (从ELRepo信息库安装一个新的最新内核) \nyum --enablerepo=elrepo-kernel install kernel-ml# --enablerepo是在CentOS系统上启用特定存储库的选项。默认情况下，启 用“ elrepo”存储库，但不启用“ elrepo-kernel”存储库\n\n\n离线升级\n\n1.用可以联网的机器下载升级包\nyum install yum-plugin-downloadonly# 下载kernel-ml 包yum install --enablerepo=elrepo-kernel --downloadonly --downloaddir=./ kernel-ml\n\n2.将rpm包上传到要升级的服务器\nrpm -ivh 下载的内核升级的rpm包名称\n\n5、在centos7上配置Grub2 使用以下awk命令检查Grub2中所有可用的内核版本 \nawk -F\\&#x27; &#x27;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#x27; /etc/grub2.cfg# 0 : CentOS Linux (5.16.0-1.el7.elrepo.x86_64) 7 (Core)# 1 : CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core)# 2 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)# 3 : CentOS Linux (0-rescue-6ae14471c6f9480298cc7690f9d1803e) 7 (Core)\n\n (可以看到有两个版本的内核3.10.0和5.6.7) \n6、配置系统默认内核5.6.7grub2-set-default 0\n\n 使用“ gurb2-mkconfig”命令生成grub2配置，然后重新启动服务器 \ngrub2-mkconfig -o /boot/grub2/grub.cfg\n\n7、 重启查看内核版本rebootuname -r\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS7.6安装部署gitlab","url":"/20241121/CentOS/f426798f8891/","content":"CentOS7.6安装部署GitLabGitLab基本介绍GitLab是利用Ruby on Rails一个开源的版本管理系统，实现一个自托管的Git项目仓库，可通过Web界面进行访问公开的或者私人项目。\n与Github类似，GitLab能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序(Wall)进行交流。\n它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。\n本篇教程将教你如何安装部署及使用GitLab。\nGit的家族成员\nGit：是一种版本控制系统，是一个命令，是一种工具。\nGitlib：是用于实现Git功能的开发库。\nGithub：是一个基于Git实现的在线代码托管仓库，包含一个网站界面，向互联网开放。\nGitLab：是一个基于Git实现的在线代码仓库托管软件，你可以用gitlab自己搭建一个类似于Github一样的系统，一般用于在企业、学校等内部网络搭建git私服。\nGitlab的服务构成\nNginx：静态web服务器。\ngitlab-shell：用于处理Git命令和修改authorized keys列表。\ngitlab-workhorse: 轻量级的反向代理服务器。\nlogrotate：日志文件管理工具。\npostgresql：数据库。\nredis：缓存数据库。\nsidekiq：用于在后台执行队列任务（异步执行）。\nunicorn：An HTTP server for Rack applications，GitLab Rails应用是托管在这个服务器上面的。\nGitLab Shell\nGitLab Shell有两个作用：为GitLab处理Git命令、修改authorized keys列表。\n当通过SSH访问GitLab Server时，GitLab Shell会：\n\n限制执行预定义好的Git命令（git push, git pull, git annex）\n调用GitLab Rails API 检查权限\n执行pre-receive钩子（在GitLab企业版中叫做Git钩子）\n执行你请求的动作 处理GitLab的post-receive动作\n处理自定义的post-receive动作\n\n当通过http(s)访问GitLab Server时，工作流程取决于你是从Git仓库拉取(pull)代码还是向git仓库推送(push)代码。\n如果你是从Git仓库拉取(pull)代码，GitLab Rails应用会全权负责处理用户鉴权和执行Git命令的工作；\n如果你是向Git仓库推送(push)代码，GitLab Rails应用既不会进行用户鉴权也不会执行Git命令，它会把以下工作交由GitLab Shell进行处理：\n\n调用GitLab Rails API 检查权限\n执行pre-receive钩子（在GitLab企业版中叫做Git钩子）\n执行你请求的动作\n处理GitLab的post-receive动作\n处理自定义的post-receive动作\n\nGitLab Workhorse\nGitLab Workhorse是一个敏捷的反向代理。它会处理一些大的HTTP请求，比如文件上传、文件下载、Git push&#x2F;pull和Git包下载。其它请求会反向代理到GitLab Rails应用，即反向代理给后端的unicorn。\nGitlab环境部署 1、配置yum源 \nvi /etc/yum.repos.d/gitlab-ce.repo\n\n 复制以下内容： \n[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1\n\n 2、更新本地yum缓存 \nyum makecache\n\n3、安装GitLab社区版\nyum install gitlab-ce           #自动安装最新版yum install gitlab-ce-x.x.x     #安装指定版本\n\n4、安装Git工具linux：安装Git，使用自带的源安装。\nyum install git\n\nGitLab常用命令vi /etc/gitlab/gitlab.rb    # 修改默认的配置文件；gitlab-ctl reconfigure    # 启动服务；gitlab-ctl start    # 启动所有 gitlab 组件；gitlab-ctl stop    # 停止所有 gitlab 组件；gitlab-ctl restart    # 重启所有 gitlab 组件；gitlab-ctl status    # 查看服务状态；gitlab-rake gitlab:check SANITIZE=true --trace    # 检查gitlab；gitlab-ctl tail     # 查看日志；\n\nGitLab使用登录GitLab\n1、在浏览器的地址栏中输入ECS服务器的公网IP即可登录GitLab的界面，第一次登录使用的用户名为root \n密码查看 cat /etc/gitlab/initial_root_password。\n2、首次登录后修改密码。密码修改成功后，输入新密码进行登录。\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS7修改内核启动顺序","url":"/20241121/CentOS/b005c791d5fd/","content":"CentOS7修改内核启动顺序1.首先查看当前系统有几个内核#方式1awk -F\\&#x27; &#x27;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#x27; /etc/grub2.cfg# 0 : CentOS Linux (5.16.0-1.el7.elrepo.x86_64) 7 (Core)# 1 : CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core)# 2 : CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)# 3 : CentOS Linux (0-rescue-6ae14471c6f9480298cc7690f9d1803e) 7 (Core)# 方式2cat /boot/grub2/grub.cfg |grep menuentry\n\n2.查看当前默认内核grub2-editenv list# saved_entry=CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core)\n\n3.更改默认启动内核grub2-set-default &#x27;5.16.0-1.el7.elrepo.x86_64) 7 (Core)&#x27;\n\n4.重启机器生效reboot\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS7宕机后Mysql服务无法启动","url":"/20241121/Mysql/ceec2329e7dc/","content":"CentOS7宕机后Mysql服务无法启动1.尝试重启mysqld服务systemctl restart mysqld\n重启后卡死在执行命令中,长时间不退出,放弃\n尝试手动启动\n/usr/sbin/mysqld start\n也不好使放弃\n2.先尝试修改innodb_force_recovery参数innodb_force_recovery 配置1-6选项：1 (SRV_FORCE_IGNORE_CORRUPT): 忽略检查到的 corrupt 页。尽管检测到了损坏的 page 仍强制服务运行。一般设置为该值即可，然后 dump 出库表进行重建。2 (SRV_FORCE_NO_BACKGROUND): 阻止主线程的运行，如主线程需要执行 full purge 操作，会导致 crash。 阻止 master thread 和任何 purge thread 运行。若 crash 发生在 purge 环节则使用该值。3 (SRV_FORCE_NO_TRX_UNDO): 不执行事务回滚操作。4 (SRV_FORCE_NO_IBUF_MERGE): 不执行插入缓冲的合并操作。如果可能导致崩溃则不要做这些操作。不要进行统计操作。该值可能永久损坏数据文件。若使用了该值，则将来要删除和重建辅助索引。5 (SRV_FORCE_NO_UNDO_LOG_SCAN): 不查看重做日志，InnoDB 存储引擎会将未提交的事务视为已提交。此时 InnoDB 甚至把未完成的事务按照提交处理。该值可能永久性的损坏数据文件。6 (SRV_FORCE_NO_LOG_REDO): 不执行前滚的操作。恢复时不做 redo log roll-forward。使数据库页处于废止状态，继而可能引起 B 树或者其他数据库结构更多的损坏innodb_force_recovery建议：1、如果MySQL服务故障重启后，因为事务回滚导致异常，可以将参数innodb_force_recovery设置为3跳过回滚阶段2、如果因为MySQL数据页损坏导致异常，可以使用SELECT+WHERE查找出未损坏数据并将其通过mysqldump导出。3、将innodb_force_recovery参数设置大于0启动服务后，应通过修改端口或域名(VIP)指向来屏蔽应用访问。4、将innodb_force_recovery参数设置大于0启动服务后，可以通过mysqlcheck命令来对表进行检查/分析/优化/修复。5、使用force_recovery重启服务前，建议对数据库所有文件进行备份，避免修复过程中对数据进行二次损害。在日常运维中，应将使用innodb_force_recovery参数进行数据恢复作为最后手段，做好完善的备份恢复机制，避免对数据库做高危操作。\n\nvi /etc/my.cnf# 添加innodb_force_recovery = 3\n若成功启动后注释掉此参数 重启mysqld服务\n但我的问题未解决\n2.查看&#x2F;usr&#x2F;sbin&#x2F;mysqld status[root@master3 ~]# /usr/sbin/mysqld  status2022-07-11T02:24:32.318716Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.16) starting as process 169392022-07-11T02:24:32.320750Z 0 [ERROR] [MY-010123] [Server] Fatal error: Please read &quot;Security&quot; section of the manual to find out how to run mysqld as root!2022-07-11T02:24:32.320815Z 0 [ERROR] [MY-010119] [Server] Aborting2022-07-11T02:24:32.321037Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.16)  MySQL Community Server - GPL.\n\n百度后修改user参数无果后放弃\n3.查看msyql的binlog文件mysqlbinlog --start-datetime=&quot;2022-06-29 16:00:00&quot; --stop-datetime=&quot;2022-07-11 09:00:00&quot;  --base64-output=decode-rows -v binlog.000025\n未见到异常信息\n4.尝试mysqld –daemonize 方式启动mysqld --daemonize# 开启新的窗口查看日志tail -1000f /var/log/mysqld.log\n运行后看到&#x2F;var&#x2F;log&#x2F;mysqld.log日志内报错经验证排除了内存，句柄数等问题. 并且切换至root用户打开大量线程时没有问题。试着调整ulimit各项参数,最后发现是”max user processes “参数有问题，通过root用户调整大小至 12000,线程数也随着增大.\n2022-07-11T02:39:22.810927Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.16) starting as process 19842022-07-11T02:39:22.871940Z 1 [ERROR] [MY-012574] [InnoDB] Unable to lock ./ibdata1 error: 112022-07-11T02:40:59.893262Z 1 [ERROR] [MY-012574] [InnoDB] Unable to lock ./ibdata1 error: 112022-07-11T02:41:00.893393Z 1 [ERROR] [MY-012574] [InnoDB] Unable to lock ./ibdata1 error: 112022-07-11T02:41:01.893623Z 1 [ERROR] [MY-012574] [InnoDB] Unable to lock ./ibdata1 error: 112022-07-11T02:41:02.893861Z 1 [ERROR] [MY-012574] [InnoDB] Unable to lock ./ibdata1 error: 112022-07-11T02:41:02.894715Z 1 [ERROR] [MY-012592] [InnoDB] Operating system error number 11 in a file operation.2022-07-11T02:41:02.894745Z 1 [ERROR] [MY-012596] [InnoDB] Error number 11 means &#x27;Resource temporarily unavailable&#x27;2022-07-11T02:41:02.894773Z 1 [ERROR] [MY-012215] [InnoDB] Cannot open datafile &#x27;./ibdata1&#x27;2022-07-11T02:41:02.894839Z 1 [ERROR] [MY-012959] [InnoDB] Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!2022-07-11T02:41:02.894868Z 1 [ERROR] [MY-012930] [InnoDB] Plugin initialization aborted with error Cannot open a file.2022-07-11T02:41:03.496799Z 1 [ERROR] [MY-010334] [Server] Failed to initialize DD Storage Engine2022-07-11T02:41:03.497069Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.2022-07-11T02:41:03.497874Z 0 [ERROR] [MY-010119] [Server] Aborting2022-07-11T02:41:03.498844Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.16)  MySQL Community Server - GPL.\n\n终于发现报错\n问题解决方案vi /etc/security/limits.conf# 添加下列2参数 * soft nproc 12000* hard nproc 12000\nnproc就是”max user processes”，完整描述是: nproc - max number of processes参数含义:\n单个用户可以启动的线程数，因为进程也会启动一个线程，所以也间接对进程数有限制。\n注意：\n该参数只对普通用户有用，root用户不在此限制。 所以用root用户可以启动几万个线程，无法重现这个问题.\n重新加载系统参数\nsysctl -p\n\n重启mysqld服务\nsystemctl restart mysqld# 服务启动成功\n\n最后注释掉&#x2F;etc&#x2F;my.cnf中的innodb_force_recovery参数未添加则无需注释#innodb_force_recovery &#x3D; 3在重启mysqld服务\nsystemctl restart mysqld\n\n\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"CentOS中Kvm可视化WebVirtMgr安装部署","url":"/20241121/KVM/6ebade68a5ad/","content":"CentOS中Kvm可视化WebVirtMgr安装部署温馨提示：安装KVM是需要2台都操作的，因为我们是打算将2台都设置为宿主机所有都需要安装KVM相关组件\ngithub地址github.com&#x2F;retspen&#x2F;web…\nWebVirtMgr是一个基于libvirt的Web界面，用于管理虚拟机。它允许您创建和配置新域，并调整域的资源分配。VNC查看器为来宾域提供完整的图形控制台。KVM是目前唯一支持的虚拟机管理程序。\n1.安装源和依赖Centos&#x2F;RHEL 6.Xyum -y install http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmyum -y install git python-pip libvirt-python libxml2-python python-websockify supervisor nginx\n\nCentos7yum -y install http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm if this doesn&#x27;t work, use yum install epel-releaseyum -y install git python-pip libvirt-python libxml2-python python-websockify supervisor nginxyum -y install gcc python-develpip install numpy\n\nDebian 7, Ubuntu 12.04 and until ( excluding ) Ubuntu 20.04apt-get install git python-pip python-libvirt python-libxml2 novnc supervisor nginx \n\nUbuntu 20.04 and aboveapt-get install git python3-pip python3-libvirt python-libxml2 novnc supervisor nginx\n\n2.克隆仓库并安装 Django 等 python 包git clone https://github.com/retspen/webvirtmgr.gitcd webvirtmgr# or python-pip (RedHat, Fedora, CentOS, OpenSuse)pip install -r requirements.txt# ( on Ubuntu 20.04 possible also : sudo apt install imagemagick )# 初始化环境. manage.py syncdb# 配置Django 静态页面. manage.py collectstatic\n\n按提示输入 root 用户密码，该用户后面将用来登陆 WebVirtMgr。\nYou just installed Django&#x27;s auth system, which means you don&#x27;t have any superusers defined.Would you like to create one now? (yes/no): yes (Put: yes)Username (Leave blank to use &#x27;admin&#x27;): admin (Put: your username or login)E-mail address: username@domain.local (Put: your email)Password: xxxxxx (Put: your password)Password (again): xxxxxx (Put: confirm password)Superuser created successfully.\n\n./manage.py createsuperuser\n\n然后测试：\n./manage.py runserver 0:8000\n\n用浏览器打开 http://your-ip:8000 并用刚才的用户登陆\n然后点击右上角添加一个 Add connection，把 localhost 这个 libvirtd 的链接用 tcp 的方式添加进去，用户名和密码是刚才初始化的 libvirt 的管理员 admin 和密码，链接如果能够正常添加的话，表明可以正常运行了。\n3.下载安装 Nginx若已经安装nginx或rpm包之类的安装，直接设置nginx的配置文件即可\n下载编译nginxcd /usr/local/mkdir nginx &amp;&amp; cd nginxwget https://nginx.org/download/nginx-1.20.1.tar.gztar xf nginx-1.20.1.tar.gzcd nginx-1.20.1/yum install -y gcc glibc gcc-c++ prce-devel openssl-devel pcre-develuseradd -s /sbin/nologin nginx -M./configure --prefix=/root/cby/kvm/nginx/ --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_modulemake &amp;&amp; make install\n\n设置nginx的配置文件编辑webvirtmgr.conf到/etc/nginx/conf.d目录\nserver &#123;    listen 80 default_server;    server_name $hostname;    #access_log /var/log/nginx/webvirtmgr_access_log;     location /static/ &#123;        root /var/www/webvirtmgr/webvirtmgr; # 文件的路径        expires max;    &#125;    location ~ .*\\.(js|css)$ &#123;           proxy_pass http://127.0.0.1:8000;    &#125;    location / &#123;        proxy_pass http://127.0.0.1:8000;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-for $proxy_add_x_forwarded_for;        proxy_set_header Host $host:$server_port;        proxy_set_header X-Forwarded-Proto $scheme;        proxy_connect_timeout 600;        proxy_read_timeout 600;        proxy_send_timeout 600;        client_max_body_size 1024M; # Set higher depending on your needs     &#125;&#125;\n\n如果nginx有默认的配置文件，需要注释掉默认的&#x2F;etc&#x2F;nginx&#x2F;nginx.conf(Ubuntu 14.04 LTS配置文件夹在 /etc/nginx/sites-enabled/default)\nvim /etc/nginx/nginx.conf\n\n#    server &#123;#        listen       80 default_server;#        server_name  localhost;#        root         /usr/share/nginx/html;##        #charset koi8-r;##        #access_log  /var/log/nginx/host.access.log  main;##        # Load configuration files for the default server block.#        include /etc/nginx/default.d/*.conf;##        location / &#123;#        &#125;##        # redirect server error pages to the static page /40x.html#        ##        error_page  404              /404.html;#        location = /40x.html &#123;#        &#125;##        # redirect server error pages to the static page /50x.html#        ##        error_page   500 502 503 504  /50x.html;#        location = /50x.html &#123;#        &#125;#    &#125;\n\n启动nginx# 测试nginx配置ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginxnginx -tnginx: the configuration file /usr/local/nginx//conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx//conf/nginx.conf test is successful# 启动nginxnginx# 或者可以添加匹配值文件启动nginx -c /etc/nginx/conf.d/webvirtmgr.conf# 重启nginx -s reload\n\n\n\n4.安装Supervisor服务CentOS, RedHat, Fedorachown -R nginx:nginx /var/www/webvirtmgr\n\n创建&#x2F;etc&#x2F;supervisord.d&#x2F;webvirtmgr.ini文件 并添加配置\n[program:webvirtmgr]command=/usr/bin/python /var/www/webvirtmgr/manage.py run_gunicorn -c /var/www/webvirtmgr/conf/gunicorn.conf.pydirectory=/var/www/webvirtmgrautostart=trueautorestart=truelogfile=/var/log/supervisor/webvirtmgr.loglog_stderr=trueuser=nginx[program:webvirtmgr-console]command=/usr/bin/python /var/www/webvirtmgr/console/webvirtmgr-consoledirectory=/var/www/webvirtmgrautostart=trueautorestart=truestdout_logfile=/var/log/supervisor/webvirtmgr-console.logredirect_stderr=trueuser=nginx\n\nOpenSuSEchown -R nginx:nginx /var/www/webvirtmgr\n\n创建&#x2F;etc&#x2F;supervisord.conf文件 并添加配置\n[program:webvirtmgr]command=/usr/bin/python /srv/www/webvirtmgr/manage.py run_gunicorn -c /srv/www/webvirtmgr/conf/gunicorn.conf.pydirectory=/srv/www/webvirtmgrautostart=trueautorestart=truestdout_logfile=/var/log/supervisor/webvirtmgr.logredirect_stderr=trueuser=nginx[program:webvirtmgri-console]command=/usr/bin/python /srv/www/webvirtmgr/console/webvirtmgr-consoledirectory=/var/www/webvirtmgrautostart=trueautorestart=truestdout_logfile=/var/log/supervisor/webvirtmgr-console.logredirect_stderr=trueuser=nginx\n\nDebian, Ubuntuservice novnc stopinsserv -r novncvi /etc/insserv/overrides/novnc#!/bin/sh### BEGIN INIT INFO# Provides:          nova-novncproxy# Required-Start:    $network $local_fs $remote_fs $syslog# Required-Stop:     $remote_fs# Default-Start:     # Default-Stop:      # Short-Description: Nova NoVNC proxy# Description:       Nova NoVNC proxy### END INIT INFOchown -R www-data:www-data /var/www/webvirtmgr\n\n在&#x2F;etc&#x2F;supervisor&#x2F;conf.d创建&#x2F;etc&#x2F;supervisord.conf文件 并添加配置\n[program:webvirtmgr]command=/usr/bin/python /var/www/webvirtmgr/manage.py run_gunicorn -c /var/www/webvirtmgr/conf/gunicorn.conf.pydirectory=/var/www/webvirtmgrautostart=trueautorestart=truestdout_logfile=/var/log/supervisor/webvirtmgr.logredirect_stderr=trueuser=www-data[program:webvirtmgr-console]command=/usr/bin/python /var/www/webvirtmgr/console/webvirtmgr-consoledirectory=/var/www/webvirtmgrautostart=trueautorestart=truestdout_logfile=/var/log/supervisor/webvirtmgr-console.logredirect_stderr=trueuser=www-data\n\n5.重启supervisor服务Ubuntu, Debianservice supervisor stopservice supervisor start\n\nCentOS, RedHat, Fedoraservice supervisord stopservice supervisord start\n\ncentos没有supervisord服务可以重新加载一下\nsystemctl daemon-reload\n\n5.1更新- CentOS, RedHat, FedoraRead README.md check settings (maybe something has changed) and then:\ncd /var/www/webvirtmgrgit pull./manage.py collectstaticservice supervisord restart\n\n5.2更新- Debian &amp; UbuntuRead README.md check settings (maybe something has changed) and then:\ncd /var/www/webvirtmgrgit pull./manage.py collectstaticservice supervisor restart\n\n5.3更新 - OpenSuSERead README.md check settings (maybe something has changed) and then:\ncd /srv/www/webvirtmgrgit pull./manage.py collectstaticservice supervisord restart\n\n5.4DebugIf you have error or not run panel (only for DEBUG or DEVELOP):\n./manage.py runserver 0:8000\n\nEnter in your browser:\nhttp://x.x.x.x:8000 (x.x.x.x - your server IP address )\n\n\n\n6.web界面配置添加主机设置存储\n1.Add Connection 添加宿主机(即KVM主机)\n2.点击SSH连接\n3.Label 为主机名，必须为主机名做免密\n4.IP 为宿主机IP\n5.用户名为服务器用户名\n6.点击添加\n如下图 ,为添加kvm主机效果图:\n\n\n安装完成\n","categories":["CentOS","KVM"],"tags":["CentOS","KVM"]},{"title":"CentOS中systemctl设置systemd.service服务守护进程","url":"/20241121/CentOS/f765416dfd23/","content":"CentOS之systemctl设置systemd.service服务守护进程一.介绍Systemctl是linux系统继init.d之后的一个systemd工具，主要负责控制systemd系统和管理系统服务\nsystemd即为system daemon,是linux下的一种init软件\n有时我们将自定义程序注册为systemd service 进程管理交由系统管理，可以方便启动停止，亦可以实现服务异常退出重启,开机自启动。 减少自定义程序服务管理的时间消耗。\n二.参数了解systemctl service自定义注册服务的一些参数：systemctl管理的服务脚本存放在&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F; 有user与systemd区分需要开机自启动存放在system目录下：&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system并且服务的脚本一般以.service结尾\n例如：sshd 登录系统服务， 查看定义systemctl cat sshd\n#&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;sshd.service\n[Unit]Description=OpenSSH server daemonDocumentation=man:sshd(8) man:sshd_config(5)After=network.target sshd-keygen.serviceWants=sshd-keygen.service [Service]EnvironmentFile=/etc/sysconfig/sshdExecStart=/usr/sbin/sshd -D $OPTIONSExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failureRestartSec=42s [Install]WantedBy=multi-user.target\n\n可以看到service文件一般由：[Unit]、[Service]和[Install] 三部分组成\n备注： 官方分为：[Unit]、[unit type] 和[Install]\nunit type 如果一个单元具有特殊类型定义则将它们定义在类型命名之后，比如Service定义为[Service]\n参数详解:\n1.[Unit]主要是对这个服务的说明，内容， 文档介绍以及对一些依赖服务定义\n[Unit]\n\n具体参数:\n\nDescription服务描述，作为systemctl status 命令输出的一个介绍\nDocumentation一个url 定义服务的具体介绍网址\nAfter在什么服务启动之后。\nBefore在什么服务启动之前启动。\nRequires依赖其他的单元服务， 需要与列出的服务一起激活，若任何服务无法启动，则该单元不会被激活。\nWants比Requires依赖性弱，弱其他服务没有启动成功，该服务也不受影响，只是表示一种推荐。\n2.[Service]服务的主体定义，主要定义服务的一些运行参数，及操作动作。\n[Service]Type\n\n1.simple默认参数，进程作为主进程\n2.forking是后台运行的形式，主进程退出，os接管子进程\n3.oneshot 类似simple，在开始后续单元之前，过程退出\n4.DBUS 类似simple，但随后的单元只在主进程获得D总线名称之后才启动\n5.notify 类似simple，但是随后的单元仅在通过sd_notify()函数发送通知消息之后才启动\n6.idle类似simple，服务二进制文件的实际执行被延迟到所有作业完成为止，不与其他服务的输出相混合,如状态输出与服务的shell输出混合\n\n备注：以上的类似simple指的是类似simple将启动进程作为主进程进行运行\n\n具体参数:\n\nUser设置服务运行的用户。\nGroup设置服务运行的用户组。\nPIDFile为存放PID的文件路径, 对于type设置为forking建议使用该项。 systemd will read the PID of the main process of the daemon after start-up of the service. systemd will not write to the file configured here,although it will remove the file after the service has shut down if it still exists.\nExecStart服务的具体运行命令,ExecStartPre和ExecStartPost指定在ExecStart前后执行的自定义命令。若使用Type &#x3D; OnHead可以指定多个自定义命令，将依次执行这些命令。\nExecReload为重启命令，重新加载的动作， 重新加载时执行的命令或者脚本。\nExecStop为停止命令，停止时要执行的命令或脚本。\nExecStartPre启动服务之前执行的命令。\nExecStartPost启动服务之后执行的命令。\nExecStopPost停止服务之后执行的命令。\nRestart定义何种情况Systemd会自动重启当前服务，值：包括always（总是重启）、no 、on-success、on-failure、on-abnormal、on-abort、on-watchdog对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal。\nRestartSec设置在重启服务(Restart=)前暂停多长时间。 默认值是100毫秒(100ms)。 如果未指定时间单位，那么将视为以秒为单位。 例如设为”20”等价于设为”20s”。\nTimeoutStartSec等待启动的时间。如果守护进程服务没有在配置的时间内发送启动完成的信号，则该服务将被认为失败， 服务将退出。以秒为单位， “0”来禁用。默认为， 默认使用DefaultTimeoutStartSec，若使用Type&#x3D;oneshot，则该模式默认情况下超时是禁用的。\nTimeoutStopSec等待关闭的超时时间。\nTimeoutSec快速配置TimeoutStartSec和TimeoutStopSec时间。\nKillModecontrol-group（默认值）：当前控制组里面的所有子进程，都会被杀掉process：只杀主进程mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号none：没有进程会被杀掉，只是执行服务的 stop 命令。\nEnvironmen指定环境变量。\nPrivateTmpPrivateTmp&#x3D;True表示给服务分配独立的临时空间。\nKillSignal服务的所有进程都将会根据 KillSignal= 的设置被立即全部杀死。 与 ExecReload= 一样。\n注意：[Service]部分的启动、重启、停止命令必须使用绝对路径，使用相对路径则会报错\n3.[Install]服务安装的相关设置，一般可设置为多用户的\n[Install]\n\n具体参数:\n\nRequiredBy依赖该服务的服务列表。\nWantedBy表示该服务所在的 Target， multi-user.target 可以设置为多用户模式具体参考手systemd.unit(5)。\n4.服务状态列表\nloaded 系统服务已经初始化完成，加载过配置\nactvie(running) 正常运行\nactvie(exited) 正常结束的服务，\nactive(waitting) 正在执行当中， 等待其他的事件才继续处理\ninactive 服务关闭\nenabled 服务开机启动\ndisabled 服务开机不自启\nstatic 服务开机启动项不可被管理\nfalied 系统配置错误\n\nSystemd 统一管理所有 Unit 的启动日志。journalctl命令查看所有日志(内核日志和应用日志)。日志的配置文件是&#x2F;etc&#x2F;systemd&#x2F;journald.conf。\n三.示例运行shell的一个示例：.service文件：\n[Unit]Description=my test serviceAfter=network.target remote-fs.target nss-lookup.target [Service]Type=simplePIDFile=/tmp/my.pidExecStartPre=/usr/bin/rm -f /tmp/my.pidExecStart= /usr/sbin/test.shKillSignal=SIGQUITTimeoutStopSec=5KillMode=processPrivateTmp=true #[Install]#WantedBy=multi-user.target\n\n.shell文件test.sh：\n#!/bin/bashwhile truedo   echo `date`,&quot;ok&quot; &gt;&gt;/tmp/result.logdone\n\n注意#!&#x2F;bin&#x2F;bash是必须的，否则会提示203错误异常，服务启动失败\n四.设置自动重启(1).最简单的自动重启范例\n[Unit]Description=mytest [Service]Type=simpleExecStart=/root/mytest.shRestart=alwaysRestartSec=5StartLimitInterval=0 [Install]WantedBy=multi-user.target\n\n重点参数详解\n\nRestart&#x3D;always: 只要不是通过systemctl stop来停止服务，任何情况下都必须要重启服务，默认值为no\nRestartSec&#x3D;5: 重启间隔，比如某次异常后，等待5(s)再进行启动，默认值0.1(s)\nStartLimitInterval: 无限次重启，默认是10秒内如果重启超过5次则不再重启，设置为0表示不限次数重启\n\n(2).案例需求\n需求：有个业务，当程序因受到OOM而退出的时候，不希望自动重启（此时需要人工介入排查），其他情况下可以自动重启\n分析：OOM就是通过kill -9来杀进程，因此只要找到方法，告诉systemd当该服务遇到kill -9时候不自动重启即可\n(3).RestartPreventExitStatus参数\n查询man systemd.service发现，systemd的[Service]段落里支持一个参数，叫做RestartPreventExitStatus\n该参数从字面上看，意思是当符合某些退出状态时不要进行重启。\n该参数的值支持exit code和信号名2种，可写多个，以空格分隔，例如\nRestartPreventExitStatus=143 137 SIGTERM SIGKILL\n\n表示，当退出情况只要符合以下4种情况中任意一种时候，则不再进行重启\nexit code为143\nexit code为137\n信号为TERM\n信号为KILL\n五.注意事项(1)RestartPreventExitStatus与Restart的关系配置RestartPreventExitStatus&#x3D;后，并没有完全忽略Restart&#x3D;，而是指当退出情况与RestartPreventExitStatus&#x3D;匹配的时候，才忽略Restart&#x3D;，若没有匹配，根据Restart&#x3D;该怎么样还怎么样（具体详见后面的详细测试数据）\n(2)kill子进程会是什么情况若systemd启动的不是一个简单进程，而是会派生子进程的情况（比如执行shell脚本，shell脚本里启动多个程序），那么当另外开一个窗口通过 kill-信号测试时，会是什么情况呢，先贴出测试方法\nExecStart&#x3D;&#x2F;root&#x2F;mem改为ExecStart&#x3D;&#x2F;root&#x2F;mytest.sh\n&#x2F;root&#x2F;mytest.sh内容为\n\n #!/bin/bash\n sleep 100000 &amp;\n sleep 200000\n\n测试结果\n\n若kill 主进程PID（kill不带参数），则主进程状态为 code=killed,signal=TERM\n若kill -9 主进程PID，则主进程状态为 code=killed,signal=KILL\n若kill 最后一个子进程PID（kill不带参数），则systemd不认为是接收到信号，而是根据最后一个进程的exit code进行处理，此时主进程状态为 code=exited,status=143\n若kill -9 最后一个子进程PID，此时主进程状态为 code=exited,status=137\n\n(3))systemd 日志重定向\n[Unit]Description=agent server daemonAfter=network.target[Service]Type=simpleExecStart=/bin/sh -c &#x27;/usr/local/artifacts/agent/bin/-agent -c /usr/local/artifacts/agent/conf/cfg.agent.json 1&gt;&gt;/usr/local/artifacts/agent/logs/out.log 2&gt;&amp;1&#x27;Restart=on-failure [Install]WantedBy=multi-user.target\n\nThe End\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS之开启firewalld白名单","url":"/20241121/CentOS/a5061d2ddde3/","content":"CentOS之开启firewalld白名单​\tFirewalld 是可用于许多 Linux 发行版的防火墙管理解决方案，它充当 Linux 内核提供的 iptables 数据包过滤系统的前端。在本教程中，介绍如何为服务器设置防火墙，并使用 firewall-cmd 管理工具来管理防火墙添加 IP 白名单。\nfirewalld中的区域(zone)​\t区域(zone)基本上是一组规则，它们决定了允许哪些流量，具体取决于你对计算机所连接的网络的信任程度。为网络接口分配了一个区域，以指示防火墙应允许的行为。Firewalld 一般已经默认内置了 **9 个区域(zone)**，大部分情况下，这些已经足够使用，按从最不信任到最受信任的顺序为：\n\ndrop：最低信任级别。所有传入的连接都将被丢弃而不会回复，并且只能进行传出连接。\nblock：与上述类似，但不是简单地删除连接，而是使用 icmp-host-prohibitedor 和 icmp6-adm-prohibited 消息拒绝传入的请求。\npublic：表示不信任的公共网络。您不信任其他计算机，但可能会视情况允许选择的传入连接。默认情况下，此区域为激活状态。\nexternal：如果你使用防火墙作为网关，则为外部网络。将其配置为 NAT 转发，以便你的内部网络保持私有但可访问。\ninternal：external 区域的另一侧，用于网关的内部。这些计算机值得信赖，并且可以使用一些其他服务。\ndmz：用于 DMZ (DeMilitarized Zone) 中的计算机（将无法访问网络其余部分的隔离计算机），仅允许某些传入连接。\nwork：用于工作机。信任网络中的大多数计算机。可能还允许其他一些服务。\nhome：家庭环境。通常，这意味着您信任其他大多数计算机，并且将接受其他一些服务。\ntrusted：信任网络中的所有计算机。可用选项中最开放的，应谨慎使用。\n\n安装并启用防火墙Firewalld 是在某些 Linux 发行版上默认安装的，但有时候需$ sudo firewall-cmd–state要手动安装。CentOS 下的安装命令如下：\nyum install firewalld\n\n启用 Firewalld 服务并允许自启动\nsystemctl start firewalldsystemctl enable firewalld\n\n确认 Firewalld 服务是否正在运行\nfirewall-cmd --state\n\n返回 running 则表示已经运行；not running 则表示没有运行。\n查看当前的防火墙规则输入以下命令，可以看到当前选择哪个区域作为默认区域：\nfirewall-cmd --get-default-zone\n\n一般情况下会返回 public输入以下内容进行确认哪个区域处于激活状态：\nfirewall-cmd --get-active-zones\n\n一般情况下，在只有一个网卡的情况下会返回：\npublic  interfaces: eth0\n\n获取可用区域的列表，输入以下命令：\nfirewall-cmd --get-zones# 返回：# block dmz drop external home internal public trusted work\n\n通过指定 –zone 的 –list-all 参数，我们可以看到与区域关联的特定配置：\nfirewall-cmd --zone=home --list-all\n\n返回值：\nhome  target: default  icmp-block-inversion: no  interfaces:   sources:   services: dhcpv6-client mdns samba-client ssh  ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules:\n\n获取可用区域列表的详细信息，输入以下命令：\nfirewall-cmd --list-all-zones\n\n使用 Firewalld 配置 IP 白名单如前所述，Firewalld 有内置的区域，可以利用这些区域不同的特性，来简单快捷地配置 IP 白名单。具体做法就是，首先要收集你要允许的 IP 白名单列表，比如 Cloudflare 的所有IP范围:\n173.245.48.0/20103.21.244.0/22103.22.200.0/22103.31.4.0/22141.101.64.0/18108.162.192.0/18190.93.240.0/20188.114.96.0/20197.234.240.0/22198.41.128.0/17162.158.0.0/15104.16.0.0/12172.64.0.0/13131.0.72.0/22\n\n与此同时，你也要加入你自己的 IP 地址，否则白名单一旦生效，可能会将你阻挡在外而无法连接。\n将这些 IP 列表逐一加入 trusted 区域，使用命令如下：\nfirewall-cmd --permanent --zone=trusted --add-source=173.245.48.0/20firewall-cmd --permanent --zone=trusted --add-source=131.0.72.0/22\n\n使 trusted 区域设置生效，使用命令如下：\nfirewall-cmd --reload\n\n确认 trusted 区域是否设置正确，使用命令如下：\nfirewall-cmd --zone=trusted --list-all\n\n返回：\ntrusted (active)  target: ACCEPT  icmp-block-inversion: no  interfaces:   sources: 173.245.48.0/20 …… 131.0.72.0/22  services:   ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules:\n\n因为此时已经设置了 trusted 区域，所以还需要切换默认区域从 public 到 drop，以达到无视所有接入连接的目的。使用命令如下：\nfirewall-cmd --set-default-zone=drop\n\n再将默认网卡 eth0 分配给 drop 区域，使用命令如下：\nfirewall-cmd --permanent --zone=drop --change-interface=eth0\n\n使白名单最终生效，使用命令如下（注意：请再次确认你的所有 IP 都加入了 trusted 区域）：\nfirewall-cmd --reload\n\n至此，白名单设置正式生效。\n添加对外暴露的端口查看所有打开的端口\nfirewall-cmd --zone=public --list-ports\n\n添加开放端口\n# permanent永久生效，没有此参数重启后失效firewall-cmd --zone=public --add-port=80/tcp --permanent\n\n删除开放端口\nfirewall-cmd --zone=public --remove-port=80/tcp --permanent\n\n使开通的端口生效\nfirewall-cmd --reload\n\n\n\n本文来自安服仔\nEnd\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS免密登录","url":"/20241121/CentOS/ac7a3f1d0d51/","content":"CentOS免密登录#!/bin/shpass=root123yum install expect -yecho &quot;y&quot;|ssh-keygen -t rsa -P &quot;&quot; -f /root/.ssh/id_rsaiplist=&quot;k8s-master1 k8s-master2 k8s-master3&quot;for i in $&#123;iplist&#125;;doexpect -c  &quot;spawn ssh-copy-id -i /root/.ssh/id_rsa.pub root@$&#123;i&#125;expect &#123;    &quot;yes/no*&quot; &#123; send &quot;yes&quot;\\r;exp_continue&#125;    &quot;password*&quot; &#123; send &quot;$&#123;pass&#125;&quot;\\r;exp_continue&#125;    timeout &#123; &#125;&#125;&quot;doneexit","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS增加分区空间","url":"/20241121/CentOS/415017b0b5ea/","content":"CentOS增加分区空间CentOS增加home的分区空间给&#x2F;根目录1.umout home目录umount /home\n\n2.删除&#x2F;home所在的lv逻辑卷/dev/mapper/centos-home具体名称查看自己虚拟机\nlvremove /dev/mapper/centos-home\n\n3.扩展&#x2F;root所在的lv，增加700G/dev/mapper/centos-root具体名称查看自己虚拟机\nlvextend -L +700G  /dev/mapper/centos-root\n\n4.扩展&#x2F;root文件系统xfs_info /dev/mapper/centos-rootxfs_growfs /dev/mapper/centos-root\n\n5.重新创建home lvlvcreate -L 150G -n home centos\n\n6.创建文件系统mkfs.xfs  /dev/mapper/centos-home\n\n7.挂载mount  /dev/mapper/centos-home  /home\n\n8.查看df -h\n\nCentOS增加新硬盘给&#x2F;根目录1.建立新分区# /dev/sdb 是硬盘名称,fdisk -l查看fdisk /dev/sdb# 1.执行 fdisk /dev/sdb 对 sdb 进行分区# 2.输入 p 查看分区表，上图显示 sdb 并没有分区。（输入m获取帮助信息）# 3.接下来进行分区，输入 n 建立新分区，接着输入 p 选择主分区，分区号和扇区号默认即可，这样会将整个硬盘都添加到新分区中。# 4.如上图，分区添加成功。输入 w 写入硬盘。\n2.将硬盘写入系统# 注意文件系统格式：(此操作为格式化硬盘相应分区)# 1.文件系统格式xfsmkfs.xfs  /dev/sdb1# 2.文件系统格式ext4mkfs.ext4 /dev/sdb1\n3.创建物理卷pv# 查看物理卷pvpvdisplay# 创建物理卷pvpvcreate /dev/sdb1\n4.扩容卷组vg# 查看卷组vgdisplay#将新创建的 物理卷pv ：/dev/sdb1 追加到当前卷组里，也就是 &quot;centos&quot; 中，对应上面的 VG Name，扩容卷组。vgextend centos /dev/sdb1\n5.逻辑卷扩容 LV# 查看当前逻辑卷lvdisplay# 1.将硬盘所有的空间分配给/dev/centos/rootlvextend -l +100%FREE /dev/centos/root# 2.将硬盘的指定大小分配给/dev/centos/rootlvcreate -L 150G -n home centos\n6.扩容文件系统# 扩容文件系统xfs_growfs /dev/centos/root\n\n7.查看df -h\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS安装Greenplum5集群教程","url":"/20241121/Greenplum/ec69fc016f15/","content":"安装Greenplum5集群教程\ngreenplum的数据库是一个大规模并行处理(MPP)数据库服务器架构，该架构是专为管理大规模的数据仓库和商业智能设计的。\n\n\nGreenplum数据存储和处理大数据量，是通过跨多个服务器和主机分配数据和负载。Greenplum数据库是基于PostgreSQL8.2单个数据库的阵列，这些数据库像单个数据库一样共同工作。master主机是Greenplum数据库系统的入口点，它是客户端连接并提交SQL语句的接口数据库实例。Master协调系统中其他数据库实例（称为Segment）的工作负载，处理数据和存储。分段通过互连，Greenplum数据库的网络层互相通信和主控\n\n一.greenplum组件Greenplum数据库系统的主要组件以及与每个组件关联的硬件注意事项和概念：Greenplum Master，Segments和Interconnect。此外，系统可能具有用于数据加载的可选ETL主机和用于监视查询工作负载和性能的Greenplum性能监视器。\n1、基于postgresql\ngreenplum的数据库基于postgresql的开源技术，它本质上是几个postgresql数据库实例一起协作当做一个有凝聚力数据库管理系统（dbms）使用。postgresql的内部一杯修改和补充，以支持Greenplum数据库的并行架构。例如，该系统目录，优化器，查询执行器，和事务管理器组件以被修改和改进，以支持能够在所有并行的postgresql数据库实例同事执行查询。\n2、Interconnnect\nGreenplum Interconnect可使不同的Postgresql实例之间进行通信，并且是系统表现为一个逻辑数据库\n3、greenplum master\n3.1. master功能作用\n\nmaster是greenplu数据库系统的入口，数据库服务器进程接收客户端连接并处理系统用户发出的sql命令。\nmaster维护系统目录(一组包含有关greenplum数据库系统本身的元数据的系统表，但是master不包含任何用户数据。数据只保存在segment主机上。\nmaster验证客户端连接，处理传入的sql命令，在每个segment主机之间分配工作负载，协调每个segment返回的结果。并将结果返回客户端。\n\n3.2. standby\n\nstandby通过事务日志复制进程保持最新状态，该进程在standby上运行，并同步master和standby主机之间的数据。如果master主机失败，日志复制关闭；当standby处于活动状态时，日志复制重建上次成功提交事务时的master的状态。\n由于Master不包含任何用户数据，因此只需要在Master和Standby之间同步系统目录表。当这些表被更新时，更改会自动复制到Standby，以便始终与主站同步\n\n4、greenplum segment\n4.1、segment主要作用\n\nsegment主要是存储数据及数据查询处理。\n用户自定义的表(非系统表，master存储系统表)及索引分布在可用的segment；每个segment包含数据的不同。用户不能直接与grenplum数据系统的segment交互，而是通过master进行交互。\n在参考Greenplum数据库硬件配置中，每个segment主机的segment实例数由有效的CPU或CPU内核的数量决定。 例如，如果你的segment主机有两个双核处理器，则每个主机可能有两个或四个主要segment。 如果你的segment主机有三个四核处理器，则每个主机可能有三个，六个或十二个segment。性能测试将有助于确定所选硬件平台的最佳segment。\n\n4.2、segment冗余 (mirror)镜像\n\n配置mirror，当你的primary segment不可用时则mirror segment允许数据查询故障切换到镜像segment。\nmirror segment必须始终位于与primary 不同的主机上\n\n4.3、性能\n\nSegment主机执行大部分数据库处理，因此配置好Segment主机服务器，以便从你的Greenplum数据库系统获得最佳性能。Greenplum数据库的性能将与数组中最慢的Segment服务器一样快，也就是性能以最慢的Segment为准\n\n4.4、网络\n\n网络互连是greenplum 数据系统的网络层，当用户连接到数据库并发查询时，会在每个segment上创建进程处理该查询的工作。网络互连是指的segment 之间的进程通信所依赖的网络基础设施。默认情况下，greenplum数据库互连具有流量控制的UDP（用户数据报协议）协议发送消息。greenplum做了附加分组和检查不由UDP协议执行，性能和可扩展行超过TCP。\n\n5、 ETL主机加载数据\n\ngreenplum支持使用其外部表功能可以快速并行的数据加载。通过使用外部表与greenplum数据库的并行文件服务器(gpfdist)结合使用，管理员可以从其greenplum数据库系统实现最大的并行度和负载带宽。\n使用gpfdist文件服务器程序的一个优点是，它可以确保在从外部表数据文件读取时，您的Greenplum数据库系统中的所有Segment都被充分利用。\n\n6、Greenplum性能监视器\n\nGreenplum还提供可选的监控和管理工具，管理员可以安装并启用Greenplum数据库。 要使用Greenplum性能监视器，您的Greenplum数据库阵列中的每个主机都必须安装并启用数据收集代理。 当你开始Greenplum性能监视器，代理开始收集关于查询和系统利用率的数据，代理定期将数据发送到Greenplum Master（通常每15秒）。用户可以查询Greenplum Performance Monitor数据库，查看有效查询和历史查询和系统性能数据。 Greenplum Performance Monitor还具有图形化的基于Web的用户界面，用于查看这些性能指标并以其他方式管理其Greenplum\n\n7、postgresql 体系结构\npostgresql数据库主要五大部分组成：连接管理系统(系统控制器)、编译执行系统、存储管理系统、事务系统、系统表。以下模块功能介绍：\n\n1、 连接管理系统：机构外部操作对系统的请求，对操作请求进行预处理和分发，具有系统逻辑控制作用。\n2、编译执行系统：由查询编译器、查询执行器、DDL编译器组成，完成操作请求在数据库中的分析处理和转化工作，最终实现屋里存储介质数据的操作；\n3、存储管理系统：由索引管理器、内存管理器、外存管理器组成，负责存储和管理物理数据，提供对编译查询系统的支持；\n4、事务系统：包含 事务管理器、日志管理器、并发控制、锁管理器组成，日志管理器和事务管理器完成对操作请求处理的事务一致性支持，锁管理器和并发控制提供对并发访问数据的一致性支持；\n5、系统表：是postgresql数据库的元信息管理中心，包括数据库对象信息和数据库管理控制信息。系统表管理元数据信息，将postgresql数据库的各个模块有机的连接在一起，形成一个高效的数据管理系统。\n\n二.基础环境centos7.4 16核32G\n\n\n\nip\nhostname\n角色\n\n\n\n192.168.0.44\nmaster\nmaster、segment\n\n\n192.168.0.43\nsegment1\nsegment、standby\n\n\n192.168.0.40\nsegment2\nsegment\n\n\n三. 服务器配置参数更改\n1.添加hosts\n\ncat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.0.44 master192.168.0.43 segment1192.168.0.40 segment2 EOF\n\n\n2.配置tcp参数\n\n 修改/etc/sysctl.conf\ncat &gt;&gt; /etc/sysctl.conf  &lt;&lt;EOF kernel.shmmni = 4096kernel.shmall = 40000000000kernel.shmmax = 287194767360kernel.sem = 250 512000 100 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.defalut.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152#vm.overcommit_memory = 2     ###测试环境要取消这个，否则oracle启不来 ### 值为1\n\n\n注意：kernel.shmmax &#x3D; 5000000000 单位b（bit），值的大小为实际内存的50%。kernel.shmmax参数调整系统内存的50% \n\n 执行 sysctl -p  使资源文件生效；\n\n3.添加limits.conf\n\ncat &gt;&gt; /etc/security/limits.conf &lt;&lt;EOF* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072EOF\n\n\n4.修改I&#x2F;O调度算法\n\necho deadline &gt; /sys/block/vdb/queue/schedulergrubby --update-kernel=ALL --args=&quot;transparent_hugepage=never&quot;\n\n\n5.设置gpadmin用户，密码\n\nuseradd  gpadmin -d /home/gpadminusermod -L gpadmin#提权visudo  gpadmin    ALL=(ALL)       ALLgpadmin    ALL=(ALL)       NOPASSWD:ALL#创建密码echo &quot;gpadmin&quot; | passwd --stdin gpadmin注：注意设置密码为了后面gpssh-exkeys -f list 使用\n\n四.下载&amp;安装\n1.下载安装\n\n名称                     版本\n操作系统                 CentOS  64bit\ngreenplum             greenplum-db-appliance-5.10.2-rhel6-x86_64.zip\n文件系统                 ext4\n 链接： \t\t\t\t\t\n 提取码：\t\t\t\t7qzv \n 首先在master节点上安装，将greenplum-db-appliance-5.10.2-rhel6-x86_64.zip上传到&#x2F;home&#x2F;gpadmin目录下 \nsu - gpadmincd /home/gpadminunzip greenplum-db-appliance-5.10.2-rhel6-x86_64.zip./greenplum-db-appliance-5.10.2-rhel6-x86_64.bin\n\n\n2.安装目录相关文件授权\n\nchown -R gpadmin.gpadmin /usr/local/greenplum-db* #需要在每个节点执行授权chown gpadmin.gpadmin /usr/local \n\n 以下开始，进入gpadmin用户下操作 ：\n#hostlist自定义的,一般为hostnamecat &gt; list &lt;&lt;EOFmastersegment1segment2EOF\n\n\n3.添加环境变量\n\ncat /usr/local/greenplum-db/greenplum_path.sh &gt;&gt;~/.bashrc cat &gt;&gt;  ~/.bashrc &lt;&lt;EOFMASTER_DATA_DIRECTORY=/home/gpdata/master/gpseg-1export MASTER_DATA_DIRECTORYEOFsource ~/.bashrc \n\n\n4.配置免密登陆\n\ngpssh-exkeys -f list\n\n\n5.检查环境（可以忽略）\n\ngpcheck -f list\n\n\n6.在segment节点安装greenplum-db-5.10.2-rhel7-x86_64\n\ngpseginstall -f list\n\n\n7.创建数据目录\n\ngpssh -f list -e &quot; mkdir -p /home/gpdata/&#123;primary,mirror,master&#125;&quot;gpssh -f list -e &quot; sudo chown gpadmin.gpadmin /home/gpdata&quot;# 创建master目录mkdir -p /opt/data/master\t\n\n\n8.创建初始化config文件\n\ncat &gt; gpinitsystem_config &lt;&lt;EOF###需要的参数，目录可以根据需要更改##ARRAY_NAME=&quot;Greenplum Data Platform&quot;SEG_PREFIX=gpsegPORT_BASE=40000declare -a DATA_DIRECTORY=(/home/gpdata/primary /home/gpdata/primary /home/gpdata/primary)MASTER_HOSTNAME=master    #修改为主节点主机名MASTER_DIRECTORY=/home/gpdata/masterMASTER_PORT=5432TRUSTED_SHELL=sshCHECK_POINT_SEGMENTS=8ENCODING=UNICODEdeclare -a MIRROR_DATA_DIRECTORY=(/home/gpdata/mirror /home/gpdata/mirror /home/gpdata/mirror)EOF\n\n\n9.初始化数据库\n\ngpinitsystem -a -c gpinitsystem_config -h list\n\n重新加载配置文件gpstop -u#启动服务gpstart  -a#停止服务gpstop -a\n\n五. 配置修改简单实用-添加允许连接的网段\n\n注意：根据服务器的具体情况进行配置修改 \n\n 示例：host 库名 用户名 ip&#x2F;24 trust \ncat /home/gpdata/master/gpseg-1/pg_hba.confhost\tall\tgpadmin\t192.168.0.43/32\ttrusthost\tall\tall\t0.0.0.0/0\ttrustlocal    all         gpadmin         identhost     all         gpadmin         127.0.0.1/28    trusthost     all         gpadmin         192.168.0.44/32       trust\n\n六. 登录测试[root@slave3 gpadmin]# psql -d postgrespsql: FATAL:  no pg_hba.conf entry for host &quot;[local]&quot;, user &quot;root&quot;, database &quot;postgres&quot;, SSL off[root@slave3 gpadmin]# su - gpadminLast login: Mon Jan 25 13:45:50 CST 2021 on pts/0[gpadmin@slave3 ~]$  psql -d postgrespsql (8.3.23)Type &quot;help&quot; for help.postgres=# \\l                    List of databases     Name     |  Owner  | Encoding |  Access privileges  --------------+---------+----------+--------------------- 430000_RMDSD | gpadmin | UTF8     |  postgres     | gpadmin | UTF8     |  template0    | gpadmin | UTF8     | =c/gpadmin                                             : gpadmin=CTc/gpadmin template1    | gpadmin | UTF8     | =c/gpadmin                                             : gpadmin=CTc/gpadmin(4 rows)\n\n修改密码\nalter role gpadmin with password &#x27;gpadmin&#x27;; \n\n七. 授权根据不同的库创建不同的用户名密码xnreport 8ql6,reportcrm 8ql6,crmxncc 8ql6,xncc\n-- 给database授权create user superuser xnreport password &#x27;8ql6,report&#x27;;grant all privileges on databases xn_report to xnreport;-- 给table授权grant all privileges on table table_name to xnreport;\n\n八. 添加mirror# 1、在所有需要添加mirror的主机，创建存放mirror的数据目录（如果已经创建，忽略此操作）mkdir /opt/data/mirror# 以下再master操作# 2、生成配置文件gpaddmirrors -o addmirror# 添加mirrorgpaddmirrors -a -i addmirror -v \n\n九. 添加standby-- 1、在standy节点上创建master目录-- 2、在master上执行gpinitstandby -a -s segment1\n\npostgres=# select * from pg_stat_replication ; procpid | usesysid | usename | application_name | client_addr | client_port |         backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state---------+----------+---------+------------------+-------------+-------------+-------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------    6931 |       10 | gpadmin | walreceiver      | 192.168.0.43 |       48800 | 2018-12-24 11:51:05.376815+08 | streaming | 0/14000C18    | 0/14000C18     | 0/14000C18     | 0/14000C18      |             1 | syncpostgres=# select a.dbid,a.content,a.role,a.port,a.hostname,b.fsname,c.fselocation from gp_segment_configuration a,pg_filespace b,pg_filespace_entry c where a.dbid=c.fsedbid and b.oid=c.fsefsoid order by content;\n\nEND\n原文链接\n","categories":["Greenplum"],"tags":["CentOS","Greenplum"]},{"title":"CentOS安装Greenplum教程-由单机master到集群","url":"/20241121/Greenplum/0723f01d081e/","content":"CentOS安装Greenplum教程-由单机master到集群\nGreenplum的数据库是一个大规模并行处理(MPP)数据库服务器架构，该架构是专为管理大规模的数据仓库和商业智能设计的。Greenplum数据存储和处理大数据量，是通过跨多个服务器和主机分配数据和负载。Greenplum数据库是基于PostgreSQL8.2单个数据库的阵列，这些数据库像单个数据库一样共同工作。master主机是Greenplum数据库系统的入口点，它是客户端连接并提交SQL语句的接口数据库实例。Master协调系统中其他数据库实例（称为Segment）的工作负载，处理数据和存储。分段通过互连，Greenplum数据库的网络层互相通信和主控\n\ngreenplum组件Greenplum数据库系统的主要组件以及与每个组件关联的硬件注意事项和概念：Greenplum Master，Segments和Interconnect。此外，系统可能具有用于数据加载的可选ETL主机和用于监视查询工作负载和性能的Greenplum性能监视器。\n1、基于postgresql\ngreenplum的数据库基于postgresql的开源技术，它本质上是几个postgresql数据库实例一起协作当做一个有凝聚力数据库管理系统（dbms）使用。postgresql的内部一杯修改和补充，以支持Greenplum数据库的并行架构。例如，该系统目录，优化器，查询执行器，和事务管理器组件以被修改和改进，以支持能够在所有并行的postgresql数据库实例同事执行查询。\n2、Interconnnect\nGreenplum Interconnect可使不同的Postgresql实例之间进行通信，并且是系统表现为一个逻辑数据库\n3、greenplum master\n3.1. master功能作用\n\nmaster是greenplu数据库系统的入口，数据库服务器进程接收客户端连接并处理系统用户发出的sql命令。\nmaster维护系统目录(一组包含有关greenplum数据库系统本身的元数据的系统表，但是master不包含任何用户数据。数据只保存在segment主机上。\nmaster验证客户端连接，处理传入的sql命令，在每个segment主机之间分配工作负载，协调每个segment返回的结果。并将结果返回客户端。\n\n3.2. standby\n\nstandby通过事务日志复制进程保持最新状态，该进程在standby上运行，并同步master和standby主机之间的数据。如果master主机失败，日志复制关闭；当standby处于活动状态时，日志复制重建上次成功提交事务时的master的状态。\n由于Master不包含任何用户数据，因此只需要在Master和Standby之间同步系统目录表。当这些表被更新时，更改会自动复制到Standby，以便始终与主站同步\n\n4、greenplum segment\n4.1、segment主要作用\n\nsegment主要是存储数据及数据查询处理。\n用户自定义的表(非系统表，master存储系统表)及索引分布在可用的segment；每个segment包含数据的不同。用户不能直接与grenplum数据系统的segment交互，而是通过master进行交互。\n在参考Greenplum数据库硬件配置中，每个segment主机的segment实例数由有效的CPU或CPU内核的数量决定。 例如，如果你的segment主机有两个双核处理器，则每个主机可能有两个或四个主要segment。 如果你的segment主机有三个四核处理器，则每个主机可能有三个，六个或十二个segment。性能测试将有助于确定所选硬件平台的最佳segment。\n\n4.2、segment冗余 (mirror)镜像\n\n配置mirror，当你的primary segment不可用时则mirror segment允许数据查询故障切换到镜像segment。\nmirror segment必须始终位于与primary 不同的主机上\n\n4.3、性能\n\nSegment主机执行大部分数据库处理，因此配置好Segment主机服务器，以便从你的Greenplum数据库系统获得最佳性能。Greenplum数据库的性能将与数组中最慢的Segment服务器一样快，也就是性能以最慢的Segment为准\n\n4.4、网络\n\n网络互连是greenplum 数据系统的网络层，当用户连接到数据库并发查询时，会在每个segment上创建进程处理该查询的工作。网络互连是指的segment 之间的进程通信所依赖的网络基础设施。默认情况下，greenplum数据库互连具有流量控制的UDP（用户数据报协议）协议发送消息。greenplum做了附加分组和检查不由UDP协议执行，性能和可扩展行超过TCP。\n\n5、 ETL主机加载数据\n\ngreenplum支持使用其外部表功能可以快速并行的数据加载。通过使用外部表与greenplum数据库的并行文件服务器(gpfdist)结合使用，管理员可以从其greenplum数据库系统实现最大的并行度和负载带宽。\n使用gpfdist文件服务器程序的一个优点是，它可以确保在从外部表数据文件读取时，您的Greenplum数据库系统中的所有Segment都被充分利用。\n\n6、Greenplum性能监视器\n\nGreenplum还提供可选的监控和管理工具，管理员可以安装并启用Greenplum数据库。 要使用Greenplum性能监视器，您的Greenplum数据库阵列中的每个主机都必须安装并启用数据收集代理。 当你开始Greenplum性能监视器，代理开始收集关于查询和系统利用率的数据，代理定期将数据发送到Greenplum Master（通常每15秒）。用户可以查询Greenplum Performance Monitor数据库，查看有效查询和历史查询和系统性能数据。 Greenplum Performance Monitor还具有图形化的基于Web的用户界面，用于查看这些性能指标并以其他方式管理其Greenplum\n\n7、postgresql 体系结构\npostgresql数据库主要五大部分组成：连接管理系统(系统控制器)、编译执行系统、存储管理系统、事务系统、系统表。以下模块功能介绍：\n\n1、 连接管理系统：机构外部操作对系统的请求，对操作请求进行预处理和分发，具有系统逻辑控制作用。\n2、编译执行系统：由查询编译器、查询执行器、DDL编译器组成，完成操作请求在数据库中的分析处理和转化工作，最终实现屋里存储介质数据的操作；\n3、存储管理系统：由索引管理器、内存管理器、外存管理器组成，负责存储和管理物理数据，提供对编译查询系统的支持；\n4、事务系统：包含 事务管理器、日志管理器、并发控制、锁管理器组成，日志管理器和事务管理器完成对操作请求处理的事务一致性支持，锁管理器和并发控制提供对并发访问数据的一致性支持；\n5、系统表：是postgresql数据库的元信息管理中心，包括数据库对象信息和数据库管理控制信息。系统表管理元数据信息，将postgresql数据库的各个模块有机的连接在一起，形成一个高效的数据管理系统。\n\n安装单机master环境：\n\nGreenplum6\nCentos7\n\n\n\n\nip地址\nhostname\n角色\n\n\n\n192.168.137.31\nmaster\nmaster\n\n\n192.168.137.32\nsegment1\nsegment\n\n\n192.168.137.33\nsegment2\nsegment\n\n\n一.Greenplum 6.4 下载Greenplum下载地址\n二.安装单机master环境准备2.1 关闭防火墙\nsystemctl stop firewalldsystemctl disable firewalld\n\n 2.2 关闭SELINUX  \nsetenforce 0sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# 或者vi /etc/selinux/configSELINUX=disabled\n\n 2.3 改机器名 \nhostnamectl set-hostname 主机名hostnamectl set-hostname master\n\n 2.4 更改hosts文件\ncat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.137.31 master masterEOF\n\n 2.5创建gpaadmin用户和用户组 \ngroupadd -g 3030 gpadminuseradd -u 3030 gpadmin -g gpadmin -d /home/gpadmin# 更改gpadmin密码echo &quot;gpadmin&quot; | passwd --stdin gpadmin\n\n 2.6配置内核参数 \ncat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 250 512000 100 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 10000 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2EOF\n\n# 使配置生效sysctl -p\n\n  2.7 更改&#x2F;etc&#x2F;security&#x2F;limits.conf \ncat &gt;&gt;/etc/security/limits.conf &lt;&lt; EOF*soft nofile 65536*hard nofile 65536*soft nproc 131072*hard nproc 131072EOF\n\n 2.8 CentOS7需要加入服务logind \necho &quot;RemoveIPC=no&quot; &gt;&gt; /etc/systemd/logind.confservice systemd-logind restart\n\n 2.9 设置network \nvi /etc/sysconfig/network# 添加：networking=yeshostname=master  # 不区分大小写\n\n 2.10 重启服务器reboot \nreboot\n\n三.安装GreenPlum安装方式:\n\nrpm包 - 本次用rpm安装\nzip压缩包\nbin二进制包\n\n 3.1 gp6安装包上传到服务器并安装 \n# 上传到master服务器上，并安装rpm包yum -y install greenplum-db-6.1.0-rhel6-x86_64.rpm\n\n#默认安装到/usr/local下，授权给gpadminchown -R gpadmin /usr/local/greenplum*chgrp -R gpadmin /usr/local/greenplum*\n\n# 加载greenplum环境source /usr/local/greenplum-db/greenplum_path.sh\n\n 3.2创建instance需要的目录 \n# 创建目录mkdir -p /data/gpdatamkdir -p /data/gpdata/mastermkdir -p /data/gpdata/gp1mkdir -p /data/gpdata/gp2mkdir -p /data/gpdata/gp3mkdir -p /data/gpdata/gp4\n\n# 修改目录属主chown -R gpadmin:gpadmin /data/gpdatachown -R gpadmin:gpadmin /data/gpdata/masterchown -R gpadmin:gpadmin /data/gpdata/gp*\n\n 3.3切换用户gpadmin \n# 切换到gpadminsu - gpadmincd\n\n 3.4 &#x2F;home&#x2F;gpadmin&#x2F;.bash_profile文件添加以下内容： \ncat &gt;&gt; /home/gpadmin/.bash_profile &lt;&lt; EOF# 确认安装路径是否相同，脚本是否同名source /usr/local/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/data/gpdata/master/gpseg-1export PGPORT=2345export PGUSER=gpadminexport PGDATABASE=gpdbEOF\n\n 3.5 &#x2F;home&#x2F;gpadmin&#x2F;.bashrc文件添加同3.4的内容 \ncat &gt;&gt; /home/gpadmin/.bashrc &lt;&lt; EOF# 确认安装路径是否相同，脚本是否同名source /usr/local/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/data/gpdata/master/gpseg-1export PGPORT=2345export PGUSER=gpadminexport PGDATABASE=gpdbEOF\n\n 3.6设置子节点的host \n# 添加一行master（单机版只有一个host）vi all_hosts_filemaster\n\n 3.7 权限互通 \n# 免密登录gpssh-exkeys -f all_hosts_file\n\n 3.8 编辑gp初始化文件 \nvi initgp_config# 写入:SEG_PREFIX=gpsegPORT_BASE=33000declare -a DATA_DIRECTORY=(/data/gpdata/gp1 /data/gpdata/gp2 /data/gpdata/gp3 /data/gpdata/gp4)MASTER_HOSTNAME=masterMASTER_PORT=2345MASTER_DIRECTORY=/data/gpdata/masterDATABASE_NAME=gpdb\n\n 3.9设置节点服务器 \nvi seg_hosts_file# 写入：本例单机，只有master这一台master\n\n\n\n四.初始化GreenPlum 4.1 初始化命令 \ngpinitsystem -c initgp_config -h seg_hosts_file\n\n\n服务器初始化没有设置分配swap空间，会初始化失败，分配好swap空间即可 \n\n五.连接GP 5.1psql登录修改密码 \nsu - gpadminpsql -p 2345#  修改数据库密码alter role gpadmin with password &#x27;gpadmin&#x27;;退出： \\q\n\n 5.2远程连接配置 \nvi /data/gpdata/master/gpseg-1/postgresql.conf# 修改：#listen_addresses = &#x27;*&#x27;，去#注释vi /data/gpdata/master/gpseg-1/pg_hba.conf# 添加：host     all         gpadmin         0.0.0.0/0               md5\n\n 5.3重新加载配置文件 \ngpstop -u\n\n\n\n5.4 其他启停命令\ngpstart #正常启动gpstop #正常关闭gpstop -M fast #快速关闭gpstop –r #重启\n\n\n\n安装master后添加segment节点一.更改segment系统配置 1.1更改机器名为segment1 \nhostnamectl set-hostname segment1\n\n 1.2修改&#x2F;etc&#x2F;hosts文件 \nvi /etc/hosts# 新增192.168.137.32 segment1 segment1\n\n# 从master复制到segmentscp /etc/hosts segment1:/etc\n\n 1.3关闭selinux及防火墙 \n 参考前文 \n# selinux文件可用scp命令由master复制到segment节点scp /etc/selinux/config segment1:/etc/selinux\n\n 1.4修改&#x2F;etc&#x2F;sysconfig&#x2F;network文件(进入每台segment中修改) \nvi /etc/sysconfig/network# Created by anacondanetworking=yeshostname=segment1 # 主机名\n\n 1.5复制master的&#x2F;etc&#x2F;sysctl.conf文件到segment \nscp /etc/sysctl.conf segment1:/etc/sysctl -p # segment 执行\n\n 1.6修改&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf文件并复制到segment \nvi /etc/security/limits.d/20-nproc.conf# Default limit for number of user&#x27;s processes to prevent# accidental fork bombs.# See rhbz #432903 for reasoning.*          soft    nproc     4096root       soft    nproc     unlimited\n\n# 复制到segmentscp /etc/security/limits.d/20-nproc.conf segment1:/etc/security/limits.d/\n\n 1.7复制&#x2F;etc&#x2F;security&#x2F;limits.conf到segment节点 \nscp /etc/security/limits.conf  segment1:/etc/security/\n\n 1.8创建gpadmin组及用户 \ngroupadd -g 3030 gpadminuseradd -u 3030 gpadmin -g gpadmin -d /home/gpadmin# 更改gpadmin密码echo &quot;gpadmin&quot; | passwd --stdin gpadmin\n\n 1.9配置扩展segment节点需要的文件 \n# 修改master主机gpadmin目录下的all_hosts_file,将所有节点的主机名都加进去vi /home/gpadmin/all_hosts_filemastersegment1segment2...\n\n# 修改master主机gpadmin目录下的seg_hosts_file,将segment节点的主机名加进去vi /home/gpadmin/seg_hosts_filesegment1segment2...\n\n# 新增hosts_expand文件,此次增加的segment# 将当前需要新增的节点主机名加入(如此次需要加入segment1,就将segment1写入)vi /home/gpadmin/hosts_expandsegment1...\n\n1.10 创建需要的目录 \n# 创建目录mkdir -p /data/gpdatamkdir -p /data/gpdata/mastermkdir -p /data/gpdata/gp1mkdir -p /data/gpdata/gp2mkdir -p /data/gpdata/gp3mkdir -p /data/gpdata/gp4\n\n# 修改目录属主chown -R gpadmin:gpadmin /data/gpdatachown -R gpadmin:gpadmin /data/gpdata/masterchown -R gpadmin:gpadmin /data/gpdata/gp*\n\n二.配置SSH免密登录 2.1master主机生成密钥  \n# 注意执行用户为gpadminssh-keygen -t rsa# 一直回车Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): /root/.ssh/id_rsa already exists.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:8Z+R0Truk1NIVC4Tl6w5VTgP2WrjhM3ZtmYu2Ykn8v8 root@masterThe key&#x27;s randomart image is:+---[RSA 2048]----+|            .oo*.||            .=O .||        .  .+BoB ||         o  **O +||        S ..=* o.||           o.+o+ ||            +oB .||           o+= = ||            ++=.E|+----[SHA256]-----+\n\n\nid_rsa为私钥，id_rsa.pub为公钥 \n\n 2.2将公钥复制到segment的 ~&#x2F;.ssh&#x2F;authorized_keys文件(.ssh目录及authorized_keys可能要提前创建) \n# 注意执行用户为gpadminscp ~/.ssh/id_rsa.pub segment1:~/.ssh/authorized_keys\n\n 2.3设置目录权限 \n# gpadmin用户的.ssh目录的权限必须是700(非常重要)chmod 700 .ssh# gpadmin用户的.ssh/authorized_keys文件权限必须是600chmod 600 .ssh/authorized_keys\n\n 2.4测试节点之间是否连通 \ngpssh-exkeys -f all_hosts_file[gpadmin@master ~]$ gpssh-exkeys -f all_hosts_file[STEP 1 of 5] create local ID and authorize on local host  ... /home/gpadmin/.ssh/id_rsa file exists ... key generation skipped[STEP 2 of 5] keyscan all hosts and update known_hosts file[STEP 3 of 5] retrieving credentials from remote hosts  ... send to segment1[STEP 4 of 5] determine common authentication file content[STEP 5 of 5] copy authentication files to all remote hosts  ... finished key exchange with segment1[INFO] completed successfully# 最后出现completed successfully即证明节点之间已连通\n\n三.在segment安装GreenPlum 3.1在segment安装GreenPlum的rpm包 \n# 上传到所有segment服务器上，并安装rpm包yum -y install greenplum-db-6.1.0-rhel6-x86_64.rpm# 或者用rpm命令安装rpm -ivh greenplum-db-6.1.0-rhel6-x86_64.rpm\n\n# 切换gpadmin用户,将segment节点/usr/local的读权限给到gpadmin,否则会报错chown -R gpadmin:gpadmin /usr/local/greenplum-db*\n\n四.环境配置 4.1复制&#x2F;home&#x2F;gpadmin&#x2F;.bash_profile到segment \nscp /home/gpadmin/.bash_profile segment1:/home/gpadmin/scp /home/gpadmin/.bash_profile segment2:/home/gpadmin/...\n\n 4.2使环境变量生效 \nsu - gpadminsource .bash_profile\n\n 4.3复制&#x2F;home&#x2F;gpadmin&#x2F;.bashrc到segment \nscp /home/gpadmin/.bashrc segment1:/home/gpadmin/scp /home/gpadmin/.bashrc segment2:/home/gpadmin/...\n\n五.使用gpexpand函数增加segment节点 5.1gpexpand -f hosts_expand \n[gpadmin@master ~]$ gpexpand -f hosts_expand20210514:23:06:59:044510 gpexpand:master:gpadmin-[INFO]:-local Greenplum Version: &#x27;postgres (Greenplum Database) 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c&#x27;20210514:23:06:59:044510 gpexpand:master:gpadmin-[INFO]:-master Greenplum Version: &#x27;PostgreSQL 9.4.24 (Greenplum Database 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c) on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 6.4.0, 64-bit compiled on Apr 21 2021 19:24:38&#x27;20210514:23:06:59:044510 gpexpand:master:gpadmin-[INFO]:-Querying gpexpand schema for current expansion stateSystem Expansion is used to add segments to an existing GPDB array.gpexpand did not detect a System Expansion that is in progress.Before initiating a System Expansion, you need to provision and burn-inthe new hardware.  Please be sure to run gpcheckperf to make sure thenew hardware is working properly.Please refer to the Admin Guide for more information.Would you like to initiate a new System Expansion Yy|Nn (default=N):&gt; y20210514:23:07:00:044510 gpexpand:master:gpadmin-[ERROR]:-gpexpand failed: Hosts file hosts_gpexpand not found Exiting...20210514:23:07:00:044510 gpexpand:master:gpadmin-[INFO]:-Shutting down gpexpand...[gpadmin@master ~]$ gpexpand -f hosts_expand 20210514:23:07:10:044551 gpexpand:master:gpadmin-[INFO]:-local Greenplum Version: &#x27;postgres (Greenplum Database) 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c&#x27;20210514:23:07:10:044551 gpexpand:master:gpadmin-[INFO]:-master Greenplum Version: &#x27;PostgreSQL 9.4.24 (Greenplum Database 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c) on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 6.4.0, 64-bit compiled on Apr 21 2021 19:24:38&#x27;20210514:23:07:10:044551 gpexpand:master:gpadmin-[INFO]:-Querying gpexpand schema for current expansion stateSystem Expansion is used to add segments to an existing GPDB array.gpexpand did not detect a System Expansion that is in progress.Before initiating a System Expansion, you need to provision and burn-inthe new hardware.  Please be sure to run gpcheckperf to make sure thenew hardware is working properly.Please refer to the Admin Guide for more information.Would you like to initiate a new System Expansion Yy|Nn (default=N):&gt; y    By default, new hosts are configured with the same number of primary    segments as existing hosts.  Optionally, you can increase the number    of segments per host.    For example, if existing hosts have two primary segments, entering a value    of 2 will initialize two additional segments on existing hosts, and four    segments on new hosts.  In addition, mirror segments will be added for    these new primary segments if mirroring is enabled.    How many new primary segments per host do you want to add? (default=0):&gt; 4Enter new primary data directory 1:&gt; /data/gpdata/gp1Enter new primary data directory 2:&gt; /data/gpdata/gp2Enter new primary data directory 3:&gt; /data/gpdata/gp3  Enter new primary data directory 4:&gt; /data/gpdata/gp4Generating configuration file...20210514:23:08:27:044551 gpexpand:master:gpadmin-[INFO]:-Generating input file...Input configuration file was written to &#x27;gpexpand_inputfile_20210514_230827&#x27;.Please review the file and make sure that it is correct then re-runwith: gpexpand -i gpexpand_inputfile_20210514_230827                20210514:23:08:27:044551 gpexpand:master:gpadmin-[INFO]:-Exiting...\n\n\n 成功会显示已生成gpexpand_inoutfile开头的文件 \n\n 5.2修改gpexpand_inputfile_20xxx(每个节点保留4行) \nsegment1|segment1|33000|/data/gpdata/gp1/gpseg4|6|4|psegment1|segment1|33001|/data/gpdata/gp2/gpseg5|7|5|psegment1|segment1|33002|/data/gpdata/gp3/gpseg6|8|6|psegment1|segment1|33003|/data/gpdata/gp4/gpseg7|9|7|p# 当前只增加一个节点，所以只保留前面4行master|master|33004|/data/gpdata/gp1/gpseg8|10|8|pmaster|master|33005|/data/gpdata/gp2/gpseg9|11|9|pmaster|master|33006|/data/gpdata/gp3/gpseg10|12|10|pmaster|master|33007|/data/gpdata/gp4/gpseg11|13|11|psegment1|segment1|33004|/data/gpdata/gp1/gpseg12|14|12|psegment1|segment1|33005|/data/gpdata/gp2/gpseg13|15|13|psegment1|segment1|33006|/data/gpdata/gp3/gpseg14|16|14|psegment1|segment1|33007|/data/gpdata/gp4/gpseg15|17|15|p\n\n\n 剩下的都可删掉，否则gpexpand -i 命令会失败,报错 \n\n 5.3gpexpand -i 脚本文件 \n[gpadmin@master ~]$ gpexpand -i gpexpand_inputfile_20210514_23082720210514:23:11:57:044991 gpexpand:master:gpadmin-[INFO]:-local Greenplum Version: &#x27;postgres (Greenplum Database) 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c&#x27;20210514:23:11:57:044991 gpexpand:master:gpadmin-[INFO]:-master Greenplum Version: &#x27;PostgreSQL 9.4.24 (Greenplum Database 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c) on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 6.4.0, 64-bit compiled on Apr 21 2021 19:24:38&#x27;20210514:23:11:57:044991 gpexpand:master:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state20210514:23:11:58:044991 gpexpand:master:gpadmin-[INFO]:-Heap checksum setting consistent across cluster20210514:23:11:58:044991 gpexpand:master:gpadmin-[INFO]:-Syncing Greenplum Database extensions...20210514:23:12:02:044991 gpexpand:master:gpadmin-[INFO]:-&#123;&#x27;segment1&#x27;: &#x27;/data/gpdata/gp1/gpseg4:33000:true:false:6:4::-1:,/data/gpdata/gp2/gpseg5:33001:true:false:7:5::-1:,/data/gpdata/gp3/gpseg6:33002:true:false:8:6::-1:,/data/gpdata/gp4/gpseg7:33003:true:false:9:7::-1:&#x27;&#125;20210514:23:12:19:044991 gpexpand:master:gpadmin-[INFO]:-Cleaning up temporary template files20210514:23:12:19:044991 gpexpand:master:gpadmin-[INFO]:-Cleaning up databases in new segments.20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Unlocking catalog20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Unlocked catalog20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Creating expansion schema20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database template120210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database postgres20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database gpdb20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-************************************************20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Initialization of the system expansion complete.20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-To begin table expansion onto the new segments20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-rerun gpexpand20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-************************************************20210514:23:12:21:044991 gpexpand:master:gpadmin-[INFO]:-Exiting...\n\n\n (出现Initialization of the system expansion complete即增加节点成功) \n\n5.4删除增加节点生成的schema\n\n (不删除以后则无法增加节点)\n\n[gpadmin@master ~]$ gpexpand -c20210515:00:49:17:051000 gpexpand:master:gpadmin-[INFO]:-local Greenplum Version: &#x27;postgres (Greenplum Database) 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c&#x27;20210515:00:49:17:051000 gpexpand:master:gpadmin-[INFO]:-master Greenplum Version: &#x27;PostgreSQL 9.4.24 (Greenplum Database 6.16.0 build commit:5650be2b79197fed564dca8d734d10f2a76b876c) on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 6.4.0, 64-bit compiled on Apr 21 2021 19:24:38&#x27;20210515:00:49:17:051000 gpexpand:master:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state20210515:00:49:18:051000 gpexpand:master:gpadmin-[WARNING]:-Expansion has not yet completed.  Removing the expansion20210515:00:49:18:051000 gpexpand:master:gpadmin-[WARNING]:-schema now will leave the following tables unexpanded:20210515:00:49:18:051000 gpexpand:master:gpadmin-[WARNING]:-20210515:00:49:18:051000 gpexpand:master:gpadmin-[WARNING]:-These tables will have to be expanded manually by setting20210515:00:49:18:051000 gpexpand:master:gpadmin-[WARNING]:-the distribution policy using the ALTER TABLE command.Are you sure you want to drop the expansion schema? Yy|Nn (default=N):&gt; yDo you want to dump the gpexpand.status_detail table to file? Yy|Nn (default=Y):&gt; y20210515:00:49:22:051000 gpexpand:master:gpadmin-[INFO]:-Dumping gpexpand.status_detail to /home/gpdata/master/gpseg-1/gpexpand.status_detail20210515:00:49:22:051000 gpexpand:master:gpadmin-[INFO]:-Removing gpexpand schema20210515:00:49:22:051000 gpexpand:master:gpadmin-[INFO]:-Cleanup Finished.  exiting...\n\n 5.5重分布表 \ngpexpand -d 60:00:00\n\n\n (出现completed successfully即重分布成功) \n\n 5.6失败回滚 \ngpexpand -r\n\n 5.7进入GP查询节点信息 \nsu - gpadmin# 登录greenplum,注意端口(前面的初始化配置文件设置的)psql -p 2345# 执行sql查询集群状态select * from gp_segment_configuration;\n\ngpdb=# select * from gp_segment_configuration; dbid | content | role | preferred_role | mode | status | port  | hostname | address  |           datadir           ------+---------+------+----------------+------+--------+-------+----------+----------+-----------------------------    1 |      -1 | p    | p              | n    | u      |  2345 | master   | master   | /data/gpdata/master/gpseg-1    2 |       0 | p    | p              | n    | u      | 33000 | master   | master   | /data/gpdata/gp1/gpseg0    3 |       1 | p    | p              | n    | u      | 33001 | master   | master   | /data/gpdata/gp2/gpseg1    4 |       2 | p    | p              | n    | u      | 33002 | master   | master   | /data/gpdata/gp3/gpseg2    5 |       3 | p    | p              | n    | u      | 33003 | master   | master   | /data/gpdata/gp4/gpseg3    6 |       4 | p    | p              | n    | u      | 33000 | segment1 | segment1 | /data/gpdata/gp1/gpseg4    7 |       5 | p    | p              | n    | u      | 33001 | segment1 | segment1 | /data/gpdata/gp2/gpseg5    8 |       6 | p    | p              | n    | u      | 33002 | segment1 | segment1 | /data/gpdata/gp3/gpseg6    9 |       7 | p    | p              | n    | u      | 33003 | segment1 | segment1 | /data/gpdata/gp4/gpseg7(9 rows)\n\n完\n原文链接\n","categories":["CentOS","Greenplum"],"tags":["CentOS","Greenplum"]},{"title":"CentOS安装Oracle11G","url":"/20241121/Oracle/231d1d6a5934/","content":"CentOS安装Oracle11G一.下载上传Oracle11G1.下载\n链接如下:\noracle下载地址 | LiuSw’ Blog\n2.上传\n使用ftp软件或者rz  -y上传\n如果rz –y命令不能使用,请安装lrzsz\nyum install lrzsz -y\n\n二.基础配置0.设置交换空间\noracle需要设置交换空间\n\n1.关闭selinux\nsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# 或者vim /etc/selinux/config# 将SELINUX=enforcing改成SELINUX=disabled\n\n2.关闭防火墙\nsystemctl stop firewalldsystemctl disable firewalld\n\n3.安装依赖\nyum install gcc make binutils gcc-c++ compat-libstdc++-33elfutils-libelf-devel elfutils-libelf-devel-static ksh libaio libaio-develnumactl-devel sysstat unixODBC unixODBC-devel pcre-devel –y\n\n4.添加安装用户和用户组\ngroupadd oinstallgroupadd dbauseradd -g oinstall -G dba oraclepasswd oracleid oracle\n\n5.修改内核参数配置文件\nvi /etc/sysctl.conf \n\n在末尾添加以下内容：\nfs.aio-max-nr = 1048576fs.file-max = 6815744kernel.shmall = 2097152# 其中kernel.shmmax = 1073741824为本机物理内存（2G）的一半，单位为byte。kernel.shmmax = 1073741824kernel.shmmni = 4096kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576\n\n# 加载配置sysctl -p\n\n6.修改用户的限制文件\nvim /etc/security/limits.conf\n\n在末尾添加以下内容：\noracle           soft    nproc           2047oracle           hard    nproc           16384oracle           soft    nofile          1024oracle           hard    nofile         65536oracle           soft    stack           10240 \n\nvim /etc/pam.d/login\n\n在末尾添加以下内容：\nsession required  /lib64/security/pam_limits.sosession required   pam_limits.so \n\nvim /etc/profile\n\n在末尾添加以下内容：\n#oracle配置if [ $USER = &quot;oracle&quot; ]; then  if [ $SHELL = &quot;/bin/ksh&quot; ]; then      ulimit -p 16384      ulimit -n 65536  else      ulimit -u 16384 -n 65536  fifi\n\n7.创建安装目录和设置文件权限\nmkdir -p /data/oracle/product/11.2.0mkdir /data/oracle/oradatamkdir /data/oracle/inventorymkdir /data/oracle/fast_recovery_areachown -R oracle:oinstall /data/oraclechmod -R 775 /data/oracle\n\n8.设置oracle用户环境变量\nsu -l oraclevim .bash_profile\n\n在末尾添加如下内容：\n# oracleORACLE_BASE=/data/oracleORACLE_HOME=$ORACLE_BASE/product/11.2.0ORACLE_SID=orclPATH=$PATH:$ORACLE_HOME/binexport ORACLE_BASE ORACLE_HOME ORACLE_SID PATH\n\n注意，标红处必须与创建的数据库实例名称一致，否则数据库启动后无法访问。第一次配置完记得source一下。\nsource .bash_profile\n\n三.安装Oracle1.根据响应文件静默安装Oracle11g\ncd /software/database/./runInstaller -silent -responseFile /home/oracle/response/db_install.rsp -ignorePrereq\n\n如果执行以上命令出错，会提示有参数格式，按照提示参数格式修改修改即可，一般是由于word中的字体、符号格式复制到客户端命令行后不一致引起，修改即可。\n开始Oracle在后台静默安装。安装过程中，如果提示[WARNING]不必理会，此时安装程序仍在后台进行，如果出现Successfully Setup Software，则安装程序已经停止了。\n可以在以下位置找到本次安装会话的日志:&#x2F;data&#x2F;oracle&#x2F;inventory&#x2F;logs&#x2F;installActions2022-09-22_02-14-55PM.log\n切换终端执行top命令查看后台进程一直是在安装的，&#x2F;data目录也在不断增大，当出现以下提示时，代表安装成功：\nSuccessfully Setup Software.\n\n2.切换root身份登录，执行脚本\nsh /data/oracle/inventory/orainstRoot.sh# Changing permissions of /data/oracle/inventory.# Adding read,write permissions for group.# Removing read,write,execute permissions for world.# Changing groupname of /data/oracle/inventory to oinstall.# The execution of the script is complete.\n\nsh /data/oracle/product/11.2.0/root.shCheck /data/oracle/product/11.2.0/install/root_iZ2f570bi1k56uZ_2022-09-22_03-25-04.log for the output of root script\n\n四.以静默方式配置监听使用oracle用户登录\nsu -l oraclenetca /silent /responseFile /home/oracle/response/netca.rsp \n\n注意此处，必须使用&#x2F;silent &#x2F;responseFile格式，而不是-silent -responseFile，因为是静默安装。\n成功运行后，在&#x2F;data&#x2F;oracle&#x2F;product&#x2F;11.2.0&#x2F;network&#x2F;admin中生成listener.ora和sqlnet.ora\n通过netstat命令可以查看1521端口正在监听。\nyum install -y net-toolsnetstat tunlp | grep 1521\n\n五.静默方式建立新库，同时也建立一个对应的实例vi /home/oracle/response/dbca.rsp\n\n修改文件中以下参数：\n[GENERAL]# oracle版本，不能更改RESPONSEFILE_VERSION = &quot;11.2.0&quot;# Description   : Type of operationOPERATION_TYPE = &quot;createDatabase&quot;[CREATEDATABASE]# Description   : Global database name of the database# 全局数据库的名字=SID+主机域名# 第三方工具链接数据库的时候使用的service名称GDBNAME = &quot;orcl.test&quot;# Description   : System identifier (SID) of the database# 对应的实例名字SID = &quot;orcl&quot;# Description   : Name of the template# 建库用的模板文件TEMPLATENAME = &quot;General_Purpose.dbc&quot;# Description   : Password for SYS user# SYS管理员密码SYSPASSWORD = &quot;123456&quot;# Description   : Password for SYSTEM user# SYSTEM管理员密码SYSTEMPASSWORD = &quot;123456&quot;# Description   : Password for SYSMAN user# SYSMAN管理员密码SYSMANPASSWORD = &quot;123456&quot;# Description   : Password for DBSNMP user# DBSNMP管理员密码DBSNMPPASSWORD = &quot;123456&quot;# Description   : Location of the data file&#x27;s# 数据文件存放目录DATAFILEDESTINATION =/data/oracle/oradata# Description   : Location of the data file&#x27;s# 恢复数据存放目录RECOVERYAREADESTINATION=/data/oracle/fast_recovery_area# Description   : Character set of the database# 字符集，重要!!! 建库后一般不能更改，所以建库前要确定清楚。# (CHARACTERSET = &quot;AL32UTF8&quot; NATIONALCHARACTERSET= &quot;UTF8&quot;)CHARACTERSET = &quot;ZHS16GBK&quot;# Description   : total memory in MB to allocate to Oracle# oracle内存1638MB,物理内存2G*80%TOTALMEMORY = &quot;1638&quot;\n\n进行静默配置：\ndbca -silent -responseFile /home/oracle/response/dbca.rsp\n\n建库后进行实例进程检查：\nps -ef | grep ora_ | grep -v grep\n\n查看监听状态：\nlsnrctl status\n\n数据库创建完成。\n有关详细信息, 请查看以下位置的日志文件: &#x2F;data&#x2F;oracle&#x2F;cfgtoollogs&#x2F;dbca&#x2F;orcl&#x2F;orcl.log。\n数据库信息:\n全局数据库名:orcl.test系统标识符 (SID):orcl\n\n登录查看实例状态：\nsqlplus / as sysdbaSQL&gt; select status from v$instance;\n\n\n\n如果报错：【ORA-12162: TNS:net service name is incorrectly specified】\n错误原因：【这个错误是因为ORACLE_SID变量没有传进去造成的。】\n解决方法：\n1．查看当前ORACLE_SID\n[oracle@iZ2f570bi1k56uZ ~]$ echo $ORACLE_SID\norcl         \n2．修改ORACLE_SID和&#x2F;home&#x2F;oracle&#x2F;response&#x2F;dbca.rsp中的一样                                                                                  \n[oracle@iZ2f570bi1k56uZ ~]$ export ORACLE_SID&#x3D;orcl\n3．如果遇到ORA-12162: TNS:net service name is incorrectly specified.错误\n参考文章：【ORA-12162: TNS:net service name is incorrectly specified.】\n4．如果依然不能登陆，尝试修改orcle文件夹的权限\n【如果本地连接时，出现监听错误，参考Linux中安装Oracle11g后出现监听的问题及解决办法】\n六.命令行模式静默删除1、首先查看dbca的帮助信息\ndbca -help\n\n修改&#x2F;home&#x2F;oracle&#x2F;response&#x2F;dbca.rsp文件里以下几个参数，下面三个参数根据建库实际情况进行修改：\nOPERATION_TYPE = &quot;deleteDatabase&quot;SOURCEDB = &quot;orcl&quot;SYSDBAUSERNAME = &quot;sys&quot;SYSDBAPASSWORD = &quot;123456&quot;\n\n然后运行：\ndbca -silent -responseFile /home/oracle/response/dbca.rsp\n\n各参数含义如下:\n\nsilent 表示以静默方式删除\n\n-responseFile 表示使用哪个响应文件,必需使用绝对路径\n\nRESPONSEFILE_VERSION 响应文件模板的版本,该参数不要更改\n\nOPERATION_TYPE 安装类型,该参数不要更改\n\nSOURCEDB 数据库名,不是全局数据库名,即不包含db_domain\n\n\n很简单数据库卸载完成了，请注意，只是数据库卸载完了，数据库软件还是在的。\n2、使用DBCA卸载数据库\ndbca -silent -delete Database -responseFile dbca.rsp\n\n\n-silent表示静默安装，免安装交互，大部分安装信息也不输出\n\n-responseFile指定应答文件，要求用绝对路径\n\n\n本文来自linux安装Oracle11G - 淼淼之森 - 博客园 (cnblogs.com)\n","categories":["CentOS","Oracle"],"tags":["CentOS","Oracle"]},{"title":"CentOS将可执行程序打成rpm包","url":"/20241121/CentOS/ba8ffa7afaf2/","content":"CentOS将可执行程序打成rpm包使用fpm工具直接打包1.安装rubyfpm是ruby写的，系统环境需要ruby,且ruby版本大于2.4\nyum -y install ruby rubygems ruby-devel\n\n 查看当前rubygem仓库 \ngem sources list\n\n添加国内仓库\ngem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/gem sources list\n\n2.安装fpmgem从rubygem仓库安装软件类似于yum安装。\n(1)直接安装安装成功可跳过下一步手动安装\n#CentOS 6 安装方式gem install json -v 1.8.3gem install fpm -v 1.3.3#CentOS 7 安装方式gem install fpm\n\n(2)ruby版本过低时,进行手动安装卸载老版本ruby\nyum remove ruby -y\n\n 下载ruby稳定版 \ncd /usr/local/wget https://cache.ruby-lang.org/pub/ruby/2.5/ruby-2.5.1.tar.gz\n\n安装ruby\ntar zxf ruby-2.5.1.tar.gzcd ruby-2.5.1./configure --prefix=/usr/local/rubymakemake install\n\n安装后查看版本\n/usr/local/ruby/bin/ruby -v\n\n设置环境变量\nvi /etc/profile# 最后一行添加export PATH=$PATH:/usr/local/ruby/bin\n\n验证\nsource /etc/profileruby -v\n\n重新安装fpm\ngem install fpm\n\n3.fpm打包fpm常见参数，详情查看man帮助。\n-s 指定源类型\n-t 指定目标类型，就是你想要制作什么包\n-n 指定包的名字\n-v 指定包的版本号\n-C 指定打包的相对路径\n-d 指定依赖于哪些包\n打包实例:\n#把编译好的文件按绝对路径创建相应的目录，并移动到位，如htop命令的目录结构如下./htop/└── usr    ├── local    │   └── bin    │       └── htop        └── share        ├── applications        │   └── htop.desktop        ├── man        │   └── man1        │       └── htop.1        └── pixmaps            └── htop.png\n\n#切换目录[root@localhost ~]# cd htop#执行fpm打包命令[root@localhost htop]# fpm -s dir -t rpm -n htop -v 2.2.0 ./usr/Created package &#123;:path=&gt;&quot;htop-2.2.0-1.x86_64.rpm&quot;&#125;\n\n 此时会在当前目录下生成rpm包\n[root@localhost htop]# lshtop-2.2.0-1.x86_64.rpm  usr\n\n使用rpm命令在另一台服务器上安装该软件包\nrpm -ivh htop-2.2.0-1.x86_64.rpm Preparing...                          ################################# [100%]Updating / installing...   1:htop-2.2.0-1                     ################################# [100%]\n\n\n\n参考文献1\n参考文献2\n完\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS常用设置","url":"/20250409/CentOS/f483fd2ecb83/","content":"\nCentOS常用设置关闭防火墙systemctl stop firewalldsystemctl disable firewalld\n\n关闭swap\n临时关闭\n\nswapoff -a \n\n\n永久关闭\n\nsed -i &#x27;/swap/s/^/#/&#x27; /etc/fstabcat /etc/fstab\n\n开通swap\n创建缓存文件 \n\ndd if=/dev/zero of=/var/swap bs=1024 count=2048000# if 表示infile，of表示outfile，bs=1024代表增加的模块大小，count=2048000代表2048000个模块，也就是2G空间# 执行时间较长，且根据文件大小而定，耐心等待 ...\n\n\n启用swap\n\nmkswap /var/swapmkswap -f /var/swapswapon /var/swap\n\n\n设置swap文件永久有效\n\nvi /etc/fstab/var/swap swap swap defaults 0 0\n\n\n重启检查是否已完成配置 \n\n关闭selinux\n临时关闭\n\nsetenforce 0\n\n\n永久关闭\n\nsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# 或者vim /etc/selinux/config# 将SELINUX=enforcing改成SELINUX=disabled\n\n免密登录ssh-copy-id IP地址ssh-copy-id 主机名\n\n修改地址后地址失效ssh-keygen -R IP地址ssh-keygen -R 主机名\n\n挂载本地ISOmount -t iso9660 -o loop ISO地址 挂载目录\n\n下载yum包及依赖yum install yum-plugin-downloadonlyyum install --downloadonly --downloaddir=路径 包名称#或yum install yum-utilsyumdownloader --resolve --destdir=路径 包名称\n\nKubernetes强制删除podkubectl delete pod podName -n NAMESPACE --force --grace-period=0\n\n查看Linux下进程占用排行 下面的 ps 命令格式为你提供有关内存消耗最大进程的更多信息 。\nps aux --sort -rss | head\n\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDmysql     1064  3.2  5.4 886076 209988 ?       Ssl  Oct25  62:40 /usr/sbin/mysqldvarnish  23396  0.0  2.9 286492 115616 ?       SLl  Oct25   0:42 /usr/sbin/varnishd -P /var/run/varnish.pid -f /etc/varnish/default.vcl -a :82 -T \n\n 使用以下 ps 命令格式可在输出中仅展示有关内存消耗过程的特定信息。 \nps -eo pid,ppid,%mem,%cpu,cmd --sort=-%mem | head\n\n  PID  PPID %MEM %CPU CMD 1064     1  5.4  3.2 /usr/sbin/mysqld23396 23386  2.9  0.0 /usr/sbin/varnishd -P /var/run/varnish.pid -f /etc/varnish/default.vcl -a :82 -T 127.0.0.1:6082 -S /etc/varnish/secret -s malloc,256M\n\n如果你只想查看命令名称而不是命令的绝对路径，请使用下面的 ps 命令格式。\nps -eo pid,ppid,%mem,%cpu,comm --sort=-%mem | head\n\n  PID  PPID %MEM %CPU COMMAND 1064     1  5.4  3.2 mysqld23396 23386  2.9  0.0 cache-main 1105     1  2.7  0.0 named23377 23375  2.3  0.2 nginx\n\n Linux 的 top 命令是用来监视 Linux 系统性能的最好和最知名的命令。它在交互界面上显示运行的系统进程的实时视图。但是，如果要查找内存消耗最大的进程。\ntop -c -b -o +%MEM | head -n 20 | tail -15\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND 1064 mysql     20   0  886076 209740   8388 S   0.0  5.4  62:41.20 /usr/sbin/mysqld23396 varnish   20   0  286492 115616  83572 S   0.0  3.0   0:42.24 /usr/sbin/varnishd -P /var/run/varnish.pid -f /etc/varnish/default.vcl -a :82 -T 127.0.0.1:6082 -S /etc/varnish/secret -s malloc,256M 1105 named     20   0  311712 108204   2424 S   0.0  2.8   0:16.41 /usr/sbin/named -u named -c /etc/named.conf23377 nobody    20   0  153240  89432   2432 S   0.0  2.3   4:35.74 nginx: worker process\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS更改IP后ssh无法连接","url":"/20241121/CentOS/356d71c9acc9/","content":"CentOS更改IP后ssh无法连接通过ssh-keygen命令解决&#96;\nssh-keygen -R IP地址\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS更新OpenSSH到8.6版本教程","url":"/20241121/CentOS/1d6bbd34382c/","content":"CentOS更新OpenSSH到8.6版本教程漏洞分类：通用漏洞检测危险等级：高风险影响主机：192.168.1.11 [ TCP / 22 ]输出信息：Installed version: 8.0Fixed version: 8.1Installation path / port: 22/tcp详细描述：[CVE-2019-16905]OpenSSH（OpenBSD Secure Shell）是OpenBSD计划组的一套用于安全访问远程计算机的连接工具。该工具是SSH协议的开源实现，支持对所有的传输进行加密，可有效阻止窃听、连接劫持以及其他网络级的攻击。OpenSSH 7.7版本至7.9版本和8.1之前的8.x版本中存在输入验证错误漏洞。该漏洞源于网络系统或产品未对输入的数据进行正确的验证。CVE：CVE-2019-16905CNNVD：CNNVD-201910-599\n\nssh更新\n安装依赖\nyum install -y pam-devel libselinux-devel zlib-devel openssl-devel wget gcc\n\n下载软件\ncd /usr/local/srcwget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.6p1.tar.gz\n\n备份\ncp /etc/ssh/sshd_config sshd_config.backupcp /etc/pam.d/sshd /etc/pam.d/sshd.backup\n\n卸载旧版ssh\n\n\n此时可以删除低版本包，当然这个操作是有风险的，如果删除了但是安装不成功就远程连接不上去了，只有自己去远程服务器边上安装，保险办法是通过另外的手段远程连接，我自己没有遇到这个问题，所以就不赘述。\nrpm -qa | grep opensshrpm -e --nodeps `rpm -qa | grep openssh`\n\n\n解压新版压缩包\ntar -zxvf openssh-8.6p1.tar.gzcd  openssh-8.6p1\n\n设置编译配置\n./configure --prefix=/usr --sysconfdir=/etc/ssh --with-md5-passwords --with-pam --with-zlib --with-tcp-wrappers --with-ssl-dir=/usr/local/ssl --without-hardening\n\n文件授权\nchmod 600 /etc/ssh/ssh_host_rsa_key /etc/ssh/ssh_host_ecdsa_key /etc/ssh/ssh_host_ed25519_key\n\n编译\nmakemake install\n\n复制文件\ncp -a contrib/redhat/sshd.init  /etc/init.d/sshdchmod u+x /etc/init.d/sshd\n\n修改配置\nvim /etc/ssh/sshd_config# 修改#PermitRootLogin prohibit-password项，去掉注释#并把prohibit-password改为yes，修改后即为PermitRootLogin yes#去掉注释#PasswordAuthentication yes变为`PasswordAuthentication yes 如果使用证书登陆可以关闭这个#升级之后sftp如果不好用了，需要注意的是要查看一下相关设置，新的是Subsystem sftp /usr/libexec/sftp-server原始系统自带的是Subsystem sftp /usr/libexec/openssh/sftp-server# 一般系统是开启PAM模块的，但是编译安装升级后，这个是关闭状态，如果启用PAM，需要有一个控制文件，去掉注释#UsePAM no 变为UsePAM yes,如果打开这个模式，不要忘了把配置文件/etc/pam.d/sshd.backup改回来,否则登陆不了。\n\n还原pam.d下的sshd文件\nmv /etc/pam.d/sshd.backup /etc/pam.d/sshd\n\n设置开机自启\nchkconfig --add sshdchkconfig sshd on\n\n\n测试\nssh -V\n\n成功！\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS设置服务守护进程","url":"/20241121/CentOS/fda32bb546ee/","content":"CentOS设置服务守护进程一.介绍Systemctl是linux系统继init.d之后的一个systemd工具，主要负责控制systemd系统和管理系统服务\nsystemd即为system daemon,是linux下的一种init软件\n有时我们将自定义程序注册为systemd service 进程管理交由系统管理，可以方便启动停止，亦可以实现服务异常退出重启,开机自启动。 减少自定义程序服务管理的时间消耗。\n二.参数了解systemctl service自定义注册服务的一些参数：systemctl管理的服务脚本存放在&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F; 有user与systemd区分需要开机自启动存放在system目录下：&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system并且服务的脚本一般以.service结尾\n例如：\nsystemctl cat sshd\n\n# /usr/lib/systemd/system/sshd.service[Unit]Description=OpenSSH server daemonDocumentation=man:sshd(8) man:sshd_config(5)After=network.target sshd-keygen.serviceWants=sshd-keygen.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/sshdExecStart=/usr/sbin/sshd -D $OPTIONSExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failureRestartSec=42s[Install]WantedBy=multi-user.target\n\n\n可以看到service文件一般由：[Unit]、[Service]和[Install] 三部分组成\n备注： 官方分为：[Unit]、[unit type] 和[Install]unit type 如果一个单元具有特殊类型定义则将它们定义在类型命名之后，比如Service定义为[Service]\n1.[Unit]主要是对这个服务的说明，内容， 文档介绍以及对一些依赖服务定义**\n具体参数:Description 服务描述，作为systemctl status 命令输出的一个介绍Documentation 一个url 定义服务的具体介绍网址After 在什么服务启动之后Before 在什么服务启动之前启动Requires 依赖其他的单元服务， 需要与列出的服务一起激活，若任何服务无法启动，则该单元不会被激活Wants 比Requires依赖性弱，弱其他服务没有启动成功，该服务也不受影响，只是表示一种推荐\n2.[Service]服务的主体定义，主要定义服务的一些运行参数，及操作动作。\n具体参数:\nType\n\n1.simple默认参数，进程作为主进程\n2.forking是后台运行的形式，主进程退出，os接管子进程\n3.oneshot 类似simple，在开始后续单元之前，过程退出\n4.DBUS 类似simple，但随后的单元只在主进程获得D总线名称之后才启动\n5.notify 类似simple，但是随后的单元仅在通过sd_notify()函数发送通知消息之后才启动\n6.idle类似simple，服务二进制文件的实际执行被延迟到所有作业完成为止，不与其他服务的输出相混合,如状态输出与服务的shell输出混合\n\n备注：以上的类似simple指的是类似simple将启动进程作为主进程进行运行\nUser​\t设置服务运行的用户\nGroup\n​\t设置服务运行的用户组,\nPIDFile\n​\t为存放PID的文件路径, 对于type设置为forking建议使用该项。 systemd will readthe PID of the main process of the daemon after start-up of the service. systemd will not write to the file configured here,although it will remove the file after the service has shut down if it still exists.\nExecStart\n​\t服务的具体运行命令,ExecStartPre和ExecStartPost指定在ExecStart前后执行的自定义命令。若使用Type &#x3D; OnHead可以指定多个自定义命令，将依次执行这些命令。\nExecReload\n​\t为重启命令，重新加载的动作， 重新加载时执行的命令或者脚本\nExecStop\n​\t为停止命令，停止时要执行的命令或脚本\nExecStartPre\n​\t启动服务之前执行的命令\nExecStartPost\n​\t启动服务之后执行的命令\nExecStopPost\n​\t停止服务之后执行的命令\nRestart\n​\t定义何种情况Systemd会自动重启当前服务，值：包括always（总是重启）、no 、on-success、on-failure、on-abnormal、on-abort、on-watchdog对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal\nRestartSec\n​\t设置在重启服务(Restart=)前暂停多长时间。 默认值是100毫秒(100ms)。 如果未指定时间单位，那么将视为以秒为单位。 例如设为”20”等价于设为”20s”。\nTimeoutStartSec\n​\t等待启动的时间。如果守护进程服务没有在配置的时间内发送启动完成的信号，则该服务将被认为失败， 服务将退出。以秒为单位， “0”来禁用。默认为， 默认使用DefaultTimeoutStartSec，若使用Type&#x3D;oneshot，则该模式默认情况下超时是禁用的\nTimeoutStopSec\n​\t等待关闭的超时时间\nTimeoutSec\n​\t快速配置TimeoutStartSec和TimeoutStopSec时间\nKillMode\n\ncontrol-group（默认值）：当前控制组里面的所有子进程，都会被杀掉\nprocess：只杀主进程\nmixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号\nnone：没有进程会被杀掉，只是执行服务的 stop 命令。\n\nEnvironmen\n​\t指定环境变量\nPrivateTmp&#x3D;True\n​\t表示给服务分配独立的临时空间\n注意：**[Service]部分的启动、重启、停止命令必须使用绝对路径，使用相对路径则会报错**\nKillSignal\n​\t服务的所有进程都将会根据 KillSignal= 的设置被立即全部杀死。 与 ExecReload= 一样\n3.[Install]服务安装的相关设置，一般可设置为多用户的。\n具体参数:\n\nRequiredBy 依赖该服务的服务列表\nWantedBy 表示该服务所在的 Target， multi-user.target 可以设置为多用户模式具体参考手册systemd.unit(5)\n\n服务状态列表：\n\nloaded 系统服务已经初始化完成，加载过配置\nactvie(running) 正常运行\nactvie(exited) 正常结束的服务\nactive(waitting) 正在执行当中， 等待其他的事件才继续处理\ninactive 服务关闭\nenabled 服务开机启动\ndisabled 服务开机不自启\nstatic 服务开机启动项不可被管理\nalied 系统配置错误\n\nSystemd 统一管理所有 Unit 的启动日志。journalctl命令查看所有日志(内核日志和应用日志)日志的配置文件是&#x2F;etc&#x2F;systemd&#x2F;journald.conf\n4.示例运行shell的一个示例：.service文件：\n[Unit]Description=my test serviceAfter=network.target remote-fs.target nss-lookup.target [Service]Type=simplePIDFile=/tmp/my.pidExecStartPre=/usr/bin/rm -f /tmp/my.pidExecStart= /usr/sbin/test.shKillSignal=SIGQUITTimeoutStopSec=5KillMode=processPrivateTmp=true #[Install]#WantedBy=multi-user.target\n\ntest.sh文件内容\n#!/bin/bashwhile truedo   echo `date`,&quot;ok&quot; &gt;&gt;/tmp/result.logdone\n\n注意**#!&#x2F;bin&#x2F;bash**是必须的，否则会提示203错误异常，服务启动失败\n三.SYSTEMCTL设置自动重启1.最简单的自动重启范例[Unit]Description=mytest [Service]Type=simpleExecStart=/root/mytest.shRestart=alwaysRestartSec=5StartLimitInterval=0 [Install]WantedBy=multi-user.target\n\n重点参数详解\n\nRestart&#x3D;always: 只要不是通过systemctl stop来停止服务，任何情况下都必须要重启服务，默认值为no\nRestartSec&#x3D;5: 重启间隔，比如某次异常后，等待5(s)再进行启动，默认值0.1(s)\nStartLimitInterval: 无限次重启，默认是10秒内如果重启超过5次则不再重启，设置为0表示不限次数重启\n\n2.案例需求需求：有个业务，当程序因受到OOM而退出的时候，不希望自动重启（此时需要人工介入排查），其他情况下可以自动重启\n分析：OOM就是通过kill -9来杀进程，因此只要找到方法，告诉systemd当该服务遇到kill -9时候不自动重启即可\n3.RestartPreventExitStatus参数查询man systemd.service发现，systemd的[Service]段落里支持一个参数，叫做RestartPreventExitStatus\n该参数从字面上看，意思是当符合某些退出状态时不要进行重启。\n该参数的值支持exit code和信号名2种，可写多个，以空格分隔，例如\nRestartPreventExitStatus=143 137 SIGTERM SIGKILL\n\n表示，当退出情况只要符合以下4种情况中任意一种时候，则不再进行重启：\n\nexit code为143\n\nexit code为137\n\n信号为TERM\n\n信号为KILL\n\n\n4.注意事项4.1.RestartPreventExitStatus与Restart的关系配置RestartPreventExitStatus&#x3D;后，并没有完全忽略Restart&#x3D;，而是指当退出情况与RestartPreventExitStatus&#x3D;匹配的时候，才忽略Restart&#x3D;，若没有匹配，根据Restart&#x3D;该怎么样还怎么样（具体详见后面的详细测试数据）\n4.2.kill子进程会是什么情况若systemd启动的不是一个简单进程，而是会派生子进程的情况（比如执行shell脚本，shell脚本里启动多个程序），那么当另外开一个窗口通过 kill-信号测试时，会是什么情况呢，先贴出测试方法\nExecStart&#x3D;&#x2F;root&#x2F;mem改为ExecStart&#x3D;&#x2F;root&#x2F;mytest.sh\n&#x2F;root&#x2F;mytest.sh内容为\n#!/bin/bashsleep 100000 &amp;sleep 200000\n\n测试结果\n\n若kill 主进程PID（kill不带参数），则主进程状态为 code=killed,signal=TERM\n若kill -9 主进程PID，则主进程状态为 code=killed,signal=KILL\n若kill 最后一个子进程PID（kill不带参数），则systemd不认为是接收到信号，而是根据最后一个进程的exit code进行处理，此时主进程状态为 code=exited,status=143\n若kill -9 最后一个子进程PID，此时主进程状态为 code=exited,status=137\n\n四.systemd 日志重定向[Unit]Description=agent server daemonAfter=network.target[Service]Type=simpleExecStart=/bin/sh -c &#x27;/usr/local/artifacts/agent/bin/-agent -c /usr/local/artifacts/agent/conf/cfg.agent.json 1&gt;&gt;/usr/local/artifacts/agent/logs/out.log 2&gt;&amp;1&#x27;Restart=on-failure [Install]WantedBy=multi-user.target\n\n注意文件夹事先实现创建！！\n来自lonnng2004的博客\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS配置本地yum源","url":"/20241121/CentOS/ff18412a14df/","content":"CentOS配置本地yum源\n为了方便测试与开发，常常需要在虚拟机上安装各种开发环境，如果没有yum源，安装rpm需要解决各种依赖关系非常麻烦\n\n环境\n[root@master01 ~]# cat /etc/redhat-releaseCentOS Linux release 7.6.1810 (Core)\n1. 创建挂载目录mkdir /mnt/yum\n2.挂载OS文件将iso文件上传至虚拟机中，也可以直接通过虚拟机挂载iso\n[root@master01 /]# mount -t iso9660 -o loop /guqi/CentOS-6.5-x86_64-bin-DVD1.iso /mnt/yumls /mnt/yum# CentOS_BuildTag  EULA  images    Packages                  repodata              RPM-GPG-KEY-CentOS-Debug-6     RPM-GPG-KEY-CentOS-Testing-6# EFI              GPL   isolinux  RELEASE-NOTES-en-US.html  RPM-GPG-KEY-CentOS-6  RPM-GPG-KEY-CentOS-Security-6  TRANS.TBL\n3. 修改yum配置文件ls -l /etc/yum.repos.d/# total 16# -rw-r--r--. 1 root root 1926 Nov 27  2013 CentOS-Base.repo# -rw-r--r--. 1 root root  638 Nov 27  2013 CentOS-Debuginfo.repo# -rw-r--r--. 1 root root  630 Nov 27  2013 CentOS-Media.repo# -rw-r--r--. 1 root root 3664 Nov 27  2013 CentOS-Vault.repo\n将其他yum源方式禁用，将.repo文件备份，创建bak文件夹\nmkdir /etc/yum.repos.d/bakmv /etc/yum.repos.d/*.repo  /etc/yum.repos.d/bak\n4. 配置本地yum源cat &gt;&gt; /etc/yum.repos.d/CentOS-Media-local.repo &lt;&lt;EOF[c6-media]name=CentOS-$releasever - Mediabaseurl=file:///mnt/yum/   #iso挂载目录gpgcheck=1enabled=1                         #设置为1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6EOF\n5. 清除原有的yum信息，生成yum元数据[root@master01 /]# yum clean allLoaded plugins: fastestmirror, securityCleaning repos: c6-mediaCleaning up Everything[root@master01 /]# yum makecache\n\n6.将本地源给其他内网服务器使用\n(1).基于ftp方式配置本地yum发布源\n\n为本机配置ftp服务器，将其发布供网络其它客户端作为yum源\n\nyum安装或者rpm -ivh安装vsftpd\n\n[root@master01 ~] # yum -y install vsftpd\n\n\n关闭Seliunx\n\n[root@master01 ~] # sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config[root@master01 ~] # setenforce 0\n\n修改ftp配置文件\n\n[root@master01 ~] # vi /etc/vsftpd/vsftpd.confanon_root=/mnt/yum\n\n启动ftp设置开机自启\n\n[root@master01 ~]# systemctl start vsftpd[root@master01 ~]# systemctl enable vsftpd\n\n测试ftp服务可用性\n\n[root@master01 ~]# ftp localhost# Trying ::1...# Connected to localhost (::1).# 220 (vsFTPd 3.0.2)# Name (localhost:root): anonymous# 331 Please specify the password.# Password:# 230 Login successful.# Remote system type is UNIX.# Using binary mode to transfer files.ftp&gt; ls# 229 Entering Extended Passive Mode (|||41973|).# 150 Here comes the directory listing.# -r--r--r--    1 0        0              14 Sep 14 09:06 CentOS_BuildTag# dr-xr-xr-x    3 0        0              33 Sep 14 09:06 EFI  ### Author : Leshami# -r--r--r--    1 0        0            215 Sep 14 09:06 EULA ###  Blog : # # # http://blog.csdn.net/leshami# -r--r--r--    1 0        0          18009 Sep 14 09:06 GPL# dr-xr-xr-x    2 0        0              41 Sep 14 09:06 LiveOS# dr-xr-xr-x    2 0        0          200704 Sep 14 09:08 Packages# -r--r--r--    1 0        0            1690 Sep 14 09:08 RPM-GPG-KEY-CentOS-7# -r--r--r--    1 0        0            1690 Sep 14 09:08 RPM-GPG-KEY-CentOS-Testing-7# -r--r--r--    1 0        0            2883 Sep 14 09:08 TRANS.TBL# dr-xr-xr-x    3 0        0              54 Sep 14 09:06 images# dr-xr-xr-x    2 0        0            4096 Sep 14 09:06 isolinux# dr-xr-xr-x    2 0        0            4096 Sep 14 09:08 repodata# 226 Directory send OK.\n\n配置防火墙或者关闭防火墙\n\n# 配置防火墙[root@master01 ~]# firewall-cmd --add-service=ftp --permanent[root@master01 ~]# firewall-cmd --add-service=ftp[root@master01 ~]# systemctl reload firewalld.service# 关闭防火墙[root@master01 ~]# systemctl stop firewalld[root@master01 ~]# systemctl disable firewalld\n\n\n(2).基于http方式配置本地yum发布源\n\n除了支持ftp方式外，也可以通过http方式将其发布供网络其它客户端作为yum源\n\n安装httpd或者python搭建简单的文件共享服务\n\n# 安装httpd[root@master01 ~]# yum install httpd[root@master01 ~]# systemctl enable httpd[root@master01 ~]# systemctl start httpd# python搭建简单的文件共享服务cd /mnt/yum# 80端口容易冲突，所以选用8001端口nohup python -m SimpleHTTPServer 8001 &amp;\n\n\n配置防火墙或者关闭防火墙\n\n# 配置防火墙[root@master01 ~]# firewall-cmd --add-service=http --permanent[root@master01 ~]# firewall-cmd --add-service=http[root@master01 ~]# systemctl reload firewalld.service# 关闭防火墙[root@master01 ~]# systemctl stop firewalld[root@master01 ~]# systemctl disable firewalld\n\n\n将CentOS 光盘文件copy到&#x2F;var&#x2F;www&#x2F;html&#x2F;repo此处使用了链接方式，将其链接到已经在本地磁盘的&#x2F;mnt&#x2F;yum\n\n[root@master01 ~]# ln -sv /mnt/yum /var/www/html/repo/var/www/html/repo -&gt; /mnt/yum[root@master01 ~]# ls /var/www/html/repo# CentOS_BuildTag EULA images LiveOS repo RPM-GPG-KEY-CentOS-7 TRANS.TBL# EFI GPL isolinux Packages repodata RPM-GPG-KEY-CentOS-Testing-7\n\n通过浏览器校验，此时应该可以看到文件列表(此处略) http://192.168.1.1/repo\n\n\n配置客户端repo文件\n\n[root@centos7-web ~]# mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/[root@centos7-web ~]# yum-config-manager --add-repo=http://192.168.1.1/repo# Loaded plugins: fastestmirror, langpacks# adding repo from: http://192.168.1.1/repo[192.168.1.1_repo]name=added from: http://192.168.1.175/repobaseurl=http://192.168.1.175/repoenabled=1[root@centos7-web ~]# yum clean all# Loaded plugins: fastestmirror, langpacks# Cleaning repos: 192.168.1.175_repo# Cleaning up everything# Cleaning up list of fastest mirrors[root@centos7-web ~]# yum makecache# Loaded plugins: fastestmirror, langpacks# 192.168.1.1_repo | 3.6 kB 00:00:00# (1/4): 192.168.1.1_repo/group_gz | 155 kB 00:00:00# (2/4): 192.168.1.1_repo/primary_db | 2.8 MB 00:00:00# (3/4): 192.168.1.1_repo/other_db | 1.2 MB 00:00:00# (4/4): 192.168.1.1_repo/filelists_db | 2.9 MB 00:00:01# Determining fastest mirrors# Metadata Cache Created[root@centos7-web ~]# yum repolist# Loaded plugins: fastestmirror, langpacks# Loading mirror speeds from cached hostfile# repo id repo name status# 192.168.1.1_repo added from: http://192.168.1.1/repo 3,723# repolist: 3,723\n7.本地源更新\n将rpm包放入package，执行createrepo\n\ncreaterepo --update /mnt/yum\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"CentOS配置硬盘自动挂载","url":"/20241121/CentOS/219db4dbd72b/","content":"CentOS配置硬盘自动挂载1.查看Linux硬盘信息sudo fdisk -l\n\n2.格式化新硬盘**（很危险，注意操作时确定硬盘分区的位置) **\n# 注意文件系统格式：# 1.文件系统格式xfsmkfs.xfs  /dev/sdb1# 2.文件系统格式ext4mkfs.ext4 /dev/sdb1\n\n3.创建&#x2F;data目录（&#x2F;data目录为硬盘将挂载的地方）\nmkdir /data\n\n4.挂载分区mount /dev/sdb1 /data\n\n5.查看磁盘分区的UUIDblkid# /dev/sda1: UUID=&quot;8048997a-16c9-447b-a209-82e4d380326e&quot; TYPE=&quot;ext4&quot;  # /dev/sda5: UUID=&quot;0c5f073a-ad3f-414f-85c2-4af83f6a437f&quot; TYPE=&quot;swap&quot;  # /dev/sdb1: UUID=&quot;11263962-9715-473f-9421-0b604e895aaa&quot; TYPE=&quot;ext4&quot;  # /dev/sr0: LABEL=&quot;Join Me&quot; TYPE=&quot;iso9660&quot;\n\n6.配置开机自动挂载vim /etc/fstab# 加入# 分区的UUID 挂载的路径 文件系统格式(ext4,xfs)  defaults 0 1 UUID=11263962-9715-473f-9421-0b604e895aaa /data ext4 defaults 0 1\n\n参数详情\n&lt;fs spec&gt; &lt;fs file&gt; &lt;fs vfstype&gt; &lt;fs mntops&gt; &lt;fs freq&gt; &lt;fs passno&gt;具体说明，以挂载/dev/sdb1为例：&lt;fs spec&gt;：分区定位，可以给UUID或LABEL，例如：UUID=6E9ADAC29ADA85CD或LABEL=software&lt;fs file&gt;：具体挂载点的位置，例如：/data&lt;fs vfstype&gt;：挂载磁盘类型，linux分区一般为ext4，windows分区一般为ntfs&lt;fs mntops&gt;：挂载参数，一般为defaults&lt;fs freq&gt;：磁盘检查，默认为0&lt;fs passno&gt;：磁盘检查，默认为0，不需要检查\n\n7.验证mount -a# 重启系统查看是否自动挂载reboot# 命令验证一下配置是否正确。如果配置不正确可能会导致系统无法正常启动。\n\n\n\n\n来源：简书https://www.jianshu.com/p/336758411dbf\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Cockpit安装","url":"/20241121/CentOS/657d292875b0/","content":"Cockpit安装1.Cockpit简介Cockpit是一个免费且开源的基于web的管理工具，系统管理员可以执行诸如存储管理、网络配置、检查日志、管理容器等任务。通过Cockpit提供的友好的 Web 前端界面可以轻松地管理我们的 GNU/Linux 服务器，非常轻量级，Web 界面也非常简单易用。更重要的是通过Cockpit可以实现集中式管理。\n\n2.功能特点下面是 Cockpit的一些功能和特点：\n\nCockpit使用systemd完成从运行守护进程到服务几乎所有的功能\n集中式管理，通过一个会话窗口管理网络中的所有Linux服务器\n创建和管理Docker容器\n创建和管理KVM、oVirt虚拟机\n包括 LVM 在内的存储配置\n基本的网络配置管理\n用户user account管理\n基于web的 终端\n图形化的系统性能展示\n使用sosreport收集系统配置和诊断信息\n支持Debian, Redhat, CentOS, Fedora, Atomic, Arch Linux, and Ubuntu.\n\n3.模块Package Name\t          Purposecockpit-docker            Managing Docker Containerscockpit-kubernetes\t      Visualizing and Configuring Kubernetes Clustercockpit-machines\t      Manage KVM Virtual Machinescockpit-sosreport\t      Create diagnostic report with the sosreport toolcockpit-selinux\t          Troubleshoot SELinux Issuescockpit-kdump\t          Configure Kernel Crash Dumpscockpit-subscriptions\t  Manage System subscriptioncockpit-machines-ovirt\t  Manage oVirt Virtual Machinescockpit-pcp\tReading PCP   metrics and Loading PCP archives\n\n\n4.安装确认YUM源可用这里是在CentOS7中进行安装，CentOS的软件仓库中包含有cockpit安装包\nyum list |grep cockpit\n\n[root@k8s-node1 ~]# yum list |grep cockpit cockpit.x86_64                              195.12-1.el7.centos        extras   cockpit-bridge.x86_64                       195.12-1.el7.centos        extras   cockpit-composer.noarch                     9-1.el7                    extras   cockpit-dashboard.x86_64                    195.12-1.el7.centos        extras   cockpit-doc.x86_64                          195.12-1.el7.centos        extras   cockpit-docker.x86_64                       195.12-1.el7.centos        extras   cockpit-kubernetes.x86_64                   195.12-1.el7.centos        extras   cockpit-machines.noarch                     195.6-1.el7.centos         extras   cockpit-machines.x86_64                     195.12-1.el7.centos        extras   cockpit-machines-ovirt.noarch               195.12-1.el7.centos        extras   cockpit-packagekit.noarch                   195.6-1.el7.centos         extras   cockpit-packagekit.x86_64                   195.12-1.el7.centos        extras   cockpit-pcp.x86_64                          195.12-1.el7.centos        extras   cockpit-storaged.noarch                     195.12-1.el7.centos        extras   cockpit-subscriptions.noarch                160-1.el7.centos           extras   cockpit-system.noarch                       195.12-1.el7.centos        extras   cockpit-tests.x86_64                        195.12-1.el7.centos        extras   cockpit-ws.i686                             195.10-1.el7.centos        base     cockpit-ws.x86_64                           195.12-1.el7.centos        extras   subscription-manager-cockpit.noarch         1.24.45-1.el7.centos       updates\n\n开始安装yum -y install cockpit cockpit-dashboard cockpit-storaged cockpit-packagekit\n\n启动cockpit服务systemctl enable --now cockpit.socket\n\n添加防火墙规则firewall-cmd -permanent -add-service=cockpitfirewall-cmd -reload\n\n5.登录Cockpit使用9090端口，并且为SSL加密访问，通过浏览器登陆https://ip.add.re.ss:9090\n登陆时浏览器会提示链接不安全，如果是Firfox浏览器，点击添加例外\n输入安装了cockpit的系统账号（用户名和密码和用于登录linux 服务器的用户名和密码相同）\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Docker实现导出、导入和数据搬迁","url":"/20241121/Docker/7d64abcc17e0/","content":"docker实现导出、导入和数据搬迁# 1.docker 导出 导入有二种，一种是备份镜像，一种备份容器# 2.数据搬迁，最简单粗暴就是直接COPY，volume的路径就行了\n\n一.导出导入镜像# 导出为tardocker save #ID or #Name &gt; /home/save.tar# 导入tardocker load &lt; /home/save.tar\n\n二.导出导入容器# 导出为tardocker export #ID or #Name &gt; /home/export.tar# 导入tarcat /home/export.tar | docker import - test:1.0\n\n三.数据迁移# 1.生成容器时设置了--volume，直接COPY后面的目录到新的服务器，或者路径# 2.然后docker run 把--volume指定到新的路径就行了。# 如果不知道--volume的路径，到cd /var/lib/docker/containers/下面找到对应的容器。里面有一个配置文件cat hostconfig.json&#123;&quot;Binds&quot;:[&quot;/home/docker/redmine/redmine:/home/redmine/data&quot;],&quot;ContainerIDFile&quot;:&quot;&quot;,# /home/docker/redmine/redmine，这个就是我设置的--volume，如果没有设置--volume在这里也可以找到\n","categories":["Docker"],"tags":["Docker"]},{"title":"ELK+Filebeat+Kafka集群部署","url":"/20241121/Kafka/64835691799e/","content":"ELK+Filebeat+Kafka集群部署前言业务层可以直接写入到kafka队列中，不用担心elasticsearch的写入效率问题。\n消息系统主要功能1、解耦允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束2、冗余消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。3、扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。4、灵活性 &amp; 峰值处理能力在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。5、可恢复性　系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。6、顺序保证在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）7、缓冲有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。8、异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\nRedis与Kafka我们都知道Redis是以key的hash方式来分散对列存储数据的，且Redis作为集群使用时，对应的应用对应一个Redis，在某种程度上会造成数据的倾斜性，从而导致数据的丢失。而从之前我们部署Kafka集群来看，kafka的一个topic（主题），可以有多个partition（副本），而且是均匀的分布在Kafka集群上，这就不会出现redis那样的数据倾斜性。Kafka同时也具备Redis的冗余机制，像Redis集群如果有一台机器宕掉是很有可能造成数据丢失，而Kafka因为是均匀的分布在集群主机上，即使宕掉一台机器，是不会影响使用。同时Kafka作为一个订阅消息系统，还具备每秒百万级别的高吞吐量，持久性的、分布式的特点等。\n架构图\n说明说明1、可以使用一台Nginx代理访问kibana的请求;2、三台es组成es集群，并且在三台es上面都安装kibana;（ 以下对elasticsearch简称es ），两台logstash;3、中间三台服务器就是我的kafka(zookeeper)集群啦; 上面写的 消费者&#x2F;生产者 这是kafka(zookeeper)中的概念;4、使用filebeat收集日志(windows linux等)\n角色1.es1+zookeeper+kafka+logstash: 192.168.11.156\n2.es2+zookeeper+kafka+logstash: 192.168.11.157\n3.es3+zookeeper+kafka: 192.168.11.159\n4.kibana: 192.168.11.156\n5.filebeat：客户端安装\n软件说明1.es : 7.14.0\n2.logstash : 7.14.0\n3.kibana : 7.14.0 \n4.filebeat : 7.14.0\n5.zookeeper :  3.4.14\n6.kafka : 2.13\n7.jdk : 1.8\n安装步骤1、ES集群安装配置;2、Logstash客户端配置(直接写入数据到ES集群，写入系统messages日志);3、Kafka+zookeeper集群配置;(Logstash写入数据到Kafka消息系统);4、Kibana部署;5、filebeat安装;\n详细安装步骤一.安装es集群1.上传elasticsearch-7.14.0-linux-x86_64.tar.gz安装包到服务器&#x2F;data下，并安装jdk；\n2.解压安装包；\ncd /datatar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz\n\n3.修改服务器配置；\n# 创建elk用户useradd elk elk# 修改elk用户拥有的内存权限至少需要262144；sysctl -w vm.max_map_count=262144# 修改 /etc/sysctl.conf文件vm.max_map_count=262144# 修改/etc/security/limits.conf# 在文件末尾添加下面的参数值* soft nofile 65536* hard nofile 131072\n\n4.修改es配置文件；\n# 修改相应的选项vi /data/elasticsearch-7.14.0/config/elasticsearch.yml\n\n\n\n  elasticsearch.yml\n   \n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\n# 集群名称\ncluster.name: elk-boer\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\n# 节点名称，不能重复\nnode.name: elk01\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# 是否能当master\nnode.master: true\n#\n# 是否能储存数据\nnode.data: true\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /data/elasticsearch-7.14.0/data\n# \n#\n# Path to log files:\n#\n#path.logs: /data/elasticsearch-7.14.0/logs\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\n#bootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# By default Elasticsearch is only accessible on localhost. Set a different\n# address here to expose this node on the network:\n#\n#network.host: 192.168.0.1\n# 配置可以访问的IP,无其他要求不修改\nnetwork.host: 0.0.0.0\n#\n# By default Elasticsearch listens for HTTP traffic on the first free port it\n# finds starting at 9200. Set a specific HTTP port here:\n#\n# es服务端口号\nhttp.port: 9200\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when this node is started:\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\n#discovery.seed_hosts: [\"host1\", \"host2\"]\n# 集群配置\ndiscovery.seed_hosts: [\"elk01\", \"elk02\",\"elk03\"]\n#\n# Bootstrap the cluster using an initial set of master-eligible nodes:\n#\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n# 集群配置\ncluster.initial_master_nodes: [\"elk01\", \"elk02\",\"elk03\"]\n#\n# For more information, consult the discovery and cluster formation module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n# 跨域\nhttp.cors.enabled: true \nhttp.cors.allow-origin: \"*\"\n\n  \n\n\n\n5.启动es；\n#!/bin/sh# 用elk用户 -d参数后台启动,ES_JAVA_OPTS参数为内存的一半sudo -u elk ES_JAVA_OPTS=&quot;-Xms8192m -Xmx8192m&quot;  /data/elasticsearch-7.14.0/bin/elasticsearch  -d\n\n6.访问IP:9200 ；\n[root@elk01 elasticsearch-7.14.0]# curl http://192.168.11.157:9200&#123;  &quot;name&quot; : &quot;elk02&quot;,  &quot;cluster_name&quot; : &quot;elk-boer&quot;,  &quot;cluster_uuid&quot; : &quot;QrvFk9tSQT2qPD7kBHPBLw&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;7.14.0&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;tar&quot;,    &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;,    &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;8.9.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;\n\n\n\n二.安装zoopeeker1.安装jdk1.8，并解压zookeeper压缩包；\n2.修改&#x2F;etc&#x2F;profile；\nvim  /etc/profileJAVA_HOME=/java/jdk1.8.0_161PATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOMEexport PATHexport CLASSPATH\n\n3.使用source命令使profile文件内容生效 ;\nsource  /etc/profilejava -version\n\n4.修改配置文件；\ncd /data/zookeeper/confmv  zoo_sample.cfg  zoo.cfgvim  zoo.cfg\n\n5.将 zookeeper集群的三台服务器地址填入,端口默认 ；\n# zookeeper1 配置文件实例# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/data/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=192.168.11.156:2888:3888server.2=192.168.11.157:2888:3888server.3=192.168.11.159:2888:3888\n\n6.创建tmp文件夹以及创建myid文件 ；\ncd  /data/zookeeper/echo 1 &gt; myid   #zookeeper1创建echo 2 &gt; myid   #zookeeper2创建echo 3 &gt; myid   #zookeeper3创建\n\n7.启动所有Zookeeper ；\ncd  /data/zookeeper/bin./zkServer.sh  start\n\n8.注册服务\n三.kafka1.解压kafka_2.13-2.6.0.tgz压缩包；\n2.修改配置文件；\ncd  /data/kafka/configvim  server.properties  broker.id=1                 #三台服务器id分别为1、2、3listeners=PLAINTEXT://192.168.11.156:9092     #本服务器地址（端口号默认）advertised.listeners=PLAINTEXT://192.168.11.156:9092     #本服务器地址（端口号默认）log.dirs=/data/kafka/logs      #log文件位置zookeeper.connect=192.168.11.156:2181,192.168.11.157:2181,192.168.11.159:2181    #zookeeper服务器集群IP（端口号默认）\n\n3.启动kafka\ncd  /data/kafka./bin/kafka-server-start.sh  -daemon  ./config/server.properties     #-daemon为后台启动参数\n\n四.安装Logstash1.上传并解压logstash-7.14.0-linux-x86_64.tar.gz；\n2.修改配置文件(没有可以创建)；\ninput&#123;    kafka &#123;        bootstrap_servers =&gt; &quot;192.168.11.156:9092,192.168.11.157:9092,192.168.11.159:9092&quot; #kafka服务器地址        topics =&gt; &quot;monitoring-log&quot;        group_id =&gt; &quot;elk-boer&quot;        decorate_events =&gt; true #kafka标记        consumer_threads =&gt; 1        codec =&gt; &quot;json&quot; #写入的时候使用json编码，因为logstash收集后会转换成json格式        type =&gt; &quot;monitoring-log&quot;    &#125;    kafka &#123;        bootstrap_servers =&gt; &quot;192.168.11.156:9092,192.168.11.157:9092,192.168.11.159:9092&quot; #kafka服务器地址        topics =&gt; &quot;metricbeat&quot;        group_id =&gt; &quot;elk-boer&quot;        decorate_events =&gt; true #kafka标记        consumer_threads =&gt; 1        codec =&gt; &quot;json&quot; #写入的时候使用json编码，因为logstash收集后会转换成json格式        type =&gt; &quot;metricbeat&quot;    &#125;          kafka &#123;        bootstrap_servers =&gt; &quot;192.168.11.156:9092,192.168.11.157:9092,192.168.11.159:9092&quot; #kafka服务器地址        topics =&gt; &quot;oracle-irm&quot;        group_id =&gt; &quot;elk-boer&quot;        decorate_events =&gt; true #kafka标记        consumer_threads =&gt; 1        codec =&gt; &quot;json&quot; #写入的时候使用json编码，因为logstash收集后会转换成json格式        type =&gt; &quot;oracle-irm&quot;    &#125;&#125;filter&#123;#    if [type] == &quot;monitoring-log&quot; &#123;#        grok &#123;#            match =&gt; &#123;#                &quot;message&quot; =&gt; &quot;(?&lt;Date&gt;^%&#123;DAY&#125;\\s%&#123;MONTH&#125;\\s%&#123;MONTHDAY&#125;\\s%&#123;TIME&#125;\\s%&#123;YEAR&#125;).\\[(?&lt;info&gt;[a-z]*)\\].(?&lt;fangfa&gt;[%&#123;WORD&#125;-]*).(?&lt;yunaiz&gt;[%&#123;WORD&#125;-]*).(?&lt;yuanyin&gt;[%&#123;WORD&#125;-]+).(?&lt;mess&gt;.*)&quot;#            &#125; #            overwrite =&gt; [&quot;message&quot;]#        &#125;#    &#125;       &#125;output&#123;    if [type] == &quot;monitoring-log&quot; &#123;        elasticsearch &#123;            user =&gt; &quot;elastic&quot;            password =&gt; &quot;Root@123&quot;            hosts =&gt; [&quot;192.168.11.156:9200&quot;,                      &quot;192.168.11.157:9200&quot;,                      &quot;192.168.11.159:9200&quot;]            index =&gt; &quot;monitoring-log-%&#123;+YYYY.MM&#125;&quot;        &#125;    &#125;      if [type] == &quot;metricbeat&quot; &#123;        elasticsearch &#123;            user =&gt; &quot;elastic&quot;            password =&gt; &quot;Root@123&quot;            hosts =&gt; [&quot;192.168.11.156:9200&quot;,                      &quot;192.168.11.157:9200&quot;,                      &quot;192.168.11.159:9200&quot;]            index =&gt; &quot;metricbeat-%&#123;+YYYY.MM&#125;&quot;        &#125;    &#125;     if [type] == &quot;oracle-irm&quot; &#123;        elasticsearch &#123;            user =&gt; &quot;elastic&quot;            password =&gt; &quot;Root@123&quot;            hosts =&gt; [&quot;192.168.11.156:9200&quot;,                      &quot;192.168.11.157:9200&quot;,                      &quot;192.168.11.159:9200&quot;]            index =&gt; &quot;oracle-irm-%&#123;+YYYY.MM&#125;&quot;        &#125;    &#125; #    stdout &#123; #        codec =&gt; rubydebug#    &#125;&#125;\n\n3.启动logstash；\n#!/bin/shnohup ./bin/logstash -f config/logstash.conf &amp; &gt;./nohup.out\n\n五.安装kibana1.上传并解压kibana-7.14.0-linux-x86_64.tar.gz压缩包到&#x2F;data；\n2.修改配置文件；\n# Kibana is served by a back end server. This setting specifies the port to use.#server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &#x27;localhost&#x27;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.#server.host: &quot;localhost&quot;server.host: &quot;0.0.0.0&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy.# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath# from requests it receives, and to prevent a deprecation warning at startup.# This setting cannot end in a slash.#server.basePath: &quot;&quot;# Specifies whether Kibana should rewrite requests that are prefixed with# `server.basePath` or require that they are rewritten by your reverse proxy.# This setting was effectively always `false` before Kibana 6.3 and will# default to `true` starting in Kibana 7.0.#server.rewriteBasePath: false# Specifies the public URL at which Kibana is available for end users. If# `server.basePath` is configured this URL should end with the same basePath.#server.publicBaseUrl: &quot;&quot;# The maximum payload size in bytes for incoming server requests.#server.maxPayload: 1048576# The Kibana server&#x27;s name.  This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URLs of the Elasticsearch instances to use for all your queries.\n\n3.启动kibana；\n#!/bin/shsudo -u elk /data/kibana-7.14.0/bin/kibana -c config/kibana.yml &amp; &gt;/data/kibana-7.14.0/kibana.log\n\n六.安装filebeat1.上传并解压filebeat-7.14.0-linux-x86_64.tar.gz文件到&#x2F;data;\n2.修改配置文件；\ncd /data/filebeat-7.14.0vi filebeat.yml\n\n\n\n\n3.启动filebeat；\n#!/bin/shnohup ./filebeat -e -c filebeat.yml &amp; &gt;./nohup.out\n\n5.安装windows版本filebeat，上传filebeat-7.14.0-windows-x86.zip，并解压；\n6.修改配置文件同上；\n7.注册成windows服务(注意安装路径修改)\nsc create filebeat-7.14.0 binpath= &quot;D:\\filebeat-7.14.0\\filebeat.exe -e -c D:\\filebeat-7.14.0\\filebeat.yml&quot; type= own start= auto displayname= filebeat-7.14.0\n\n8.启动windows服务；\n七.访问kibana查看数据1.http://192.168.11.156:5601/\n如下图，为接入数据后的效果:\n\n\n完(持续更新)\n","categories":["ELK","Kafka"],"tags":["ELK","Kafka"]},{"title":"ELK之es设置默认分片和副本数","url":"/20241121/ELK/b76725c8a6ba/","content":"ELK之es设置默认分片和副本数elasticsearch6设置索引的默认分片数和副本数已经不是在elasticsearch.yml文件中了，而是使用了一个索引模板的东西。\nlinux 命令执行curl -XPUT &#x27;http://xxxxx:9200/_template/template_http_request_record&#x27;   -H &#x27;Content-Type: application/json&#x27; -d  &#x27;&#123;&quot;index_patterns&quot;: [&quot;*&quot;],&quot;settings&quot;: &#123;&quot;number_of_shards&quot;: 5,&quot;number_of_replicas&quot;: 2&#125;&#125;&#x27;\n\njson接口_template/template_http_request_record&#123;    &quot;index_patterns&quot;: \\[&quot;\\*&quot;\\],  &quot;settings&quot;: &#123;     &quot;number_of_shards&quot;: 5,     &quot;number_of_replicas&quot;: 2   &#125;  &#125;\n\n\nindex_patterns 这个是作用于索引匹配\nnumber_of_shards 分片数量\nnumber_of_replicas 副本数量\n\nhead插件查询\n","categories":["ELK"],"tags":["ELK"]},{"title":"ELK定时清理索引","url":"/20241121/ELK/a15a7bd2d657/","content":"ELK定时清理索引#!/bin/bash#定时清除elk索引，7天DATE=`date -d &quot;7 days ago&quot; +%Y.%m.%d`    &gt;&gt; /tmp/clean_logstash.logINDEX=`curl -XGET &#x27;http://127.0.0.1:9200/_cat/indices/?v&#x27;|awk &#x27;&#123;print $3&#125;&#x27;`   &gt;&gt; /tmp/clean_logstash.logcurl -XDELETE  http://127.0.0.1:9200/*-$&#123;DATE&#125;   &gt;&gt; /tmp/clean_logstash.log\n\n","categories":["ELK"],"tags":["ELK"]},{"title":"ELK整体优化配置","url":"/20241121/ELK/b68d15e9f095/","content":"ELK整体优化配置一、elk 实用知识点总结编码转换问题（1）input 中的codec &#x3D;&gt; plain 转码\ncodec =&gt; plain &#123;         charset =&gt; &quot;GB2312&quot;&#125;\n\n将GB2312 的文本编码，转为UTF-8 的编码\n（2）也可以在filebeat中实现编码的转换（推荐）\nfilebeat.prospectors:- input_type: log  paths:    - c:\\Users\\Administrator\\Desktop\\performanceTrace.txt  encoding: GB2312\n\n删除日志中的多余的行（1）logstash filter 中drop 删除\nif ([message] =~ &quot;^20.*-\\ task\\ request,.*,start\\ time.*&quot;) &#123;   #用正则需删除的多余行     drop &#123;&#125;&#125; \n\ngrok 处理多种日志不同的行（1）日志示例：\n2018-03-20 10:44:01,523 [33]DEBUG Debug - task request,task Id:1cbb72f1-a5ea-4e73-957c-6d20e9e12a7a,start time:2018-03-20 10:43:59-- Request String : &#123;&quot;UserName&quot;:&quot;15046699923&quot;,&quot;Pwd&quot;:&quot;ZYjyh727&quot;,&quot;DeviceType&quot;:2,&quot;DeviceId&quot;:&quot;PC-20170525SADY&quot;,&quot;EquipmentNo&quot;:null,&quot;SSID&quot;:&quot;pc&quot;,&quot;RegisterPhones&quot;:null,&quot;AppKey&quot;:&quot;ab09d78e3b2c40b789ddfc81674bc24deac&quot;,&quot;Version&quot;:&quot;2.0.5.3&quot;&#125; -- End-- Response String : &#123;&quot;ErrorCode&quot;:0,&quot;Success&quot;:true,&quot;ErrorMsg&quot;:null,&quot;Result&quot;:null,&quot;WaitInterval&quot;:30&#125; -- End\n\n（2）在logstash filter中grok 分别处理3行\nmatch =&gt; &#123;    &quot;message&quot; =&gt; &quot;^20.*-\\ task\\ request,.*,start\\ time\\:%&#123;TIMESTAMP_ISO8601:RequestTime&#125;&quot;match =&gt; &#123;    &quot;message&quot; =&gt; &quot;^--\\ Request\\ String\\ :\\ \\&#123;\\&quot;UserName\\&quot;:\\&quot;%&#123;NUMBER:UserName:int&#125;\\&quot;,\\&quot;Pwd\\&quot;:\\&quot;(?&lt;Pwd&gt;.*)\\&quot;,\\&quot;DeviceType\\&quot;:%&#123;NUMBER:DeviceType:int&#125;,\\&quot;DeviceId\\&quot;:\\&quot;(?&lt;DeviceId&gt;.*)\\&quot;,\\&quot;EquipmentNo\\&quot;:(?&lt;EquipmentNo&gt;.*),\\&quot;SSID\\&quot;:(?&lt;SSID&gt;.*),\\&quot;RegisterPhones\\&quot;:(?&lt;RegisterPhones&gt;.*),\\&quot;AppKey\\&quot;:\\&quot;(?&lt;AppKey&gt;.*)\\&quot;,\\&quot;Version\\&quot;:\\&quot;(?&lt;Version&gt;.*)\\&quot;\\&#125;\\ --\\ \\End.*&quot;    &#125;match =&gt; &#123;    &quot;message&quot; =&gt; &quot;^--\\ Response\\ String\\ :\\ \\&#123;\\&quot;ErrorCode\\&quot;:%&#123;NUMBER:ErrorCode:int&#125;,\\&quot;Success\\&quot;:(?&lt;Success&gt;[a-z]*),\\&quot;ErrorMsg\\&quot;:(?&lt;ErrorMsg&gt;.*),\\&quot;Result\\&quot;:(?&lt;Result&gt;.*),\\&quot;WaitInterval\\&quot;:%&#123;NUMBER:WaitInterval:int&#125;\\&#125;\\ --\\ \\End.*&quot;&#125;... 等多行\n\n日志多行合并处理—multiline插件（重点）（1）日志示例：\n018-03-20 10:44:01,523 [33]DEBUG Debug - task request,task Id:1cbb72f1-a5ea-4e73-957c-6d20e9e12a7a,start time:2018-03-20 10:43:59-- Request String : &#123;&quot;UserName&quot;:&quot;15046699923&quot;,&quot;Pwd&quot;:&quot;ZYjyh727&quot;,&quot;DeviceType&quot;:2,&quot;DeviceId&quot;:&quot;PC-20170525SADY&quot;,&quot;EquipmentNo&quot;:null,&quot;SSID&quot;:&quot;pc&quot;,&quot;RegisterPhones&quot;:null,&quot;AppKey&quot;:&quot;ab09d78e3b2c40b789ddfc81674bc24deac&quot;,&quot;Version&quot;:&quot;2.0.5.3&quot;&#125; -- End-- Response String : &#123;&quot;ErrorCode&quot;:0,&quot;Success&quot;:true,&quot;ErrorMsg&quot;:null,&quot;Result&quot;:null,&quot;WaitInterval&quot;:30&#125; -- End\n\n（2）logstash grok 对合并后多行的处理（合并多行后续都一样，如下）\nfilter &#123;　　grok &#123;　　　　match =&gt; &#123;　　　　　　&quot;message&quot; =&gt; &quot;^%&#123;TIMESTAMP_ISO8601:InsertTime&#125;\\ .*-\\ task\\ request,.*,start\\ time:%&#123;TIMESTAMP_ISO8601:RequestTime&#125;\\n--\\ Request\\ String\\ :\\ \\&#123;\\&quot;UserName\\&quot;:\\&quot;%&#123;NUMBER:UserName:int&#125;\\&quot;,\\&quot;Pwd\\&quot;:\\&quot;(?&lt;Pwd&gt;.*)\\&quot;,\\&quot;DeviceType\\&quot;:%&#123;NUMBER:DeviceType:int&#125;,\\&quot;DeviceId\\&quot;:\\&quot;(?&lt;DeviceId&gt;.*)\\&quot;,\\&quot;EquipmentNo\\&quot;:(?&lt;EquipmentNo&gt;.*),\\&quot;SSID\\&quot;:(?&lt;SSID&gt;.*),\\&quot;RegisterPhones\\&quot;:(?&lt;RegisterPhones&gt;.*),\\&quot;AppKey\\&quot;:\\&quot;(?&lt;AppKey&gt;.*)\\&quot;,\\&quot;Version\\&quot;:\\&quot;(?&lt;Version&gt;.*)\\&quot;\\&#125;\\ --\\ \\End\\n--\\ Response\\ String\\ :\\ \\&#123;\\&quot;ErrorCode\\&quot;:%&#123;NUMBER:ErrorCode:int&#125;,\\&quot;Success\\&quot;:(?&lt;Success&gt;[a-z]*),\\&quot;ErrorMsg\\&quot;:(?&lt;ErrorMsg&gt;.*),\\&quot;Result\\&quot;:(?&lt;Result&gt;.*),\\&quot;WaitInterval\\&quot;:%&#123;NUMBER:WaitInterval:int&#125;\\&#125;\\ --\\ \\End&quot;　　　　&#125;　　&#125;&#125;\n\n（3）在filebeat中使用multiline 插件（推荐）\n5.5版本之后（before为例）\nfilebeat.prospectors:- input_type: log  paths:    - /root/performanceTrace*  fields:    type: zidonghualog  multiline.pattern: &#x27;.*\\&quot;WaitInterval\\&quot;:.*--\\ End&#x27;  multiline.negate: true  multiline.match: before\n\n5.5版本之前（after为例）\nfilebeat.prospectors:- input_type: log      paths:      - /root/performanceTrace*      input_type: log       multiline:           pattern: &#x27;^20.*&#x27;           negate: true           match: after\n\nlogstash filter 中的date使用（1） 日志示例\n2018-03-20 10:44:01 [33]DEBUG Debug - task request,task Id:1cbb72f1-a5ea-4e73-957c-6d20e9e12a7a,start time:2018-03-20 10:43:59\n\n（2） date 使用\ndate &#123;            match =&gt; [&quot;InsertTime&quot;,&quot;YYYY-MM-dd HH:mm:ss &quot;]            remove_field =&gt; &quot;InsertTime&quot;&#125;\n\n注：\nmatch =&gt; [&quot;timestamp&quot; ,&quot;dd/MMM/YYYY H:m:s Z&quot;]　　匹配这个字段，字段的格式为：日日/月月月/年年年年 时/分/秒 时区也可以写为：match =&gt; [&quot;timestamp&quot;,&quot;ISO8601&quot;]（推荐）\n\n（3）date 介绍\n就是将匹配日志中时间的key 替换为@timestamp 的时间，因为@timestamp 的时间是日志送到logstash 的时间，并不是日志中真正的时间。\n\n对多类日志分类处理（重点） 在filebeat 的配置中添加type 分类\nfilebeat:  prospectors:    -      paths:        #- /mnt/data/WebApiDebugLog.txt*        - /mnt/data_total/WebApiDebugLog.txt*      fields:        type: WebApiDebugLog_total    -      paths:        - /mnt/data_request/WebApiDebugLog.txt*        #- /mnt/data/WebApiDebugLog.txt*      fields:        type: WebApiDebugLog_request    -      paths:        - /mnt/data_report/WebApiDebugLog.txt*        #- /mnt/data/WebApiDebugLog.txt*      fields:        type: WebApiDebugLog_report\n\n在logstash filter中使用if，可进行对不同类进行不同处理\nfilter &#123;   if [fields][type] == &quot;WebApiDebugLog_request&quot; &#123;   #对request 类日志        if ([message] =~ &quot;^20.*-\\ task\\ report,.*,start\\ time.*&quot;) &#123;   #删除report 行                drop &#123;&#125;        &#125;    grok &#123;        match =&gt; &#123;&quot;... ...&quot;&#125;        &#125;&#125;\n\n在logstash output中使用if\nif [fields][type] == &quot;WebApiDebugLog_total&quot; &#123;    elasticsearch &#123;        hosts =&gt; [&quot;6.6.6.6:9200&quot;]        index =&gt; &quot;logstashl-WebApiDebugLog_total-%&#123;+YYYY.MM.dd&#125;&quot;        document_type =&gt; &quot;WebApiDebugLog_total_logs&quot;&#125; \n\n二、对elk 整体性能的优化关于收集日志的选择：logstash&#x2F;filter1）没有原则要求使用filebeat或logstash，两者作为shipper的功能是一样的，区别在于：\n① logstash由于集成了众多插件，如grok，ruby，所以相比beat是重量级的；\n② logstash启动后占用资源更多，如果硬件资源足够则无需考虑二者差异；\n③ logstash基于JVM，支持跨平台；而beat使用golang编写，AIX不支持；\n④ AIX 64bit平台上需要安装jdk（jre） 1.7 32bit，64bit的不支持；\n⑤ filebeat可以直接输入到ES，但是系统中存在logstash直接输入到ES的情况，这将造成不同的索引类型造成检索复杂，最好统一输入到els 的源。\n（2）总结\n　　logstash&#x2F;filter 总之各有千秋，但是，我推荐选择：在每个需要收集的日志服务器上配置filebeat，因为轻量级，用于收集日志；再统一输出给logstash，做对日志的处理；最后统一由logstash 输出给els。\nlogstash的优化相关配置（1）可以优化的参数，可根据自己的硬件进行优化配置\n① pipeline 线程数，官方建议是等于CPU内核数\n默认配置 —&gt; pipeline.workers: 2\n可优化为 —&gt; pipeline.workers: CPU内核数（或几倍cpu内核数）\n② 实际output 时的线程数\n默认配置 —&gt; pipeline.output.workers: 1\n可优化为 —&gt; pipeline.output.workers: 不超过pipeline 线程数\n③ 每次发送的事件数\n默认配置 —&gt; pipeline.batch.size: 125\n可优化为 —&gt; pipeline.batch.size: 1000\n④ 发送延时\n默认配置 —&gt; pipeline.batch.delay: 5\n可优化为 —&gt; pipeline.batch.size: 10\n（2）总结\n　　通过设置-w参数指定pipeline worker数量，也可直接修改配置文件logstash.yml。这会提高filter和output的线程数，如果需要的话，将其设置为cpu核心数的几倍是安全的，线程在I&#x2F;O上是空闲的。\n　　默认每个输出在一个pipeline worker线程上活动，可以在输出output 中设置workers设置，不要将该值设置大于pipeline worker数。\n　　还可以设置输出的batch_size数，例如ES输出与batch size一致。\n　　filter设置multiline后，pipline worker会自动将为1，如果使用filebeat，建议在beat中就使用multiline，如果使用logstash作为shipper，建议在input 中设置multiline，不要在filter中设置multiline。\n（3）Logstash中的JVM配置文件\n　　Logstash是一个基于Java开发的程序，需要运行在JVM中，可以通过配置jvm.options来针对JVM进行设定。比如内存的最大最小、垃圾清理机制等等。JVM的内存分配不能太大不能太小，太大会拖慢操作系统。太小导致无法启动。默认如下：\n-Xms256m #最小使用内存\n-Xmx1g #最大使用内存\nelasticsearch 节点优化配置（1）服务器硬件配置，OS 参数\n（a） &#x2F;etc&#x2F;sysctl.conf 配置\nvim &#x2F;etc&#x2F;sysctl.conf\nvm.swappiness = 1                     #ES 推荐将此参数设置为 1，大幅降低 swap 分区的大小，强制最大程度的使用内存，注意，这里不要设置为 0, 这会很可能会造成 OOMnet.core.somaxconn = 65535     #定义了每个端口最大的监听队列的长度vm.max_map_count= 262144    #限制一个进程可以拥有的VMA(虚拟内存区域)的数量。虚拟内存区域是一个连续的虚拟地址空间区域。当VMA 的数量超过这个值，OOMfs.file-max = 518144                   #设置 Linux 内核分配的文件句柄的最大数量\n\n[root@elasticsearch]# sysctl -p 生效一下\n（b）limits.conf 配置\nvim &#x2F;etc&#x2F;security&#x2F;limits.conf\nelasticsearch    soft    nofile          65535elasticsearch    hard    nofile          65535elasticsearch    soft    memlock         unlimitedelasticsearch    hard    memlock         unlimited\n\n（c）为了使以上参数永久生效，还要设置两个地方\nvim &#x2F;etc&#x2F;pam.d&#x2F;common-session-noninteractive\nvim &#x2F;etc&#x2F;pam.d&#x2F;common-session\n添加如下属性：\nsession required pam_limits.so\n可能需重启后生效\n（2）elasticsearch 中的JVM配置文件\n-Xms2g\n-Xmx2g\n① 将最小堆大小（Xms）和最大堆大小（Xmx）设置为彼此相等。\n② Elasticsearch可用的堆越多，可用于缓存的内存就越多。但请注意，太多的堆可能会使您长时间垃圾收集暂停。\n③ 设置Xmx为不超过物理RAM的50％，以确保有足够的物理内存留给内核文件系统缓存。\n④ 不要设置Xmx为JVM用于压缩对象指针的临界值以上；确切的截止值有所不同，但接近32 GB。不要超过32G，如果空间大，多跑几个实例，不要让一个实例太大内存\n（3）elasticsearch 配置文件优化参数\n① vim elasticsearch.yml\nbootstrap.memory_lock: true  #锁住内存，不使用swap#缓存、线程等优化如下bootstrap.mlockall: truetransport.tcp.compress: trueindices.fielddata.cache.size: 40%indices.cache.filter.size: 30%indices.cache.filter.terms.size: 1024mbthreadpool:    search:        type: cached        size: 100        queue_size: 2000\n\n② 设置环境变量\nvim &#x2F;etc&#x2F;profile.d&#x2F;elasticsearch.sh export ES_HEAP_SIZE&#x3D;2g  #Heap Size不超过物理内存的一半，且小于32G\n（4）集群的优化\n① ES是分布式存储，当设置同样的cluster.name后会自动发现并加入集群；\n② 集群会自动选举一个master，当master宕机后重新选举；\n③ 为防止”脑裂”，集群中个数最好为奇数个\n④ 为有效管理节点，可关闭广播 discovery.zen.ping.multicast.enabled: false，并设置单播节点组discovery.zen.ping.unicast.hosts: [“ip1”, “ip2”, “ip3”]\n来自[阿龙ELK重难点总结和整体优化配置 ]\n","categories":["ELK"],"tags":["ELK"]},{"title":"ELK架构说明","url":"/20241121/ELK/30975934672d/","content":"ELK架构说明一.ELK介绍1.ELK简介ELK是Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称(但是后期出现的filebeat(beats中的一种)可以用来替代logstash的数据收集功能，比较轻量级)。市面上也被成为Elastic Stack。\n　　Filebeat是用于转发和集中日志数据的轻量级传送工具。Filebeat监视您指定的日志文件或位置，收集日志事件，并将它们转发到Elasticsearch或 Logstash进行索引。Filebeat的工作方式如下：启动Filebeat时，它将启动一个或多个输入，这些输入将在为日志数据指定的位置中查找。对于Filebeat所找到的每个日志，Filebeat都会启动收集器。每个收集器都读取单个日志以获取新内容，并将新日志数据发送到libbeat，libbeat将聚集事件，并将聚集的数据发送到为Filebeat配置的输出。\n　　Logstash是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。Logstash能够动态地采集、转换和传输数据，不受格式或复杂度的影响。利用Grok从非结构化数据中派生出结构，从IP地址解码出地理坐标，匿名化或排除敏感字段，并简化整体处理过程。\n　　Elasticsearch是Elastic Stack核心的分布式搜索和分析引擎,是一个基于Lucene、分布式、通过Restful方式进行交互的近实时搜索平台框架。Elasticsearch为所有类型的数据提供近乎实时的搜索和分析。无论您是结构化文本还是非结构化文本，数字数据或地理空间数据，Elasticsearch都能以支持快速搜索的方式有效地对其进行存储和索引。\n　　Kibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。使用Kibana，可以通过各种图表进行高级数据分析及展示。并且可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以汇总、分析和搜索重要数据日志。还可以让海量数据更容易理解。它操作简单，基于浏览器的用户界面可以快速创建仪表板（dashboard）实时显示Elasticsearch查询动态\n2.为什么要使用ELK日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。\n　　往往单台机器的日志我们使用grep、awk等工具就能基本实现简单分析，但是当日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。\n　　一般大型系统是一个分布式部署的架构，不同的服务模块部署在不同的服务器上，问题出现时，大部分情况需要根据问题暴露的关键信息，定位到具体的服务器和服务模块，构建一套集中式日志系统，可以提高定位问题的效率。\n3.完整日志系统基本特征\n收集：能够采集多种来源的日志数据\n传输：能够稳定的把日志数据解析过滤并传输到存储系统\n存储：存储日志数据\n分析：支持 UI 分析\n警告：能够提供错误报告，监控机制\n\n二.ELK架构分析2.1、beats+elasticsearch+kibana模式\n　　如上图所示，该ELK框架由beats（日志分析我们通常使用filebeat）+elasticsearch+kibana构成，这个框架比较简单，入门级的框架。其中filebeat也能通过module对日志进行简单的解析和索引。并查看预建的Kibana仪表板。\n该框架适合简单的日志数据，一般可以用来玩玩，生产环境建议接入logstash\n2.2、beats+logstash+elasticsearch+kibana模式\n该框架是在上面的框架的基础上引入了logstash，引入logstash带来的好处如下：\n\n通Logstash具有基于磁盘的自适应缓冲系统，该系统将吸收传入的吞吐量，从而减轻背压\n从其他数据源（例如数据库，S3或消息传递队列）中提取\n将数据发送到多个目的地，例如S3，HDFS或写入文件\n使用条件数据流逻辑组成更复杂的处理管道\n\nfilebeat结合logstash带来的优势：\n1、水平可扩展性，高可用性和可变负载处理：filebeat和logstash可以实现节点之间的负载均衡，多个logstash可以实现logstash的高可用\n2、消息持久性与至少一次交付保证：使用Filebeat或Winlogbeat进行日志收集时，可以保证至少一次交付。从Filebeat或Winlogbeat到Logstash以及从Logstash到Elasticsearch的两种通信协议都是同步的，并且支持确认。Logstash持久队列提供跨节点故障的保护。对于Logstash中的磁盘级弹性，确保磁盘冗余非常重要。\n3、具有身份验证和有线加密的端到端安全传输：从Beats到Logstash以及从 Logstash到Elasticsearch的传输都可以使用加密方式传递 。与Elasticsearch进行通讯时，有很多安全选项，包括基本身份验证，TLS，PKI，LDAP，AD和其他自定义领域\n当然在该框架的基础上还可以引入其他的输入数据的方式：比如：TCP，UDP和HTTP协议是将数据输入Logstash的常用方法（如下图所示）：\n\n2.3、beats+缓存&#x2F;消息队列+logstash+elasticsearch+kibana模式\n在如上的基础上我们可以在beats和logstash中间添加一些组件redis、kafka、RabbitMQ等，添加中间件将会有如下好处： 第一，降低对日志所在机器的影响，这些机器上一般都部署着反向代理或应用服务，本身负载就很重了，所以尽可能的在这些机器上少做事； 第二，如果有很多台机器需要做日志收集，那么让每台机器都向Elasticsearch持续写入数据，必然会对Elasticsearch造成压力，因此需要对数据进行缓冲，同时，这样的缓冲也可以一定程度的保护数据不丢失； 第三，将日志数据的格式化与处理放到Indexer中统一做，可以在一处修改代码、部署，避免需要到多台机器上去修改配置 ；\n总结消息系统主要功能1、解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束 2、冗余 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 3、扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 4、灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。 如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 5、可恢复性 　系统的一部分组件失效时，不会影响到整个系统。 消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 6、顺序保证 在大多使用场景下，数据处理的顺序都很重要。 大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性） 7、缓冲 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 8、异步通信 很多时候，用户不想也不需要立即处理消息。 消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\nRedis与Kafka我们都知道Redis是以key的hash方式来分散对列存储数据的，且Redis作为集群使用时，对应的应用对应一个Redis，在某种程度上会造成数据的倾斜性，从而导致数据的丢失。 而从之前我们部署Kafka集群来看，kafka的一个topic（主题），可以有多个partition（副本），而且是均匀的分布在Kafka集群上，这就不会出现redis那样的数据倾斜性。Kafka同时也具备Redis的冗余机制，像Redis集群如果有一台机器宕掉是很有可能造成数据丢失，而Kafka因为是均匀的分布在集群主机上，即使宕掉一台机器，是不会影响使用。同时Kafka作为一个订阅消息系统，还具备每秒百万级别的高吞吐量，持久性的、分布式的特点等。\n","categories":["ELK"],"tags":["ELK"]},{"title":"Flink集群搭建","url":"/20241121/Hadoop/80555a687b5d/","content":"Flink集群搭建Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。\n在安装spark之前，需要安装hadoop集群环境\n集群列表\n\n\n服务器\n地址\n角色\n备注\n\n\n\nhadoop1\n192.168.11.81\nmaster slaves\n32G 12C 800G\n\n\nhadoop2\n192.168.11.82\nmaster slaves\n32G 12C 800G\n\n\nhadoop3\n192.168.11.83\nslaves\n32G 12C 800G\n\n\n一.基础环境设置关闭防火墙systemctl stop firewalldsystemctl disable firewalld\n\n关闭selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/configsetenforce 0\n\n添加hostsvi /etc/hosts192.168.11.81 hadoop1192.168.11.82 hadoop2192.168.11.83 hadoop3\n\n二.安装flink集群下载flink安装包https://archive.apache.org/dist/flink/https://archive.apache.org/dist/flink/flink-1.14.4/flink-1.14.4-bin-scala_2.12.tgz\n\n上传解压flink安装包tar -zxvf flink-1.14.4-bin-scala_2.12.tgz -C /data\n\n配置环境变量cat &gt;&gt;/etc/profile &lt;&lt;EOF# flinkexport FLINK_HOME=/data/flink-1.14.4export PATH=$PATH:$FLINK_HOME/binEOFsource /etc/profile\n\n配置flink参数cd $FLINK_HOME/confvi flink-conf.yaml# 添加jobmanager.rpc.address: hadoop1\n\n配置工作节点\nvi workers# 添加hadoop2hadoop3\n\n注:以上配置需要复制到其它节点中\n启动服务cd $FLINK_HOME/bin./start-cluster.sh\n\n关闭服务cd $FLINK_HOME/bin./stop-cluster.sh\n\n五.web可视化http://192.168.11.81:8081\n\n六.配置flink集群Standalone HA模式cd $FLINK_HOME/confvi masters# 修改hadoop2:8081hadoop1:8081\n\nvi workers# 修改hadoop1hadoop2hadoop3\n\ncd $FLINK_HOME/confvi flink-conf.yaml# 修改# 基础配置jobmanager.rpc.address: westgis181jobmanager.heap.size: 1024mtaskmanager.heap.size: 1024mtaskmanager.numberOfTaskSlots: 3parallelism.default: 5# 指定使用 zookeeper 进行 HA 协调high-availability: zookeeperhigh-availability.storageDir: hdfs:///user/flink/recoveryhigh-availability.zookeeper.quorum: 192.168.11.160:2181high-availability.zookeeper.path.root: /flink# 指定 checkpoint 的类型和对应的数据存储目录state.backend: filesystemstate.checkpoints.dir: hdfs://westgis181:9000/flink/flink-checkpointsjobmanager.execution.failover-strategy: region# Rest和网络配置rest.port: 8081rest.address: westgis181# 高级配置，临时文件目录io.tmp.dirs: /tmp# 配置 HistoryServejobmanager.archive.fs.dir: hdfs:///user/flink/recovery/completed-jobs/historyserver.web.address: westgis182historyserver.web.port: 8082historyserver.archive.fs.dir: hdfs:///user/flink/recovery/completed-jobs/historyserver.archive.fs.refresh-interval: 1000\n\ncd $FLINK_HOME/confvi zoo.cfg# 修改server.1=192.168.11.160:2888:3888server.2=192.168.11.161:2888:3888server.3=192.168.11.162:2888:3888\n\n根据自身hadoop版本和flink版本下载相应的依赖包，然后上传到$FLINK_HOME &#x2F;lib目录下，最后分发给其他节点。\n下载地址\nhttps://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/\n\n启动服务cd $FLINK_HOME/bin./start-cluster.sh\n\n验证\n[root@hadoop1 conf]# jps7616 NodeManager7074 DataNode7331 ResourceManager708 TaskManagerRunner10340 TaskManagerRunner25132 Jps7955 Master20599 TaskManagerRunner  # maste节点 flink程序6939 NameNode9243 StandaloneSessionClusterEntrypoint  # maste节点 flink程序8123 TaskManagerRunner9565 TaskManagerRunner\n\n[root@hadoop2 ~]# jps5378 Worker7331 Jps5140 NodeManager6107 TaskManagerRunner # standby节点 flink程序5022 DataNode\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"Go语言编程","url":"/20241121/Go/0344435ac4a9/","content":"Go语言编程\n\n\n\n\nThe End\n","categories":["Go"],"tags":["Go"]},{"title":"Hadoop集群搭建与调优","url":"/20241121/Hadoop/d00818e6a2e2/","content":"hadoop集群搭建集群列表\n\n\n服务器\n地址\n角色\n备注\n\n\n\nhadoop1\n192.168.11.81\nnamenode datanode\n32G 12C 800G\n\n\nhadoop2\n192.168.11.82\nsencondarynode datanode\n32G 12C 800G\n\n\nhadoop3\n192.168.11.83\ndatanode\n32G 12C 800G\n\n\nmysql8\n192.168.11.72\n元数据数据库\n\n\n\n一.基础环境设置关闭防火墙systemctl stop firewalldsystemctl disable firewalld\n\n关闭selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/configsetenforce 0\n\n创建用户和组useradd hadoopgroupadd hadoop\n\n添加hostsvi /etc/hosts192.168.11.81 hadoop1192.168.11.82 hadoop2192.168.11.83 hadoop3\n\n开启ssh免密登录ssh-keygen -t rsassh-copy-id hadoop1ssh-copy-id hadoop2ssh-copy-id hadoop3\n\n安装jdk1.8tar -xzvf jdk-8u271-linux-x64.tar.gz -C /cat &gt;&gt;/etc/profile &lt;&lt;EOF# java1.8export JAVA_HOME=/jdk1.8.0_271export JRE_HOME=/jdk1.8.0_271/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHEOF# 加载环境变量source /etc/profile# 验证javajava -version\n\n注:其它节点也需要安装jdk\n二.安装hadoop下载hadoop包http://archive.apache.org/dist/hadoop/core/https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop\n\n上传解压hadoop包tar -zxvf hadoop-2.6.4.tar.gz -C /data\n\n添加环境变量cat &gt;&gt;/etc/profile &lt;&lt;EOF# hadoopexport HADOOP_HOME=/data/hadoop-2.6.4export PATH=$HADOOP_HOME/bin:$PATHEOFsource /etc/profile\n\n配置hadoop参数配置文件放在$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;下\ncd $HADOOP_HOME/etc/hadoop/vi hadoop-env.shexport JAVA_HOME=/jdk1.8.0_271vi yarn-env.shexport JAVA_HOME=/jdk1.8.0_271\n\n配置工作节点\nvi slaveshadoop1hadoop2hadoop3\n\n配置hadoop参数\nvi core-site.xml\n\n &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/dfs/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;!-- HUE配置，没有可忽略 --&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;\n\nvi hdfs-site.xml\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;/dfs/nn/&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;/dfs/dn/&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;3&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.nameservices&lt;/name&gt;                &lt;value&gt;hadoop1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;hadoop2:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.permissions&lt;/name&gt;                &lt;value&gt;false&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;\n\ncp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml\n\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;                &lt;final&gt;true&lt;/final&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;                &lt;value&gt;hadoop3:50030&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;hadoop3:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;hadoop3:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapred.job.tracker&lt;/name&gt;                &lt;value&gt;hadoop3:9001&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;\n\nvi yarn-site.xml\n\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;hadoop1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;                &lt;value&gt;hadoop1:8032&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;                &lt;value&gt;hadoop1:8030&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;                &lt;value&gt;hadoop1:8031&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;                &lt;value&gt;hadoop1:8033&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;                &lt;value&gt;hadoop1:8088&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;                &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;        &lt;/property&gt;       &lt;property&gt;              &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;             &lt;value&gt;false&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;\n\n注:以上配置需要复制到其它节点中\n三.验证安装格式化文件系统注意:namenode第一次需要格式化文件系统，格式化后切忌不要再格式化!!!\nnamenode格式化文件系统\nhadoop namenode -format\n\n若没有hadoop命令可去$HADOOP_HOME&#x2F;bin中寻找\n同步Namenode数据（其他NN节点）\nhdfs namenode -bootstrapStandby\n\n启动服务启动namenode进程\ncd $HADOOP_HOME/sbinhadoop-daemon.sh start namenode\n\n启动datanode进程\ncd $HADOOP_HOME/sbinhadoop-daemon.sh start datanode\n\n或者一次性启动\ncd $HADOOP_HOME/sbin./start-all.sh\n\n其他启动命令\n# 初始化ZKFChdfs zkfc -formatZK# 启动初始化的NameNodehdfs --daemon start namenode# 启动JournalNodehdfs --daemon start journalnode# 停止NameNode服务hdfs --daemon stop namenode# 停止JournalNode服务hdfs --daemon stop journalnode# 启动hdfsstart-dfs.sh\n\n检查节点配置情况[root@hadoop1 ~]# jps7616 NodeManager7074 DataNode7331 ResourceManager13703 Jps6939 NameNode\n\n关闭服务$HADOOP_HOME&#x2F;sbin有相应的停止脚本\n# 停止所有服务cd $HADOOP_HOME/sbin./stop-all.sh\n\n四.web可视化各个服务启动成功后会有相应的web界面\n# 节点管理http://192.168.11.81:8088# 资源管理http://192.168.11.81:50070 \n\n五.配置文件示例\n以下为hadoop3的配置(仅供参考,配置低参数要适当调小)\n\nhadoop-env.sh\nexport JAVA_HOME=/java/jdk1.8.0_191export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_JOURNALNODE_USER=rootexport HDFS_ZKFC_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx4096m&quot;export HADOOP_PID_DIR=/opt/hadoop-3.1.3/pid\n\ncore-site.xml\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 把多个 NameNode 的地址组装成一个集群mycluster --&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://mycluster&lt;/value&gt;  &lt;/property&gt; &lt;!-- 指定 hadoop 数据的存储目录 --&gt;  &lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/opt/hadoop-3.1.3/data&lt;/value&gt;  &lt;/property&gt; &lt;!-- 配置 HDFS 网页登录使用的静态用户 --&gt;  &lt;property&gt;  &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;  &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt;  &lt;property&gt;      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;      &lt;value&gt;root&lt;/value&gt;  &lt;/property&gt;    &lt;!-- 整合hive 用户代理设置 --&gt;  &lt;property&gt;      &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;      &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;   &lt;property&gt;      &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;      &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt; &lt;!-- 配置 开启回收站 --&gt;  &lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;60&lt;/value&gt;  &lt;/property&gt; &lt;!-- 指定zkfc要连接的zkServer地址--&gt;  &lt;property&gt;    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;192.168.11.201:2181,192.168.11.202:2181,192.168.11.203:2181&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;\n\nhdfs-site.xml\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- NameNode 数据存储目录--&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/name1,file://$&#123;hadoop.tmp.dir&#125;/name2&lt;/value&gt;  &lt;/property&gt; &lt;!-- DataNode 数据存储目录--&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;file:///data1,file:///data2&lt;/value&gt;  &lt;/property&gt; &lt;!-- JournalNode 数据存储目录--&gt;  &lt;property&gt;    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;    &lt;value&gt;$&#123;hadoop.tmp.dir&#125;/jn&lt;/value&gt;  &lt;/property&gt; &lt;!-- 完全分布式集群名称--&gt;  &lt;property&gt;    &lt;name&gt;dfs.nameservices&lt;/name&gt;    &lt;value&gt;mycluster&lt;/value&gt;  &lt;/property&gt; &lt;!-- 集群中 NameNode 节点--&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;    &lt;value&gt;nn1,nn2&lt;/value&gt;  &lt;/property&gt; &lt;!-- NameNode 的RPC通信地址--&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;    &lt;value&gt;hadoop1:8020&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;    &lt;value&gt;hadoop2:8020&lt;/value&gt;  &lt;/property&gt; &lt;!-- NameNode 的HTTP通信地址--&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;    &lt;value&gt;hadoop1:9870&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;    &lt;value&gt;hadoop2:9870&lt;/value&gt;  &lt;/property&gt; &lt;!-- 指定 NameNode 元数据在JournalNode上存放的位置--&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;    &lt;value&gt;qjournal://hadoop1:8485;hadoop2:8485/mycluster&lt;/value&gt;  &lt;/property&gt; &lt;!-- 访问代理类：client用于确定哪个NameNode为Active--&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;  &lt;/property&gt; &lt;!-- 配置隔离机制,同一时刻只能有一台服务器对外响应--&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;    &lt;value&gt;sshfence&lt;/value&gt;  &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh秘钥登录--&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;    &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;  &lt;/property&gt; &lt;!-- 启用NameNode故障自动转移--&gt;  &lt;property&gt;    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;\n\nmapred-site.xml\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;  &lt;/property&gt; &lt;!-- 历史服务器端地址 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;    &lt;value&gt;hadoop1:10020&lt;/value&gt;  &lt;/property&gt; &lt;!-- 历史服务器 web 端地址 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;    &lt;value&gt;hadoop1:19888&lt;/value&gt;  &lt;/property&gt; &lt;!-- 开启 uber 模式，默认关闭 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt; &lt;!-- uber 模式中最大的 mapTask 数量，可向下修改 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;    &lt;value&gt;9&lt;/value&gt;  &lt;/property&gt; &lt;!-- uber 模式中最大的 reduce 数量，可向下修改 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt; &lt;!-- uber 模式中最大的输入数据量，默认使用 dfs.blocksize 的值，可向下修改 --&gt;  &lt;property&gt;    &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;    &lt;value&gt;&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;\n\nyarn-site.xml\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt;    &lt;!-- Site specific YARN configuration properties --&gt;     &lt;!-- 指定 MR 走 shuffle --&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;      &lt;/property&gt;     &lt;!--启用 resourcemanager ha--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;         &lt;!--声明 resourcemanager 的地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;        &lt;value&gt;cluster-yarn&lt;/value&gt;      &lt;/property&gt;     &lt;!----&gt;          &lt;property&gt;        &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt;      &lt;/property&gt;         &lt;!--故障转移的延迟时间--&gt;          &lt;property&gt;        &lt;name&gt;yarn.client.failover-sleep-base-ms&lt;/name&gt;        &lt;value&gt;1000&lt;/value&gt;      &lt;/property&gt;     &lt;!--故障转移延迟时间--&gt;      &lt;property&gt;        &lt;name&gt;yarn.client.failover-sleep-max-ms&lt;/name&gt;        &lt;value&gt;3000&lt;/value&gt;      &lt;/property&gt;    &lt;!--指定 resourcemanager的逻辑列表--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;        &lt;value&gt;rm1,rm2&lt;/value&gt;      &lt;/property&gt;        &lt;!-- ===============rm1的配置================= --&gt;    &lt;!-- 指定rm1的主机名--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;        &lt;value&gt;hadoop1&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定rm1的web端地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;        &lt;value&gt;hadoop1:8088&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定rm1的内部通信地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;        &lt;value&gt;hadoop1:8032&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定AM向rm1申请资源地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt;        &lt;value&gt;hadoop1:8030&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定供NameNode连接的地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt;        &lt;value&gt;hadoop1:8031&lt;/value&gt;      &lt;/property&gt;    &lt;!-- ===============rm2配置================= --&gt;    &lt;!-- 指定rm2的主机名--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;        &lt;value&gt;hadoop2&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定rm2的web端地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;        &lt;value&gt;hadoop2:8088&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定rm2的内部通信地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;        &lt;value&gt;hadoop2:8032&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定AM向rm2申请资源地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt;        &lt;value&gt;hadoop2:8030&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 指定供NameNode连接的地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt;        &lt;value&gt;hadoop2:8031&lt;/value&gt;      &lt;/property&gt;     &lt;!--指定 zookeeper 集群的地址--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;192.168.11.81:2181,192.168.11.82:2181,192.168.11.83:2181&lt;/value&gt;      &lt;/property&gt;         &lt;!--启用自动恢复--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;         &lt;!--指定 resourcemanager 的状态信息存储在 zookeeper 集群--&gt;      &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;      &lt;/property&gt;             &lt;!-- 环境变量的继承 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;      &lt;/property&gt;     &lt;!-- 开启日志聚集功能 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;      &lt;/property&gt;     &lt;!-- 设置日志聚集服务器地址 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.log.server.url&lt;/name&gt;        &lt;value&gt;http://hadoop1:19888/jobhistory/logs&lt;/value&gt;      &lt;/property&gt;     &lt;!-- 设置日志保留时间为 7 天 --&gt;      &lt;property&gt;        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;        &lt;value&gt;604800&lt;/value&gt;      &lt;/property&gt;     &lt;!-- ===============资源调度配置================= --&gt;     &lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true --&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;      &lt;/property&gt;     &lt;!-- application master重启时，尝试的最大次数。--&gt;      &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;          &lt;value&gt;4&lt;/value&gt;          &lt;description&gt;The maximum number of application master execution attempts. &lt;/description&gt;      &lt;/property&gt;     &lt;!--  NM管理的CPU大小--&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;        &lt;value&gt;40&lt;/value&gt;      &lt;/property&gt;       &lt;!-- NM管理的内存大小--&gt;      &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;131072&lt;/value&gt;      &lt;/property&gt;       &lt;!-- 应用程序申请的Container中最小的vcore数量--&gt;      &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;      &lt;/property&gt;     &lt;!-- 应用程序申请的Container中最大的vcore数量 --&gt;        &lt;property&gt;        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;        &lt;value&gt;4&lt;/value&gt;      &lt;/property&gt;     &lt;!-- 应用程序申请的Container中最小的内存数量--&gt;      &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;1024&lt;/value&gt;      &lt;/property&gt;    &lt;!-- 应用程序申请的Container中最大的内存数量--&gt;      &lt;property&gt;        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;        &lt;value&gt;16384&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;\n\n\n\n六.集群调优\n\n\nThe End\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"Hadoop集群相关命令","url":"/20241121/Hadoop/de5a3e920104/","content":"Hadoop集群相关命令操作集群命令\n查看dfs空间sudo -u hdfs hadoop fs -ls /hadoop fs -ls -R /aa\n\n创建文件夹hadoop fs -mkdir /ahadoop fs -mkdir -p /aa/bb/cc\n\n查看hsdf系统根目录下的所有文件包括子文件夹里面的文件hadoop fs -ls -R /aa\n\n上传文件hadoop fs -put words.txt /aahadoop fs -copyFromLocal words.txt /aa/bb\n\n下载文件hadoop fs -get /aa/words.txt ~/newwords.txthadoop fs -copyToLocal /aa/words.txt ~/newwords1.txt\n\n合并下载hadoop fs -getmerge /aa/words.txt /aa/bb/words.txt ~/2words.txt\n\n复制\n从HDFS一个路径拷贝到HDFS另一个路径\n\nhadoop fs -cp /aa/words.txt /a\n\n移动\n在HDFS目录中移动文件\n\nhadoop fs -mv /a/words.txt /aa/bb/cc\n\n删除\n删除文件或文件夹\n\nhadoop fs -rm /aa/bb/cc/words.txt\n\n\n删除空目录 \n\nhadoop fs -rmdir /aa/bb/cc/\n\n\n强制删除 \n\nhadoop fs -rm -r /aa/bb/\n\n从本地剪切文件到HDFS上hadoop fs -moveFromLocal ~/hello.txt /aa\n\n追加文件\n追加之前hello.txt到words.txt之前 \n\nhadoop fs -appendToFile ~/hello.txt /aa/words.txt\n\n查看文件内容hadoop fs -cat /aa/hello.txt\n\n查看集群的工作状态hadoop dfsadmin -report\n\nnamenode格式化(初始化)hadoop namenode -format\n\n同步namenode数据hdfs namenode -bootstrapStandby\n\n初始化zkfchdfs zkfc -formatZK\n\n启动初始化的namenodehdfs --daemon start namenode\n\n启动journalnodehdfs --daemon start journalnode\n\n停止namenode服务hdfs --daemon stop namenode\n\n停止journalnode服务hdfs --daemon stop journalnode\n\n\n\nThe End\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"Haproxy 端口转发及负载均衡配置","url":"/20241121/Haproxy/e9f950457f1d/","content":"Haproxy 端口转发及负载均衡配置前面已经详细介绍了Haproxy基础知识 , 今天这里再赘述下Haproxy的重定向跳转的设置.  haproxy利用acl来实现haproxy动静分离，然而在许多运维应用环境中，可能需要将访问的站点请求跳转到指定的站点上，比如客户单端访问kevin.a.com需要将请求转发到bobo.b.com或将http请求重定向到https请求，再比如当客户端访问出错时，需要将错误code代码提示请求到指定的错误页面，诸如此类需求实现，这种情况下就需要利用haproxy的重定向功能来达到此目的。Haproxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，支持虚拟主机，它是免费、快速并且可靠的一种解决方案。 Haproxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。Haproxy运行在时下的硬件上，完全可以支持数以万计的 并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。Haproxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。多进程或多线程模型受内存限制 、系统调度器限制以及无处不在的锁限制，很少能处理数千并发连接。事件驱动模型因为在有更好的资源和时间管理的用户端(User-Space) 实现所有这些任务，所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。这就是为什么他们必须进行优化以 使每个CPU时间片(Cycle)做更多的工作。Haproxy支持连接拒绝 : 因为维护一个连接的打开的开销是很低的，有时我们很需要限制攻击蠕虫（attack bots），也就是说限制它们的连接打开从而限制它们的危害。 这个已经为一个陷于小型DDoS攻击的网站开发了而且已经拯救了很多站点，这个优点也是其它负载均衡器没有的。\n\n一. Haproxy实现request请求重定向关于Hproxy 请求重定向主要会用到: redirect  和 redir 这两类重定向配置语法。\n1) redirect重定向用法: (redirect通常配置在haproxy acl部分)redirect一般有两个指令来执行HTTP重定向：http-requets redirect       #此种方式支持日志变量格式redirect                           #此种方式只依赖于静态字符串这两个指令的语法是相同的，即redirect现在被认为是传统和配置应该移动到http-request redirect形式。还有一个主要区别是：http-request redirect使用日志可变格式, 而redirect语句只依赖于静态字符串。\n\n2) redirect三种重定向方式a) 位置重定向使用语法如下:redirect location &lt;loc&gt; [code &lt;code&gt;] &lt;option&gt; [&#123;if | unless&#125; &lt;condition&gt;]使用位置重定向，例如下面所示指令可以将用户重定向到所提供的精确位置， 该位置可以是第三方URL链接，也可以是本地web服务的另一个访问路径.1.http-request redirect location &lt;loc&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]2.redirect location &lt;loc&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]相关指令参数如下：* &lt;loc&gt; ：一个日志格式变量 （或简单的字符串redirect语句）描述了新位置；* code &lt;code&gt;（可选）：HTTP重定向的状态代码来执行。 此选项下的允许的状态码如下所示：* &lt;option&gt;（可选):  可以是以下任何或组合的声明：1. set-cookie NAME[=value] :一个Set-Cookie头部被添加到重定向。该cookie被命名为名称，可以有一个可选的值值。2. clear-cookie NAME[=]一个特殊的Set-Cookie头被添加到重定向。该Cookie名为名称和最大年龄的cookie参数设置为0，目的是为了指示浏览器删除cookie。 注意:  在于浏览器中，这是两个不同的Cookie：NAME和NAME = 以上根据您的流量模式，必须将两个语句适应。* if | unless :  用于条件判断*&lt;condition&gt; （可选）：用于匹配acl，一般为acl的名称b) 前缀重定向使用语法如下:redirect prefix &lt;loc&gt; [code &lt;code&gt;] &lt;option&gt; [&#123;if | unless&#125; &lt;condition&gt;]使用前缀重定向，将用户重定向到由concateneting建立了一个网址&lt;pfx&gt;和完整的原始URI路径：1. http-request redirect prefix &lt;pfx&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]2. redirect prefix &lt;pfx&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]相关指令参数如下：* &lt;pfx&gt;: 一个日志格式变量 （或简单的字符串redirect语句）描述了新的位置前缀。* code &lt;code&gt;（可选）：HTTP重定向的状态代码来执行。 此选项下的允许的状态码如下所示：* &lt;option&gt;（可选): 可以是以下任何或组合的声明：drop-query ：在执行串联时从原来的URL删除查询字符串append-slash ：配合使用drop-query ，在该URL的末尾添加一个“/”字符set-cookie NAME[=value] ：一个Set-Cookie头部被添加到重定向。该cookie被命名为名称，可以有一个可选的值值。clear-cookie NAME[=] ：一个特殊的Set-Cookie头被添加到重定向。该Cookie名为名称和最大年龄的cookie参数设置为0，目的是为了指示浏览器删除cookie。* if | unless :用于条件判断* &lt;condition&gt; （可选）：用于匹配acl，一般为acl的名称 c) 协议（计划)重定向（比如将http重定向到https）使用语法如下:redirect scheme &lt;sch&gt; [code &lt;code&gt;] &lt;option&gt; [&#123;if | unless&#125; &lt;condition&gt;]使用位置重定向，例如下面所示指令可以将用户重定向到所提供的新的http协议url链接， 一般用于非安全链接跳转到安全链接，比如http跳转到https上1. http-request redirect scheme &lt;schloc&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]2. redirect scheme &lt;sch&gt; [code &lt;code&gt;] [&lt;option&gt;] [&lt;condition&gt;]相关指令参数如下：* &lt;loc&gt; ：一个日志格式变量 （或简单的字符串redirect语句）描述了新位置；* code &lt;code&gt;（可选）：HTTP重定向的状态代码来执行。 此选项下的允许的状态码如下所示：* &lt;option&gt;（可选): 可以是以下任何或组合的声明：1. set-cookie NAME[=value] :一个Set-Cookie头部被添加到重定向。该cookie被命名为名称，可以有一个可选的值值。2. clear-cookie NAME[=]一个特殊的Set-Cookie头被添加到重定向。该Cookie名为名称和最大年龄的cookie参数设置为0，目的是为了指示浏览器删除cookie。注意: 在于浏览器中，这是两个不同的Cookie：NAME和NAME = 以上根据你的流量模式，必须将两个语句适应。* if | unless :用于条件判断* &lt;condition&gt; （可选）：用于匹配acl，一般为acl的名称一个简单的实例:acl http      ssl_fc,nothttp-request redirect scheme https if http下面是redirect 综合应用的一个小示例:acl clear      dst_port  80acl secure     dst_port  8080acl login_page url_beg   /loginacl logout     url_beg   /logoutacl uid_given  url_reg   /login?userid=[^&amp;]+acl cookie_set hdr_sub(cookie) SEEN=1 redirect prefix   https://kevin.com set-cookie SEEN=1 if !cookie_setredirect prefix   https://kevin.com           if login_page !secureredirect prefix   http://kevin.com drop-query if login_page !uid_givenredirect location http://kevin.com/           if !login_page secureredirect location / clear-cookie USERID=      if logout总结: redirect三种重定向可以混合使用，比较常用的有redirect prefix 和 redirect location这两种方式，从某种理解上可以交差使用；\n\n3) redir重定向用法:(redir通常配置在haproxy backend部分)使用redir 会将发往backend的站点服务请求均以302状态响应发给需要重定向的server服务或站点，此时haproxy不需要向后端web server提交请求；需要注意的是，在prefix后面不能使用/，且不能使用相对地址，以避免造成循环，例如:frontend  main *:80    default_backend appbackend app    balance roundrobin    server node1 127.0.0.1:81 check weight 3 redir http://www.kevin.com上面配置含义:所有发往localhost:81的请求做重定向，重定向到www.kevin.com因此可以实现单台服务器的重定向又例如，如果我们要讲访问的站点重定向到grace.comfrontend  main *:80    default_backend  appbackend app    balance roundrobin    server node1 127.0.0.1:81 check weight 3 redir http://www.grace.com注意:redir只做跳转，如客户端输入:http://ip ,将会跳转到指定的页面上，此时客户端的页面的页面也会跳转到指定的页面上，之后所有的请求都会递交到该站点（前提该站点可以与客户端通讯），而不再发往haproxy代理站点，haproxy也不需要往后端web server提交客户端发过来的请求。\n\n二. haproxy实现error重定向格式为: errorfile 错误代码code 错误代码响应提示页路径* errorfile 即根据客户端页面错误code状态将指定的错误状态页面提示给客户端，比如友情提示页面，一般如下:errorfile 403 /etc/haproxy/errorfiles/403.httperrorfile 500 /etc/haproxy/errorfiles/500.httperrorfile 502 /etc/haproxy/errorfiles/502.httperrorfile 503 /etc/haproxy/errorfiles/503.httperrorfile 504 /etc/haproxy/errorfiles/504.http例如:如果想访问403页面重定向到其他页面的话 (errorloc)，则参考以下配置:frontend web_server    bind *:80    default_backend webserver    acl badguy src 172.16.50.10    block if badguy    errorloc 403 http://grace.com/         #定义错误页面重定向errorfile  表示在用户请求不存在的页面时，返回一个页面给客户端而非有haproxy生成的错误代码，可用于所有段中;格式: errorfile &lt;code&gt; &lt;file&gt;errorloc  表示请求错误时，返回一个HTTP重定向至某URL的信息，可以用于所有端中;格式: errorloc &lt;code&gt; &lt;url&gt;总结: 错误重定向可以更加友好地提示客户端错误状态，比如做定制页面化跳转，以及网站维护升级等等，当出现错误时，可以及时跳转到预定好错误提示页面上。 \n\n三. haproxy定义规则1) haproxy常用的acl匹配条件-i：不区分&lt;value&gt;中模式字符的大小写； -f：从指定的文件中加载模式；    path_beg：用于测试请求的URL是否以&lt;string&gt;指定的模式开头    例：匹配url以/static、/images、/javascript /stylesheets开头    acl url_static  path_beg  -i  /static /images /javascript /stylesheets path_end：用于测试请求的URL是否以&lt;string&gt;指定的模式结尾    例：匹配url以jpg、gif、png、css、js结尾    acl url_static  path_end -i .jpg .gif .png .css .js hdr_beg：用于测试请求报文的指定首部的开头部分是否符合&lt;string&gt;指定的模式    例：匹配请求的主机以img、video、download或ftp开头    acl host_static hdr_beg(host) -i img. video. download. ftp.    即匹配访问的域名是img.bobo.com,video.bobo.com,download.bobo.com,ftp.bobo.com url_beg：匹配的是整个url    例：匹配url为http://www.bobo.com.com    acl is_bobo.com url_beg http://www.bobo.com.com    use_backend bobo.com if is_bobo.com dst_port：判断请求的端口 hdr_sub：判断客户的user-agent    例：判断客户端的user-agent是否为手机    acl shouji hdr_sub(user-agent) -i android iphone\n\n2) haproxy定义分发规则根据请求的主机头，实现不同项目的请求，分发到不同的backend hdr_beg(host):判断主机头 例如:acl is_www hdr_beg(host) -i www.bobo.com.comacl is_wap hdr_beg(host) -i wap.bobo.com.comacl is_erp hdr_beg(host) -i erp.bobo.com.comacl is_interface hdr_beg(host) -i interface.bobo.com.comuse_backend tomcat_erp_v2 if is_erpuse_backend tomcat_interface_v2 if is_interfaceuse_backend tomcat_web_v2 if is_wwwuse_backend tomcat_mobile_v2 if is_wap 通过定义以上规则即可实现访问不同的域名分发到不同的backend\n\n3) haproxy定义重定向规则prefix:表示重定向urllocation:表示重定向访问路径，即url不变，url后边跟的路径发生改变 例如:redirect prefix http://weihu.bobo.com.com/PC if is_wwwredirect prefix http://weihu.bobo.com.com/H5 if is_wap 说明：当访问is_www时，重定向到weihu.bobo.com.com/PC当访问is_wap时，重定向到weihu.bobo.com.com/H5\n\n4) haproxy定义放行规则仅放行通过验证的IP地址或者IP范围段； 例如:如果访问的是is_www，但来源IP不是指定的IP时，用http-request deny进行拒绝； acl is_www hdr_beg(host) -i www.bobo.com.comacl is_dns src 172.16.60.0/24 218.65.212.0/24http-request deny if is_www  !is_dns（满足条件的直接进行拒绝）   也可以写为：acl is_www hdr_beg(host) -i www.bobo.com.comacl is_dns src 172.16.60.0/24 218.65.212.0/24user_backend www if is_www  is_dns（两个条件同时满足才使用后端的www） 说明：源地址有多个时，用空格进行隔开\n\n5) haproxy定义手机只能访问手机端，电脑端只能访问电脑端规则例如:当手机访问www.bobo.com时转发到wap.bobo.com当电脑访问wap.bobo.com时转发到www.bobo.com 配置如下:acl is_shouji hdr_sub(user-agent) -i android iphoneacl is_diannao hdr_beg(host) wwwredirect prefix http://wap.bobo.com if shoujiredirect prefix http://www.bobo.com if is_diannao !is_shoujiHaproxy 重定向跳转 - 示例 1(1) 首先每个域名解析到自己的ipwww.kevin.com   172.16.51.100www.grace.com   172.16.51.200www.bobo.com    172.16.51.210 (2) 域名重定向acl name_redirectA hdr_beg(host) -i www.kevin.comredirect prefix http://www.bobo.com/A if name_redirectA acl name_redirectB hdr_beg(host) -i  www.grace.comredirect prefix http://www.bobo.com/B if name_redirectB (3) 跳转规则acl name_A hdr_beg(host) -i www.kevin.comacl name_B hdr_beg(host) -i www.grace.comacl name_C hdr_beg(host) -i www.bobo.comacl api_reqA path_beg -i /A/apiacl api_reqB path_beg -i /B/apiuse_backend appserver_8081 if name_A or name_B      #匹配&quot;或&quot;的规则use_backend appserver_8082_A if name_C api_reqA     #匹配&quot;和&quot;的规则use_backend appserver_8082_B if name_C api_reqB       backend appserver_8081   balance source   server web1 172.16.51.171:8081 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3   server web2 172.16.51.174:8081 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 backend appserver_8082_A   server web1 172.16.51.180:80 weight 3 check inter 2000 rise 2 fall 3  backend appserver_8082_B   server web1 172.16.51.180:80 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3Haproxy 重定向跳转 - 示例 2redirect location &lt;to&gt; [code &lt;code&gt;] &lt;option&gt; [&#123;if | unless&#125; &lt;condition&gt;]redirect prefix &lt;to&gt; [code &lt;code&gt;] &lt;option&gt; [&#123;if | unless&#125; &lt;condition&gt;] 重定向，相当于rewrite  示例配置如下:acl shibo hdr_reg(host) -i ^(shibo.kevin.com|forum.kevin.com)       #使用正则匹配acl shibo_path path_beg -i /shibo                #url 目录acl youxi path_beg -i /youxi              acl static path_end -i .html .css .js      #url 结尾文件acl php path_end -i .php acl jsp path_end -i .jsp .do   use_backend shibo_pool if shibo or shibo_path       #注意&quot;或&quot;的匹配用&quot;or&quot;use_backend youxi_pool if youxiuse_backend static_pool if static use_backend php_pool if phpuse_backend jsp_pool if jspdefault_backend www.kevin.com                 #当满足host_bb.cn的策略,跳转(重定向)到http://www.bb.cnacl host_bb.cn hdr_beg(host) -i (bb.cn|beijing.com)redirect prefix http://www.bb.cn if host_bb.cn  #有个问题: haproxy能否在接到一个请求时选择一个后端服务器，然后301重定向url 。#主要原因是他有5个1G的出口，这样就能充分利用其带宽。#测试了一下是可以的frontend free   bind *:80   default_backend lvs2backend lvs2   mode http   option forwardfor header ORIG_CLIENT_IP   server free174 172.16.51.16:8081 redir http://free71-174-grace.com:8081 weight 10 rise 3 fall 5 check inter 2000   server free173 172.16.51.15:8081 redir http://free71-173-grace.com:8081 weight 10 rise 3 fall 5 check inter 2000       #当输入负载均衡机器的域名后，url会直接变成http://free71-17(3|4)-grace.com:8081.  acl monitor hdr_beg(host) -i monitor.kevin.com        #定义ACL名称(monitor),对应的请求的主机头是monitor.kevin.com acl shibo hdr_reg(host) -i ^(shibo.kevin.com|forum.kevin.com)  #使用正则匹配acl host_bb.cn hdr_beg(host) -i bb.cnacl host_hui.cn hdr_beg(host) -i beijing.comredirect prefix http://www.bb.cn if host_bb.cnredirect prefix http://www.bb.cn if host_hui.cn  frontend localhost    bind *:80    bind *:443 ssl crt /etc/ssl/bb.web/bb.web.pem    redirect scheme https if !&#123; ssl_fc &#125;    mode http    default_backend nodes # 上面配置中, 添加了redirect导向，如果连接不是通过SSL连接的，它将http重定向到https  acl host_bb.cn hdr_beg(host) -i bb.cnacl host_jiu.cn hdr_beg(host) -i beijing.comacl www_jiu.cn hdr_beg(host) -i www.beijing.comacl host_hui.cn hdr_beg(host) -i pp.comacl www_hui.cn hdr_beg(host) -i www.pp.comredirect prefix http://www.bb.cn if host_bb.cnredirect prefix http://www.bb.cn if host_jiu.cnredirect prefix http://www.bb.cn if www_jiu.cnredirect prefix http://www.bb.cn if host_hui.cnredirect prefix http://www.bb.cn if www_hui.cn  访问bb.cn,beijing.com,www.beijing.com,pp.com,www.pp.cpm 都跳转到http://www.bb.cnHaproxy 重定向跳转 - 示例 3 (手机规则匹配)\n\n一.  线上业务的实际需求现在根据业务的实际需要，有以下几种不同的需求。如下： a) 转发所有手机请求所有通过手机端访问http.shibo.com域名的话，全部转发到http://www.shibo.com这个地址，而PC端不受此限制。 b) 根据url进行转发如果手机端请求http.shibo.com这个域名的url中，以docs或者manager这两个关键词开头的话，把该请求转发到后端的服务器，而PC端不受此限制。 也就是说手机端访问具体的url地址的话，可以正常访问。如果是直接访问http.shibo.com域名的话，直接把该请求转发到http://www.shibo.com这个地址。 ============================================================\n\n二.  haproxy配置下面根据不同的业务需求进行配置haproxy，如下。a) 转发所有手机请求配置要把所有的手机端请求转到www.shibo.com这个地址，需要我们首先把访问的终端匹配出来，haproxy可以通过hdr_sub(user-agent)这个参数把手机端匹配出来。 手机端匹配出来后，我们就可以定义相应的规则，把手机端的请求转发到www.shibo.com这个地址了。haproxy.cf配置文件如下：[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 188   gid 188   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   maxconn 2000   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_http hdr_beg(host) http.shibo.com   acl ua hdr_sub(user-agent) -i android iphone   redirect prefix http://www.shibo.com if ua   use_backend httpserver if is_http backend httpserver    balance source    server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3温馨提示:在以上配置文件中，有以下两行需要注意：acl ua hdr_sub(user-agent) -i android iphoneredirect prefix http://www.shibo.com if ua 这两行:第一行是第一个ua规则，该规则是判断是否是手机端。注意：在此手机端，我们只匹配了安卓手机和iphone。第二行是跳转规则，如果匹配是手机端的话，那么直接跳转到http://www.shibo.com这个地址。  如果是PC端的话，默认跳转到httpserver这个后端服务器组。以上配置是一台服务器对外只提供一个域名访问的请求，如果有两个域名的话，就要进行如下配置：[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 188   gid 188   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   maxconn 2000   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_http hdr_beg(host) http.shibo.com   acl is_haproxy hdr_beg(host) haproxy.shibo.com   acl ua hdr_sub(user-agent) -i android iphone   redirect prefix http://www.shibo.com if ua !is_haproxy   use_backend haproxyserver if ua is_haproxy   use_backend haproxyserver if is_haproxy   use_backend httpserver if is_http backend httpserver   balance source   server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 backend haproxyserver   balance source   server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3\n\nb) 测试转发所有手机请求在手机浏览器中输入http.shibo.com会自动跳转到http://www.shibo.com这个地址。\n\nc) 根据url进行转发配置根据手机端请求的url进行转发的话，首先也是需要匹配出手机端，然后定义url路径规则。最后结合手机端和url路径规则，进行跳转。haproxy具体配置文件，如下：[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 188   gid 188   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   maxconn 2000   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_http hdr_beg(host) http.shibo.com   acl is_docs url_beg /docs /manager   acl ua hdr_sub(user-agent) -i android iphone   redirect prefix http://www.shibo.com if ua !is_docs   use_backend httpserver if ua is_docs   use_backend httpserver if is_http backend httpserver   balance source   server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 温馨提示:在上述配置文件中，需要以下几行解释下: acl is_docs url_beg /docs /manager定义一个is_docs规则。如果url以/docs或者/manager开头的，则全部属于该规则。 acl ua hdr_sub(user-agent) -i android iphoneredirect prefix http://www.shibo.com if ua !is_docs这两行首先是匹配出手机端，然后如果是手机端访问，并且访问的不是is_docs规则的话，则直接跳转到http://www.shibo.com这个地址。 use_backend httpserver if ua is_docs这条命令是，如果是手机端访问，并且访问的是is_docs规则的话，则直接跳转到httpserver这个后端服务器组。 如果是PC端的话，默认跳转到httpserver这个后端服务器组。\n\nd) 测试根据url进行转发手机端访问http://http.shibo.com/docs/这个连接的话，是可以直接访问的。\n\n三.  其他haproxy相关配置上面说明了有关手机的相关配置，在实际的生产环境中，有时候还会碰到一些奇奇怪怪的要求。比如要求所有手机端访问的http.shibo.com，转到指定的页面。haproxy主要配置文件如下：[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg.......................... frontend weblb   bind *:80   acl is_http hdr_beg(host) http.shibo.com   acl ua hdr_sub(user-agent) -i android iphone   redirect prefix http://www.shibo.com/?p=10624 if ua   use_backend httpserver if is_http backend httpserver   balance source   server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 以上配置是所有手机端访问的，都跳转到http://www.shibo.com/?p=10624这个页面。haproxy重定向跳转 - 示例4 (修改路径, 即重写URL)\n\n1) 访问https://www.shibo.com/guo-hui 重定向跳转到 https://www.xiaobo.com/an-huifrontend https    option http-server-close    reqadd X-Forwarded-Proto:\\ https     acl shi_bo hdr(host) -i www.shibo.com    acl guo_hui path_beg /guo-hui    acl an_hui path_beg /an-hui     reqirep ^([^\\ ]*\\ /)guo-hui(.*) \\1an-hui\\2 if shi_bo guo_hui              #注意中间的空格, 以及\\1 和 \\2;    redirect prefix https://www.xiaobo.com code 301 if shi_bo an_hui\n\n2) 访问http://front-end/app-2/do-that 重定向跳转到 http://back-end/app-2-another-path/do-thatfrontend http     acl do-that path_end -i /app-2/do-that   use_backend server1 if do-that backend server1   reqirep ^([^\\ :]*)\\ /app-2/(.*)  \\1\\ /app-2-another-path/\\2   server server 172.16.60.51\n\n3) 访问原请求为 http://www.kevin.com/OLD/ab...    重定向到 http://www.kevin.com/NEW/ab... 在nginx里可以通过rewrite来实现跳转配置, 配置内容如下:server &#123;    listen 80;    server_name www.kevin.com;    location / &#123;        rewrite ^/OLD(.*) /NEW$1 permanent;        proxy_pass http://backend_www_kevin_com;    &#125;&#125; 在haproxy里重定向跳转的配置如下:frontend web80    bind *:80         acl domain_www_kevin_com hdr_beg(host) -i www.kevin.com kevin.com    acl url_old  url_beg   -i /old     reqirep ^([^\\ ]*)\\ /old(.*) \\1\\ /new\\2  if domain_www_kevin_com url_old    use_backend kevin_com   if domain_www_kevin_com    最后总结:通过以上haproxy和nginx的重写配置, 可以看出二者配置的不同在于:-  http://www.kevin.com/old，后端真实服务器收到的请求被重写为 /new/ ，并且浏览器收到 HTTP/1.1 302 Location: /new/ ,   地址栏改为 http://www.kevin.com/new/-  http://www.kevin.com/old/，或者 http://www.kevin.com/old/ab... 后端真实服务器收到的请求被重写为 /new/ ，   浏览器并没有收到 302 , 地址栏依旧为 http://www.kevin.com/old/\n\n4) haproxy URL 重新 , 如下两个规范配置reqirep ^([^\\ ]*)\\ /books/(.*) \\1\\ /books.php?title=\\2reqirep ^([^\\ ]*)\\ /(.*)  \\1\\ /wdn/\\2 # 注意中间的空格, 以及\\1 和 \\2\n\n5) reqirep可以修改http的头; 如果haproxy 配置里要替换主机头, 则:   在backend 选项下面加入：   reqirep ^Host:\\ (.*) Host:\\ 标识haproxy重定向跳转 - 示例5 (错误/黑白名单/动静分离/读写(上传和下载)分离的重定向 )(1) haproxy 错误重定向(403), 即黑名单设置    [root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................   acl blacklist src 172.16.51.250http-request deny if blacklist   如上配置后, 当来源ip是172.16.51.250时, 就直接返回一个403错误页面!!   =============================================================当haproxy配置里限制一个来源ip访问时, 直接给用户返回403错误页面, 会显得不太友好, 所以haproxy重定向应运而生.如下配置, 当来源ip是172.16.51.250时, 直接重定向跳转到172.16.51.10的8000端口的错误页面    [root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................    acl badhost src 172.16.51.250block if badhosterrorloc 403 http://172.16.51.10:8000    在172.16.51.10机器上部署nginx, 端口为8000, 然后在nginx根目录的index.html里设置错误页面信息,错误页面信息可以自己随便定义, 比如&quot;抱歉, 页面临时出错, 运维工程师正在抢修中, 请耐心等待~&quot;    (2) haproxy 黑名单重定向[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................    acl  badhost src 172.16.51.250redirect location http://172.16.51.10:8000 if  badhost    在172.16.51.10机器上部署nginx, 端口为8000, 然后在nginx根目录的index.html里设置错误页面信息,错误页面信息可以自己随便定义, 比如&quot;抱歉, 这是一个禁止访问的来源ip地址!请尝试从其他机器访问.&quot;    (3) haproxy 网页重定向[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................    acl shibo.com hdr_beg(host) -i shibo.comredirect code 301 location http://www.shibo.com if shibo.com    温馨提示:如果不写301,只写code默认是302,临时重定向 (不推荐), 加上301则表示永久重定向    (4) haproxy 访问IP自动跳转到域名[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................    acl 172.16.51.10 hdr(host) -i 172.16.51.10redirect code 301 location http://www.shibo.com if 172.16.51.10    (5) haproxy 读写分离 配置 (即上传和下载分离)[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................    acl read method GETacl read method HEADacl write method PUTacl write method POSTuse_backend dynamic if writedefault_backend static    backend static                               #&quot;上传&quot;读取操作的负载代理.    balance   roundrobin    server    web1 172.16.51.30:80 check              backend  dynamic                         #&quot;下载&quot;写入操作的负载代理    balance   roundrobin    server    web2 172.16.51.40:80 check             (6) haproxy 动静分离 配置[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................   frontend public      bind            *:80 name clear      #bind         172.16.51.10:443 ssl crt /etc/haproxy/haproxy.pem      #use_backend    static if &#123; hdr_beg(host) -i img &#125;      #use_backend    static if &#123; path_beg /img /css   &#125;      use_backend    static2 if &#123; path_end -i .php   &#125;      default_backend static1   # The static backend backend for &#x27;Host: img&#x27;, /img and /css.backend static1      balance     roundrobin      server        statsrv1 172.16.51.20:80 check inter 1000   backend static2      balance     roundrobin      server       statsrv2 172.16.51.30:80 check inter 1000 (7) haproxy的白名单设置[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................frontend tcp-8080-front     bind *:8080     mode tcp     default_backend     tcp-8080-back  tcp-8080-back     mode tcp     balance leastconn     tcp-request content accept if &#123; src -f /usr/local/haproxy/white_ip_list &#125;     tcp-request content reject     server tcp-8080 10.1.27.20:8080 配置中/usr/local/haproxy/white_ip_list文件即为白名单文件, 在文件里配置允许的白名单地址: 一行一个IP或者IP段。haproxy 重定向跳转 - 示例6[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................acl  admin_req  path_beg  -i  /adminuse_backend  admin_80  if  admin_req backend admin_80      mode   http      balance   roundrobin      server   apphost01_8083  172.16.50.133:80  check inter 2000 fall 3  接着去172.16.50.133服务器上看下tomcat配置[root@tomcat-133 ~]# vim /usr/local/tomcat/conf/server.xml.............     &lt;Context docBase=&quot;/data02/kevin-web&quot; path=&quot;/&quot; reloadable=&quot;false&quot;/&gt; [root@tomcat-133 ~]# cd /data02/kevin-web[root@tomcat-133 kevin-web]# lsadmin  adminwechat  index.html  jquery.2.1.4.min.js  META-INF  WEB-INF  kevin-web-0.0.1-SNAPSHOT.war[root@tomcat-133 kevin-web]# lsbobo.html 最后, 可验证:访问http://www.kevin.com/admin/bobo.html  实际上返回的是172.16.50.133服务器的/data02/kevin-web/admin/bobo.html页面内容haproxy 重定向跳转 - 示例7 (haproxy 代理 https)\n\nhaproxy代理https有两种方式：1）haproxy服务器本身提供ssl证书，后面的web服务器走正常的http2）haproxy服务器本身只提供代理，后面的web服务器走https(配置ssl证书)\n\n第一种方式：haproxy服务器本身提供ssl证书  注意： 需要编译haproxy的时候支持ssl编译参数：[root@localhost ~]# make TARGET=linux26 USE_OPENSSL=1 ADDLIB=-lz[root@localhost ~]# ldd haproxy | grep ssllibssl.so.10 =&gt; /usr/lib64/libssl.so.10 (0x00007fb0485e5000)  [root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................frontend https_frontend    bind *:443 ssl crt /etc/ssl/certs/servername.pem    mode http    option httpclose    option forwardfor    reqadd X-Forwarded-Proto:\\ https    default_backend web_server  backend web_server    mode http    balance roundrobin    cookie SERVERID insert indirect nocache    server s1 172.16.50.150:80 check cookie s1    server s2 172.16.50.151:80 check cookie s2    温馨提示:  这里的pem 文件是下面两个文件合并而成[root@localhost ~]# cat servername.crt servername.key |tee servername.pem    =====================================================\n\n第二种方式：haproxy服务器本身只提供代理，没有ssl证书 （一般我们常用的就是这种方式）  这种方式，haproxy不需要重新编译支持ssl，简单方便，只需要后面的web服务器配置好ssl即可。  [root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfg..........................frontend https_frontend    bind *:443    mode tcp    default_backend web_server  backend web_server    mode tcp    balance roundrobin    stick-table type ip size 200k expire 30m    stick on src    server s1 172.16.50.150:443    server s2 172.16.50.151:443  温馨提示:  这种模式下mode 必须是tcp模式!!!!!!!!haproxy重定向跳转 - 示例8 (http, https的有关重定向, 案例分析)需要提前说明下:下面配置操作全部是在haproxy1.5.4版本下进行配置和通过测试的!haproxy1.3版本以下haproxy配置参数可能不能使用，所以需要注意haproxy的版本号以下的haproxy配置可以在线上生产环境直接使用的!\n\n一、业务要求根据系统业务的实际需要，有以下几种不同的需求： 1.1) http跳转https把所有请求http://http.kevin.com的地址全部跳转为https://http.kevin.com这个地址。 1.2) http与https并存服务器同时开放http://http.kevin.com和https://http.kevin.com的访问形式。 1.3) 同台服务器不同域名之间的https与http同一台服务器对http.kevin.com域名访问的全部跳转为https://http.kevin.com，而对haproxy.kevin.com访问走http协议，也就是跳转到http://haproxy.kevin.com这个地址。 1.4) 同台服务器多域名均使用https同一台服务器对http.kevin.com和haproxy.kevin.com访问走http是协议。\n\n 二、配置haproxy, 实现以上业务需求 2.1) http跳转https的haproxy配置文件内容说实话haproxy的https配置要比nginx配置简单的多了，只需要加入几行代码即可实现https的功能。[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 188   gid 188   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   maxconn 2000   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_http hdr_beg(host) http.kevin.com   redirect scheme https if !&#123; ssl_fc &#125;   bind *:443 ssl crt /etc/haproxy/kevin.com.pem   use_backend httpserver if is_http backend httpserver   balance source   server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3温馨提示:在以上配置文件中，只要需要注意下面四个选项的配置：tune.ssl.default-dh-param 2048                #因为我们的SSL密钥使用的是2048bit加密，所以在此进行声明。 acl is_http hdr_beg(host) http.kevin.comredirect scheme https if !&#123; ssl_fc &#125;bind *:443 ssl crt /etc/haproxy/kevin.com.pem# 这三行表示把所有访问http.kevin.com这个域名的请求，全部转发到https://http.kevin.com这个连接。 测试:发现在浏览器中，无论输入的是http.kevin.com, 还是http://http.kevin.com, 亦或是https://http.kevin.com，都会自动跳转到https://http.kevin.com。这样就达到了，把所有的http请求跳转到https的目的。\n\n2.2) http与https并存配置haproxy要实现http和https并存的话，配置也很简单，只需要把haproxy分别监控不同的端口就行，配置文件如下：[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   user haproxy   group haproxy   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   retries 3   option redispatch   maxconn 2000   timeout connect 5000ms   timeout client 50000ms   timeout server 50000ms listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_http hdr_beg(host) http.kevin.com   use_backend httpserver if is_http backend httpserver   balance source   server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 frontend weblb443   bind *:443 ssl crt /etc/haproxy/kevin.com.pem   acl is_443 hdr_beg(host) http.kevin.com   use_backend httpserver443 if is_443 backend httpserver443   balance source   server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3温馨提示:在以上配置文件中，定义了两个前端，一个前端用于监听80端口，也就是http协议。另外一个前端监听443端口，也就是https协议。此时haproxy会根据客户端请求的协议进行分发，如果发现客户端请求的是http协议，则把该请求分发到监听80端口的前端。如果发现客户端请求的是https协议，则把该请求分发到监听443端口的前端。如此就达到了haproxy让http和https并存的要求。 测试:通过测试会发现，在浏览器中如果输入的是http://http.kevin.com或者是http.kevin.com都会直接跳转到http://http.kevin.com，而输入的是https://http.kevin.com，则只会跳转到https://http.kevin.com。如此就到达了，我们业务的要求实现http和https并存。\n\n2.3) 同台服务器不同域名之间的https与http配置同台服务器不同域名之间的http和https配置比较复杂，第一需要监听两个端口，第二还要根据不同的域名进行分发。[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 188   gid 188   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   maxconn 2000   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend weblb   bind *:80   acl is_haproxy hdr_beg(host) haproxy.kevin.com   acl is_http hdr_beg(host) http.kevin.com   redirect prefix https://http.kevin.com if is_http   use_backend haproxyserver if is_haproxy backend haproxyserver   balance source   server web1 127.0.0.1:9090 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 frontend weblb443   bind *:443 ssl crt /etc/haproxy/kevin.com.pem   acl is_443 hdr_beg(host) http.kevin.com   use_backend httpserver443 if is_443 backend httpserver443   balance source   server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3温馨提示:同台服务器不同域名之间的https与http配置，上面配置了两个前端一个用于监听80端口，并且根据不同的域名进行跳转。在80端口的规则中，如果客户端请求的是http.kevin.com，这个域名的话，则haproxy会把该请求直接跳转到https://http.kevin.com。如果是haproxy.kevin.com，这个域名的话，则分发到后端的服务器。另外一个前端用于监听443端口，用于分发客户端https://http.kevin.com的请求。 测试:可以发现在浏览器中输入haproxy.kevin.com会跳转到http://haproxy.kevin.com这个地址，而如果输入的是http.kevin.com或者是http://http.kevin.com，亦或是https://http.kevin.com的话，都会跳转到https://http.kevin.com。如此就达到了上面的业务要求，同台服务器上访问haproxy.kevin.com直接跳转到80端口，如果访问的是http.kevin.com域名的话则跳转到https://http.kevin.com这个地址。\n\n2.4) 同台服务器多域名均使用https配置要使同台服务器的两个设置多个域名都使用https协议的话，配置很简单。只需要在haproxy中启用各自的https配置即可。[root@localhost ~]# vim /usr/local/haproxy/conf/haproxy.cfgglobal   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   maxconn 4096   uid 108   gid 116   daemon   tune.ssl.default-dh-param 2048 defaults   log global   mode http   option httplog   option dontlognull   option http-server-close   option forwardfor except 127.0.0.1   option redispatch   retries 3   option redispatch   timeout http-request 10s   timeout queue 1m   timeout connect 10s   timeout client 1m   timeout server 1m   timeout http-keep-alive 10s   timeout check 10s   maxconn 3000 listen admin_stats   bind 0.0.0.0:1080   mode http   option httplog   maxconn 10   stats refresh 30s   stats uri /stats   stats auth admin:admin   stats hide-version frontend web80   bind *:80   acl is_http hdr_beg(host) http.kevin.com   redirect scheme https if !&#123; ssl_fc &#125;    bind *:443 ssl crt /etc/haproxy/kevin.com.pem   acl is_haproxy hdr_beg(host) haproxy.kevin.com   redirect scheme https if !&#123; ssl_fc &#125;    bind *:443 ssl crt /etc/haproxy/kevin.com.pem   use_backend httpserver if is_http   use_backend haproxyserver if is_haproxy backend httpserver   balance source   server web1 127.0.0.1:6060 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3 backend haproxyserver   balance source   server web1 127.0.0.1:9090 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3\n测试:在浏览中无论是输入http.kevin.com、http://http.kevin.com，还是haproxy.kevin.com、http://haproxy.kevin.com，都会跳转到相应的https地址。这也达到了业务的要求\n\n\n来源博客园 https://www.cnblogs.com/kevingrace/p/10182538.html\n","categories":["Haproxy"],"tags":["Haproxy"]},{"title":"KVM虚拟化搭建","url":"/20241121/KVM/b3cecd553fa2/","content":"KVM虚拟化搭建1.关闭防火墙# 停止firewallsystemctl stop firewalld# 禁止firewall开机启动systemctl disable firewalld\n\n2.关闭enforcesetenforce 0# 将/etc/selinux/config中SELINUX=enforcing改为SELINUX=disabledvi /etc/selinux/config \n\n3.查cpu是否支持VTegrep &#x27;(vmx|svm)&#x27; --color=always /proc/cpuinfo\n\n4.检查内核模块是否加载lsmod | grep kvm\n\n5.安装kvm及依赖若离线安装请先部署本地yum源,在线安装直接安装即可\nyum install -y  virt-*  libvirt  bridge-utils qemu-img qemu-kvm virt-install libguestfs-tools\n\n设置开机启动\nsystemctl enable libvirtdsystemctl start libvirtd\n\n查看状态是否正常\nsystemctl status libvirtd\n\n配置软连接\nln -s /usr/libexec/qemu-kvm /usr/bin/qemu-kvm \n\n6.为虚拟机创建网桥首先备份一下网卡设置\ncp /etc/sysconfig/network-scripts/ifcfg-ens160  /etc/sysconfig/network-scripts/ifcfg-ens160.bak\n\n配置虚拟网桥\n# 清空ifcfg-ens160内容cat &gt;/etc/sysconfig/network-scripts/ifcfg-ens160 &lt;&lt;EOFDEVICE=ens160TYPE=EthernetBOOTPROTO=noneONBOOT=yesBRIDGE=br0EOF\n新建br0\ncat &gt;/etc/sysconfig/network-scripts/ifcfg-br0 &lt;&lt;EOFTYPE=&quot;Bridge&quot;BOOTPROTO=staticDEVICE=&quot;br0&quot;ONBOOT=&quot;yes&quot;IPADDR=192.168.1.10NETMASK=255.255.255.0GATEWAY=192.168.1.1DNS1=114.114.114.114DNS2=8.8.8.8DELAY=&quot;0&quot;EOF\n\n自动化搭建kvm简易脚本\n#!/bin/sh# 网卡名称etlName=$1# IP地址IP=$2# 网关地址GWay=$3# 子网掩码NMask=$4if [ $&#123;etlName&#125;&quot;x&quot; == &quot;x&quot; ]||[ $&#123;IP&#125;&quot;x&quot; == &quot;x&quot; ]||[ $&#123;GWay&#125;&quot;x&quot; == &quot;x&quot; ];then  echo &quot;参数错误,请检查参数设置是否正确!!!&quot;  echo &quot;参数1: 桥接网卡名称&quot;  echo &quot;参数2: 桥接ip的地址&quot;  echo &quot;参数3: 桥接网关地址&quot;  echo &quot;参数4: 桥接子网掩码&quot;  exitfiif [ $&#123;NMask&#125;&quot;x&quot; == &quot;x&quot; ];then  NMask=&quot;255.255.255.0&quot;fiecho &quot;参数1: 桥接网卡名称-&gt; $etlName&quot;echo &quot;参数2: 桥接ip的地址-&gt; $IP&quot;echo &quot;参数3: 桥接网关地址-&gt; $GWay&quot;echo &quot;参数4: 桥接子网掩码-&gt; $NMask&quot;# 停止firewallsystemctl stop firewalld# 禁止firewall开机启动systemctl disable firewalld# 关闭selinuxsetenforce 0sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# 安装kvmyum install -y  virt-* libvirt bridge-utils qemu-img qemu-kvm virt-install libguestfs-toolssystemctl enable libvirtdsystemctl start libvirtdsystemctl status libvirtdln -s /usr/libexec/qemu-kvm /usr/bin/qemu-kvm# 备份网卡cp -a /etc/sysconfig/network-scripts/ifcfg-$&#123;etlName&#125;  /etc/sysconfig/network-scripts/ifcfg-$&#123;etlName&#125;$(date +&quot;%Y%m%d&quot;).bakcat &gt;/etc/sysconfig/network-scripts/ifcfg-$&#123;etlName&#125; &lt;&lt;EOFDEVICE=$&#123;etlName&#125;TYPE=EthernetBOOTPROTO=noneONBOOT=yesBRIDGE=br0EOF# 创建桥接网卡cat &gt;/etc/sysconfig/network-scripts/ifcfg-br0 &lt;&lt;EOFTYPE=&quot;Bridge&quot;BOOTPROTO=staticDEVICE=&quot;br0&quot;ONBOOT=&quot;yes&quot;IPADDR=$&#123;IP&#125;NETMASK=$&#123;NMask&#125;GATEWAY=$&#123;GWay&#125;DNS1=114.114.114.114DNS2=8.8.8.8DELAY=&quot;0&quot;EOFexit\n\n7.重启网络systemctl restart network\n\n8.查看网桥brctl showbrctl stp br0 on\n\n9.创建虚拟机方式1\n# 创建虚拟机文件系统qemu-img create -f qcow2 centos1.qcow2 500Gqemu-img create -f qcow2 centos2.qcow2 500G# 创建虚拟机qemu-kvm -name c1 -cpu host -smp 2 -m 64 -drive file=/data/imgs/centos1.qcow2,if=virtio,media=disk,format=qcow2 -drive file=/opt/CentOS-5.5-i386-bin-DVD.iso,index=1,media=cdrom -vnc :0 -net nic,model=e1000,macaddr=52:54:00:00:01:01 -net tap,script=/etc/qemu-ifup-pn1 -daemonizeqemu-kvm -name c2 -cpu host -smp 2 -m 64 -drive file=/data/imgs/centos2.qcow2,if=virtio,media=disk,format=qcow2 -drive file=/opt/CentOS-5.5-i386-bin-DVD.iso,index=1,media=cdrom -vnc :1 -net nic,model=e1000,macaddr=52:54:00:00:00:01 -net tap,script=/etc/qemu-ifup-pn1 -daemonize\n注意：-vnc :0&#x2F;1 是窗口号；MAC 地址52:54:00 不能改，后边随意\n开2个终端，vnc 连接虚拟机\nvncviewer 192.168.30.107:0vncviewer 192.168.30.107:1\n方式2推荐此方式\n# 创建虚拟机文件系统qemu-img create -f qcow2 centos1.qcow2 500G# 用virt-manager图形化安装：略virt-manager\nvirt-manager若无法打开尝试执行export DISPLAY&#x3D;Windows的IP:0.0\nexport DISPLAY=192.168.11.13:0.0\n\n方式3\n# 创建虚拟机文件系统qemu-img create -f qcow2 centos1.qcow2 500G# virt-install安装virt-install --name=master1 --memory 8192 --vcpus=2 --disk path=/data/qemu/centos1/centos1.qcow2,size=500,format=qcow2,bus=virtio --accelerate --cdrom=/data/CentOS-7-x86_64-DVD-1810.iso --vnc --vncport=6004 --vnclisten=0.0.0.0 --network bridge=br0,model=virtio --noautoconsole\n\n\n10.KVM虚拟机日常操作命令KVM虚拟机的管理主要是通过virsh命令对虚拟机进行管理。\n下面列出kvm日常管理中的命令:\n查看KVM虚拟机配置文件及运行状态KVM虚拟机默认配置文件位置: &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;autostart目录是配置kvm虚拟机开机自启动目录。\nvirsh命令帮助:\nvirsh -help\n或直接virsh命令和，再执行子命令。如下所示。\nvirsh欢迎使用 virsh，虚拟化的交互式终端。输入：&#x27;help&#x27; 来获得命令的帮助信息&#x27;quit&#x27; 退出virsh # help# ..\n\n(1)查看kvm虚拟机状态virsh list --all\n\n(2)KVM虚拟机开机virsh start oeltest01\n\nKVM虚拟机关机或断电默认情况下virsh工具不能对linux虚拟机进行关机操作，linux操作系统需要开启与启动acpid服务。在安装KVM linux虚拟机必须配置此服务。\nchkconfig acpid onservice acpid restart\n\n(3)virsh关机virsh shutdown oeltest01\n\n(4)强制关闭电源virsh destroy wintest01\n\n(5)通过配置文件启动虚拟机virsh create /etc/libvirt/qemu/wintest01.xml\n\n(6)配置开机自启动虚拟机virsh autostart oeltest01\n\nautostart目录是kvm虚拟机开机自启动目录，可以看到该目录中有KVM配置文件链接。\n(7)导出KVM虚拟机配置文件virsh dumpxml wintest01 &gt; /etc/libvirt/qemu/wintest02.xml\n\nKVM虚拟机配置文件可以通过这种方式进行备份。\n(8)删除kvm虚拟机virsh undefine wintest01\n\n说明：该命令只是删除wintest01的配置文件，并不删除虚拟磁盘文件。\n(9)重新定义虚拟机配置文件通过导出备份的配置文件恢复原KVM虚拟机的定义，并重新定义虚拟机。\nmv /etc/libvirt/qemu/wintest02.xml /etc/libvirt/qemu/wintest01.xmlvirsh define /etc/libvirt/qemu/wintest01.xml\n\n(10)编辑KVM虚拟机配置文件virsh edit wintest01\n\nvirsh edit将调用vi命令编辑&#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;wintest01.xml配置文件。也可以直接通过vi命令进行编辑，修改，保存。可以但不建议直接通过vi编辑。\nvi /etc/libvirt/qemu/wintest01.xml\n\n(11)kvm虚拟机console控制台登录配置virsh console oeltest01\n\n(12)挂起服务器virsh suspend oeltest01\n\n(13)恢复服务器virsh resume oeltest01\n\n(14)创建虚拟机# 参数样例virt-install --name=centos1 \\--ram 1024 --vcpus=1 \\--disk path=/root/centos1.img,size=10 \\--accelerate --cdrom /root/CentOS-6.5-x86_64-bin-DVD1.iso \\--graphics vnc,port=5921 --network bridge=br0\n\n(15)如果有图形界面的话，可以进入虚拟机的界面virt-viewer centos1\n\n(16)重启虚拟机virsh reboot centos1\n\n(17)暂停虚拟机virsh suspend centos1\n\n(18)恢复虚拟机virsh resume centos1\n\n(19)克隆centos1\n如果我们要建几个一样的虚拟机，这个命令，非常有用。\nvirt-clone --connect=qemu:#/system -o centos1 -n centos3 -f /root/centos3.img\n\n(20)未登录的情况下，查看镜像目录# 查看centos.img镜像文件中/home目录virt-ls centos.img /home# tank # mysql\n\n(21)查看虚拟机的分区情况virt-filesystems -d centos1# /dev/sda1# /dev/VolGroup/lv_root\n\nvirt-df centos.img# Filesystem 1K-blocks Used Available Use%# centos.img:/dev/sda1 495844 34510 435734 7%# centos.img:/dev/VolGroup/lv_root 8780808 2842056 5492700 33%\n\n(22) 将centos1中的文件copy到tmp下面virt-copy-out -d centos1 /etc/passwd /tmp\n\n(23)mount虚拟机guestmount -a /root/centos.img -m /dev/VolGroup/lv_root --rw /mnt/usbcd /mnt/usb/ls# bin dev home lib64 media mnt opt root selinux sys usr# boot etc lib lost+found misc net proc sbin srv tmp var\n\n(24)修改kvm中虚拟机的内存大小# 修改配置文件后需要重新启动下虚拟机，先关闭它virsh shutdown vm01# Domain vm01 is being shutdown# 确认已经被关闭virsh list# Id Name State# ----------------------------------# 1 vm02 running\n\n# 注意vi直接编辑不生效virsh edit vm01 # vm01# df8604c1-dcf3-fa98-420f-6eea7b39c395# 1048576 ###本来开始设置为1G，现在这个单位是k# 1048576# 修改为1.5G\n\n# 启动virsh start vm01# Domain vm01 started\n\n(25)删除一个虚拟机可以删除一个状态为“shut off”的虚拟机。\nvirsh undefine vm01rm -f /home/data/vm01.img\n\n(26)网络设置# 查看网络virsh net-list# 名称               状态     自动开始  Persistent# --------------------------------------------------# default              活动     yes           yesbrctl show# bridge name   bridge id       STP enabled interfaces# br0       8000.0cc47acaec9a   no      eth0#                           vnet0#                           vnet1# docker0       8000.000000000000   no      # virbr0        8000.525400b7157f   yes     virbr0-nic# 移除网络brctl delif bro vnet1# 添加网络brctl addif bro vnet1\n\n(27)virtsh参数如下:autostart      #自动加载指定的一个虚拟机connect        #重新连接到hypervisorconsole        #连接到客户会话create         #从一个SML文件创建一个虚拟机start          #开始一个非活跃的虚拟机destroy        #删除一个虚拟机define         #从一个XML文件定义一个虚拟机domid          #把一个虚拟机名或UUID转换为IDdomuuid        #把一个郁闷或ID转换为UUIDdominfo        #查看虚拟机信息domstate       #查看虚拟机状态domblkstat     #获取虚拟机设备快状态domifstat      #获取虚拟机网络接口状态dumpxml        #XML中的虚拟机信息edit           #编辑某个虚拟机的XML文件list           #列出虚拟机migrate        #将虚拟机迁移到另一台主机quit           #退出非交互式终端reboot         #重新启动一个虚拟机resume         #重新恢复一个虚拟机save           #把一个虚拟机的状态保存到一个文件dump           #把一个虚拟机的内核dump到一个文件中以方便分析shutdown       #关闭一个虚拟机setmem         #改变内存的分配setmaxmem      #改变最大内存限制值suspend        #挂起一个虚拟机vcpuinfo       #虚拟机的cpu信息version        #显示virsh版本\n\n\n\n来源简书\n","categories":["KVM"],"tags":["KVM"]},{"title":"Kafka故障模拟及故障恢复","url":"/20241121/Kafka/cb7389023fa4/","content":"Kafka故障模拟及故障恢复来自Kafka中Broker节点磁盘问题的故障处理方法_参考网 (fx361.com)\n摘  要：Apache Kafka作为一种分布式的消息队列中间件，由于其具有高可靠性、高吞吐量、可持久化、可扩展性好等特点。在大数据项目中，如日志聚合、流数据处理等应用场景中被广泛使用。由于Kafka的消息需要持久化到磁盘中，磁盘故障会影响Kafka的使用，严重时会造成数据丢失。所以基于Kafka的存储特性，通过复盘和分析由于磁盘问题导致的Kafka集群故障，提出了一系列的磁盘故障处理方法，从而缩短Kafka集群故障的恢复时间。\n0  引  言Apache Kafka[1]，最初由LinkedIn公司開发，并于2011年开源[2]。2012年被孵化成为Apache软件基金会顶级项目。如今，Kafka应用于众多大数据项目中，很多互联网公司也在自己的生产环境中将Kafka作为消息中间件使用。\n1  Kafka组件及架构Kafka作为一种分布式的消息队列中间件，部署多采用若干节点构成集群的方式。在这个Kafka集群中，每个节点被称作Broker，可以理解为Kafka提供服务的一个实例。在消息（message）队列系统中，通常都会有生产者（Producer）发送消息，消费者（Consumer）消费消息，这样就构成了一个消息“流水线”的上下游，如图1所示。每条被Producer发布到Kafka集群的消息都属于一个Topic。\n2  Kafka中的文件存储介绍Topic经过Producer发布到Kafka集群中，这条Topic会根据配置被划分为多个分区（Partition），这些分区又会被均匀地分布到Kafka集群所有的Broker节点上。这样做可以通过增加分区的数量来横向增加Topic的存储数据量，并且均匀分布也可以起到负载均衡的作用。\n在存储层面，任何发布到此分区的消息都会被追加（append）到数据文件的尾部，文件以“.log”为后缀。消息被追加到分区中因为是顺序写入（write）磁盘的，因此效率非常高。如图2所示，图中不同颜色的数据文件对应的是不同的分区数据，append操作正在写入对应虚线数据文件。\n除了log文件，分区中还有一个以“.index”为后缀的索引文件，它们共同组成段（Segment）文件。在分区中会存在多个段文件，它们大小相等，但其中包含的消息数不一定相等。这种特性方便旧的段文件可以被快速删除，这样可以清理空间供新的消息进行存储，提高磁盘利用率。\n作为分布式系统，Kafka在设计上也充分考虑了高可用，从Broker的多节点到Topic的多副本。Topic的副本机制则是通过分区的副本实现的，被称为Replica，即在另一个或多个Broker节点上存在这个分区的副本。\n3  故障复盘与分析在公司某生产环境里的Kafka集群中，一个Broker节点的磁盘发生故障，导致这个Broker节点的进程退出[3]，进而影响了Kafka中的某一个Topic的正常使用。\n如果启用副本，Kafka至少不会因为单个节点不能对外服务而发生Topic不能正常使用的情况，这就是Topic的高可用性。本次故障影响使用的主要原因就是Topic没有设置副本，采用系统默认值1。在Broker节点发生磁盘故障停止服务时，由于这个Topic在故障Broker的分区没有可以使用的副本，导致了此Topic不能正常写入和消费数据的问题。\n当发生磁盘故障，通常快速恢复Kafka服务的方法就是修改Kafka的server.properties配置中log.dirs参数，将故障磁盘从配置中删除，Broker就可以启动了。Broker启动之后，节点上故障磁盘的分区会在此Broker的其他磁盘中创建。但对于这次的Kafka故障还遇到了下文提到的两种意外情况。\n3.1  启动时触发了特定版本Kafka的bug启动Broker时，日志出现异常报错，显示读取index文件损坏，不能启动，如图3所示。遇到这种问题时一般是删除抛出异常的index文件。\nindex文件存放的元数据指向对应的log文件中消息的物理偏移地址，如图4所示。\n那为什么index会发生损坏呢？这是因为index文件是一个索引文件映射，它不会对每条消息都建立索引，而是间隔indexIntervalBytes大小之后才写入一条索引条目，所以是一个稀疏文件。Kafka运行时会创建一个log.index.size.max.bytes大小的index文件，向其中写入稀疏索引，内容达到阈值后会进行滚动覆盖。根据社区jira的内容[4]，在Kafka非正常退出后会出现index损坏的情况，而在0.8及以前版本，Kafka在读取这个损坏的index文件后会出现报错退出无法启动的问题，在0.9版本中对此问题进行了修复[5]，处理的逻辑是自动清理这个文件后重建，不抛出异常。\n3.2  转移的分区将磁盘空间写满将故障磁盘从配置文件中删除后重新启动Broker，故障磁盘中所有Topic的分区副本会在剩余磁盘中重新创建，并同步消息数据，此时出现了多个大数据量的分区副本被放入同一个磁盘中，导致磁盘空间被迅速写满。这种情况下，就不能再使用剔除磁盘的方法了。紧急处理时可以采取缩短Topic的保存时间，从物理上减小Topic数据大小，然后分阶段删除磁盘中过期数据，最后重启Broker节点恢复。\n4  实验验证对于磁盘故障这类服务器常见问题，如何能将故障对Kafka集群的影响减少至最低是研究的重点。对此总结了如下的恢复步骤，并通过实验进行了验证，可供参考使用。\n4.1  紧急恢复故障时可以剔除故障磁盘后重启Step1：删除Kafka配置文件config&#x2F;server.properties中损坏的磁盘（例如data2为故障磁盘，原配置为：“log.dirs&#x3D;&#x2F;data0&#x2F;Kafka-logs，&#x2F;data1&#x2F;Kafka-logs，&#x2F;data2&#x2F;Kafka-logs，&#x2F;data3&#x2F;Kafka-logs…”，更改后配置为：“log.dirs&#x3D;&#x2F;data0&#x2F;Kafka-logs，&#x2F; data1&#x2F;Kafka-logs，&#x2F;data3&#x2F;Kafka-logs…”）。\nStep2：重启Kafka进程。\n结果：原“&#x2F;data2&#x2F;Kafka-logs，”目录下的分区会被重新分配到当前Broker的其他磁盘上。\n影响：会产生数据倾斜的情况，大数据量的分区叠加到同一个磁盘，可能造成个别磁盘被写满。\n4.2  最小化影响恢复故障集群可允许一个Broker下线时，可暂不重启Kafka进程，待磁盘更换完成后直接重启Kafka进程。\n如果当前Broker有多余的一块磁盘作备盘。当Kafka进程下线时，修改配置文件config&#x2F;server.properties（例如data2为故障磁盘，data9为备盘，原配置为：“log.dirs&#x3D;&#x2F;data0&#x2F;Kafka-logs，&#x2F;data1&#x2F;Kafka-logs，&#x2F;data2&#x2F;Kafka-logs，&#x2F;data3&#x2F;Kafka-logs…”，替换后配置为：“log.dirs&#x3D;&#x2F;data0&#x2F;Kafka-logs，&#x2F;data1&#x2F;Kafka-logs，&#x2F;data9&#x2F;Kafka-logs，&#x2F;data3&#x2F;Kafka-logs…”，用data9替换data2），直接启动Kafka进程。之后在最近的系统维护周期时间点更换坏盘。\n4.3  具体实验验证过程环境：测试集群共3个Broker（Broker1至Broker3），每个主机上挂载5块磁盘（data0至data4）。Kafka配置文件config&#x2F;server.properties配置了4块磁盘，即data0、data1、data2、data3，data4作为备盘。\nStep1：新建测试Topic为testKafka，配置分区为12，副本数为2。此时分区均匀分布，每个Broker中8个分区。\nStep2：测试Topic testKafka，进行正常的信息生产和消费，此时查看Broker3中data3目录下面文件，存在2个分区，如图5所示。\nStep3：直接删除data3目录，模拟磁盘故障，Kafka进程退出。此时，修改Kafka配置文件config&#x2F;server.properties，将data3换成data4，即四块磁盘变成了data0、data1、data2、data4。\nStep4：上述步骤完成后，重启Broker3服务，此时会发现消费Topic数据时会有短暂告警打印，后续恢复正常。\n结果：磁盘data3中的2个分区转移到备用磁盘data4中，如图6所示。\n5  结  论本文描述了在磁盘损坏后导致Kafka集群出现的几种异常情况，提出了在这些情况下的几种故障处理方法，并通过实验进行模拟验证。这些方法可以应用于日常运维Kafka集群的工作中，有效提高了Kafka集群可用性，为避免数据丢失提供了参考方案。\n","categories":["Kafka"],"tags":["Kafka"]},{"title":"Kafka数据迁移","url":"/20241121/Kafka/ab0b4106805a/","content":"Kafka数据迁移进入kafka安装目录bin下执行以下操作\ncd /data/kafka/bin\n\n1.导出主题到本地./kafka-console-consumer.sh  --bootstrap-server 192.168.11.160:9092,192.168.11.161:9092,192.168.11.162:9092 --topic  主题名称  --from-beginning  &gt; /tmp/主题名称.txt\n\n\n\n2.查看主题,如果主题不存在,创建主题2.1列出主题./kafka-topics.sh --list --bootstrap-server 192.168.11.160:9092,192.168.11.161:9092,192.168.11.162:9092\n\n2.2创建主题./kafka-topics.bat --create --bootstrap-server  192.168.11.204:9092,192.168.11.205:9092,192.168.11.206:9092 --replication-factor 2 --partitions 3 --topic 主题名称\n\n\n\n3.导入主题文件./kafka-console-producer.sh --broker-list 192.168.11.204:9092,192.168.11.205:9092,192.168.11.206:9092 --topic 主题名称 &lt; /tmp/主题名称.txt\n\n4.kafka增加分区(可不更改)./kafka-topics.sh --alter --broker-list 192.168.11.204:9092,192.168.11.205:9092,192.168.11.206:9092 --topic 主题名称 --partitions 分区数量\n\n","categories":["Kafka"],"tags":["Kafka"]},{"title":"Kafka配置SASL/PLAIN认证","url":"/20241121/Kafka/6377c01cee71/","content":"Kafka 配置 SASL&#x2F;PLAIN 认证下载安装zk与kafka略过\n安装目录为/data/kafka\n安装目录为/data/zookeeper\n配置 Kafkavim config/server.properties\n\n更改kafka配置文件,增加以下配置\ndelete.topic.enable=true  listeners=SASL_PLAINTEXT://192.168.11.14:9092advertised.listeners=SASL_PLAINTEXT://192.168.11.14:9092# 启用ACLauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer# 设置admin为超级用户super.users=User:admin# 启用SCRAM机制，采用SCRAM-SHA-512/PLAIN算法sasl.enabled.mechanisms=PLAIN# 为broker间通讯开启SCRAM机制，采用SCRAM-SHA-512/PLAIN算法sasl.mechanism.inter.broker.protocol=PLAIN# broker间通讯使用PLAINTEXT，本例中不演示SSL配置security.inter.broker.protocol=SASL_PLAINTEXTzookeeper.set.acl=true# 禁止自动创建topicauto.create.topics.enable=trueallow.everyone.if.no.acl.found=false# 参数 allow.everyone.if.no.acl.found# 设置为 true，ACL 机制改为黑名单机制，只有黑名单中的用户无法访问# 设置为 false，ACL 机制改为白名单机制，只有白名单中的用户可以访问，默认值为 false\n\n\n\n首先创建配置文件 kafka/config/kafka_server_jaas.conf\ncd kafka_2.12-2.8.0/configvim kafka_server_jaas.conf\n\nKafkaServer &#123;  org.apache.kafka.common.security.plain.PlainLoginModule required    username=&quot;admin&quot;    password=&quot;adminpass&quot;    user_admin=&quot;adminpass&quot;    user_producer=&quot;producer&quot;    user_consumer=&quot;consumer&quot;;&#125;;KafkaClient &#123;  org.apache.kafka.common.security.plain.PlainLoginModule required    username=&quot;admin&quot;    password=&quot;adminpass&quot;    user_admin=&quot;adminpass&quot;    user_producer=&quot;producer&quot;    user_consumer=&quot;consumer&quot;;&#125;;Client &#123;  org.apache.kafka.common.security.plain.PlainLoginModule required    username=&quot;admin&quot;    password=&quot;adminpass&quot;    user_admin=&quot;adminpass&quot;;&#125;;Server &#123;  org.apache.kafka.common.security.plain.PlainLoginModule required    username=&quot;admin&quot;    password=&quot;adminpass&quot;    user_admin=&quot;adminpass&quot;;&#125;;\n\nuser_用户名 配置的是 client 连接 broker 时用的用户、密码。据我尝试，必须定义一个 username 用户对应的 user_ 字段，否则连不上。就像上面，有个 username&#x3D;”testuser” ，所以必须再定义一次 user_testuser 且密码保持一致。\nKafkaServer 和 KafkaClient 是 Kafka Brokers 之间，以及 Kafka 客户端通讯的配置，Server 和 Client 是 Kafka 与 Zookeeper 之间的配置。注意这里和 Zookeeper 配置的 jaas 有点不一样，Kafka 这里是 org.apache.kafka.common.security.plain.PlainLoginModule ，Zookeeper 是 org.apache.zookeeper.server.auth.DigestLoginModule 。\n然后编辑启动脚本 bin/kafka-server-start.sh ，在后最添加一行 export 语句\nexport KAFKA_OPTS=&quot; -Djava.security.auth.login.config=/data/kafka/config/kafka_server_jaas.conf&quot;exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka &quot;$@&quot;\n\n最后再重新启动 kafka 就可以了。我们在 jaas 文件中配置了两用户，admin ,consumer和producer，其中我们设置了 admin 为超级用户，consumer和producer分别为生产者和消费者 。\n配置 Zookeeper由于 Kafka 的 metadata 数据是保存在 zookeeper 中的，所以需要设置 zookeeper 支持 SASL 验证，然后配置权限，禁止未登录用户随便删除 Topic 等。\n编辑配置文件 zookeeper&#x2F;conf&#x2F;zoo.cfg\nauthProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProviderrequireClientAuthScheme=sasljaasLoginRenew=3600000\n\n其中 authProvider.1 这里的 .1 是 server_id ，多个 zookeeper 节点每个都要配置，例如你有 3 台 zk，那么需要再加上 authProvider.2 和 authProvider.3 等。\n添加 jaas 配置文件 zookeeper&#x2F;conf&#x2F;zookeeper_jaas.conf\nClient &#123;  org.apache.zookeeper.server.auth.DigestLoginModule required    user_admin=&quot;adminpass&quot;;&#125;;Server &#123;  org.apache.zookeeper.server.auth.DigestLoginModule required    user_admin=&quot;adminpass&quot;;&#125;;\n\n修改 zookeeper&#x2F;bin&#x2F;zkEnv.sh ，添加一行\nexport SERVER_JVMFLAGS=&quot;-Djava.security.auth.login.config=$&#123;ZOOBINDIR&#125;/../conf/zookeeper_jaas.conf $SERVER_JVMFLAGS&quot;\n\n这里因为我是用的单独的 Zookeeper 程序包，如果你用的是 Kafka 自带的 Zookeeper，那 SERVER_JVMFLAGS 要改成 KAFKA_OPTS 。滚动重启 Zookeeper 。\n配置 zkCli.sh 使用上鉴权。创建配置文件 zookeeper&#x2F;conf&#x2F;adminclient_jaas.conf\nClient &#123;  org.apache.zookeeper.server.auth.DigestLoginModule required    username=&quot;admin&quot;    password=&quot;adminpass&quot;;&#125;;\n\n然后修改 zookeeper&#x2F;bin&#x2F;zkCli.sh ，在 java 后面添加，\n&quot;$JAVA&quot; &quot;-Djava.security.auth.login.config=/data/zookeeper/conf/adminclient_jaas.conf&quot; &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; \\     -cp &quot;$CLASSPATH&quot; $CLIENT_JVMFLAGS $JVMFLAGS \\     org.apache.zookeeper.ZooKeeperMain &quot;$@&quot;\n\n配置 Kafka ACL 权限先修改 Kafka 里的 kafka&#x2F;bin&#x2F;zookeeper-security-migration.sh 文件，添加\nexport KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/data/release/dp_kafka/config/kafka_server_jaas.conf&quot;\n\n然后运行命令 ，参考这里的文档\nkafka/bin/zookeeper-security-migration.sh --zookeeper.connect 127.0.0.1:2181 --zookeeper.acl secure\n\n这样 Zookeeper 里面的 metadata 都加上权限了。可以上 Zookeeper 验证下\nzookeeper/bin/zkCli.sh# 登录进 shell 后，执行 getAcl 指令getAcl /brokers/topics&#x27;sasl,&#x27;admin: cdrwa&#x27;world,&#x27;anyone: r\n\n为了用上 Kafka 自带的 shell 工具，我们要配置 jaas 认证，新建一个 kafka&#x2F;config&#x2F;adminclient-configs.conf\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule \\    required username=&quot;admin&quot; password=&quot;adminpass&quot;;security.protocol=SASL_PLAINTEXTsasl.mechanism=PLAIN\n\n使用\nkafka/bin/kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --command-config kafka/config/adminclient-configs.conf --list\n\n一些 ACL 命令使用，参考文档\n# 列出 temp 这个 topic 的 ACL 列表kafka/bin/kafka-acls.sh --bootstrap-server 127.0.0.1:9092 --command-config kafka/config/adminclient-configs.conf --list --topic &quot;temp&quot;# 对 temp 这个 topic 对用户 test 授权kafka/bin/kafka-acls.sh --bootstrap-server 127.0.0.1:9092 --command-config kafka/config/adminclient-configs.conf --add --allow-principal User:test --operation Read --operation Write --operation AlterConfigs --operation Describe --operation DescribeConfigs --operation Alter --topic &quot;temp&quot;# 去掉 delete 权限kafka/bin/kafka-acls.sh --bootstrap-server 127.0.0.1:9092 --command-config kafka/config/adminclient-configs.conf --remove --allow-principal User:test --operation Delete --topic &quot;temp&quot;\n\n配置 Consumer 和 Producer如果 Kafka 配置了认证，再用脚本消费数据就会报错\nkafka/bin/kafka-console-consumer.sh  --bootstrap-server localhost:9092 --topic  test --from-beginning WARN [Consumer clientId=consumer-console-consumer-93884-1, groupId=console-consumer-93884] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)# 以下是 Kafka Server 端报错日志INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Failed authentication with /127.0.0.1 (Unexpected Kafka request of type METADATA during SASL handshake.) (org.apache.kafka.common.network.Selector)\n\n修改 config/consumer.properties ，添加以下配置\nsecurity.protocol=SASL_PLAINTEXTsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\    username=&quot;admin&quot; \\    password=&quot;adminpass&quot;;\n\n修改 config/producer.properties ，添加以下配置\nsecurity.protocol=SASL_PLAINTEXTsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\    username=&quot;admin&quot; \\    password=&quot;adminpass&quot;;\n\n最后再尝试运行上面的命令。\nkafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \\  --consumer.config config/consumer.properties --topic  test --from-beginning kafka/bin/kafka-console-producer.sh  --broker-list localhost:9092 --topic test \\  --producer.config=config/producer.propertieskafka/bin/kafka-topics.sh --list --bootstrap-server=localhost:9092 \\  --command-config config/consumer.properties \n\n已经加上鉴权的 zookeeper 去掉鉴权如果对已经加上鉴权的 zookeeper、kafka 集群，现在不想要鉴权了，要回退回去，操作如下。\n首先运行命令，把 kafka 在 zookeeper 里面的 metadata 数据去掉认证：\nkafka/bin/zookeeper-security-migration.sh --zookeeper.connect 127.0.0.1:2181 --zookeeper.acl unsecure\n\n去掉后，可以上 zookeeper 使用 getAcl 命令确认。\n然后，修改 zookeeper 文件 zookeeper&#x2F;bin&#x2F;zkEnv.sh ，去掉之前加的\n# 把这一行注释掉# export SERVER_JVMFLAGS=&quot;-Djava.security.auth.login.config=$&#123;ZOOBINDIR&#125;/../conf/zookeeper_jaas.conf $SERVER_JVMFLAGS&quot;\n\n滚动重启 zookeeper。\n再来修改 kafka 配置，修改 kafka&#x2F;config&#x2F;kafka_server_jaas.conf ，删除连接 zookeeper 的配置 Client 和 Server 。再修改 kafka&#x2F;config&#x2F;server.properties ，删除配置项 zookeeper.set.acl&#x3D;true 。\n再滚动重启 kafka 。\n完\n参考 [给 Kafka 配置 SASL&#x2F;PLAIN 认证 )\n","categories":["Kafka"],"tags":["Kafka"]},{"title":"Keepalived与Haproxy安装配置","url":"/20241121/Keepalived/8c774a5c67da/","content":"Keepalived与Haproxy安装配置1.Keepalived安装 keepalived与k8s系统的master安装在相同服务器，那么根据描述我们可以使用VIP访问master，当master1不可用的时候，VIP漂移到master2上，保障了K8S集群的高可用性\nKeepalived主要的功能模块有三大块，分别为：虚拟IP、服务监控、虚拟服务器\n\n①虚拟IP：即为上面架构中提到的核心功能\n\n②服务监控：当本服务器的相应服务出现问题的时候降低keepalived优先级，实现IP漂移\n\n③虚拟服务器：keepalived可以为其他服务器做代理，实现负载均衡\n\n\nKeepalived安装比较简单，根据yum查询在centos7上的安装依赖\n解压后使用下面命令依次安装rpm包\ntar -zxvf keepalived.tar.gzrpm -ivh net-snmp-libs-5.7.2-48.el7_8.1.x86_64.rpmrpm -ivh --nodeps  net-snmp-agent-libs-5.7.2-48.el7_8.1.x86_64.rpmrpm -ivh keepalived-1.3.5-16.el7.x86_64.rpm\n# 注意：以前的教程中有提到rpm解决不了软件依赖关系的弊端，如果安装过程中还有依赖提示，只能慢慢摸索&#96;\nyum 安装\nyum install -y keepalived\n\n配置文件修改\nkeepalived只有一个配置文件，位置在/etc/keepalived/keepalived.conf\nvi /etc/keepalived/keepalived.confcat &gt;/etc/keepalived/keepalived.conf &lt;&lt;EOF# 全局配置 主要是配置故障发生时的通知对象以及机器标识global_defs &#123;  router_id k8s-master1  #修改为自己的hostname&#125;# 自定义VRRP实例健康检查脚本 keepalived只能做到对自身问题和网络故障的监控，Script可以增加其他的监控来判定是否需要切换主备vrrp_script check_haproxy &#123;  script &quot;/etc/keepalived/check_haproxy.sh&quot;  #检查脚本  interval 3  fall 10  timeout 9  rise 2&#125;# VRRP实例 定义对外提供服务的VIP区域及其相关属性vrrp_instance VI_1 &#123;  state MASTER     #备服务器上改为BACKUP  interface ens33    #改为自己的接口  virtual_router_id 51  priority 100     #优先级，备服务器上改为小于100的数字，90,80  advert_int 1  mcast_src_ip 192.168.150.11  #本机IP  nopreempt  authentication &#123;    auth_type PASS    auth_pass 1111  &#125;  unicast_peer &#123;    192.168.150.12    #除本机外其余两个master的IP节点    192.168.150.13  &#125;  virtual_ipaddress &#123;    192.168.150.10     #虚拟vip，自己设定  &#125;  track_script &#123;    check_haproxy  &#125;&#125;# 监测虚拟IP端口# 不检测端口可以将此段注释virtual_server 192.168.11.135 30880 &#123;    delay_loop 6    lb_algo rr    lb_kind NAT    persistence_timeout 50    protocol TCP    # 实际IP端口    real_server 192.168.11.51 30880 &#123;        weight 1        TCP_CHECK &#123;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        connect_port 30880        &#125;    &#125;&#125;EOF\n生成检查脚本\ncat &gt;/etc/keepalived/check_haproxy.sh &lt;&lt;EOF#!/bin/bash# 若检查其他服务 将haproxy替换A=\\`ps -C haproxy --no-header | wc -l\\`if [ \\$A -eq 0 ];then  systemctl stop keepalivedfiEOF\n给脚本执行权限\nchmod +x /etc/keepalived/check_haproxy.sh\n\n设置服务\nsystemctl start keepalived.servicesystemctl stop keepalived.servicesystemctl restart keepalived.servicesystemctl status keepalived.servicesystemctl enable keepalived.service\n\n2.Haproxy安装 我们使用keepalvied系统对三台K8S集群的master做了虚拟IP，使得K8S集群的访问具备了高可用性，但是并没有解决服务负载的问题\n本章节我们在keepalived和K8S中间加入了haproxy负载均衡系统\n\n①keepalived不再监控K8S的状态，改为监控haproxy的状态\n\n②由haproxy去分配访问业务流到三台K8S的master\n\n③同时haproxy支持后台业务的健康检查，使得业务同时具备高可用性和负载均衡特性\n\n\n解压依赖\ntar -zxvf haproxy_deps.tar.gz\nrpm安装\nrpm -ivh --nodeps libcom_err-devel-1.42.9-17.el7.x86_64.rpmrpm -ivh --nodeps krb5-devel-1.15.1-46.el7.x86_64.rpmrpm -ivh --nodeps pcre-devel-8.32-17.el7.x86_64.rpmrpm -ivh --nodeps zlib-devel-1.2.7-18.el7.x86_64.rpmrpm -ivh --nodeps openssl-devel-1.0.2k-19.el7.x86_64.rpmrpm -ivh --nodeps systemd-devel-219-73.el7_8.9.x86_64.rpm\n\n安装haproxy\ntar -zxvf haproxy-1.8.26.tar.gzcd haproxy-1.8.26.tar.gzmake  ARCH=x86_64 TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 USE_SYSTEMD=1  USE_CPU_AFFINITY=1  PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxy\n\n添加haproxy命令到sbin\ncp /usr/local/haproxy/sbin/ /usr/sbinhaproxy -v\n软件使用文件夹创建\nmkdir  /etc/haproxymkdir  /etc/haproxy/confmkdir  /usr/local/haproxy/run\n配置文件修改和说明\nvim /etc/haproxy/haproxy.cfg# /etc/haproxy/haproxy.cfgglobal  log     127.0.0.1 local2  chroot   /var/lib/haproxy  pidfile   /var/run/haproxy.pid  maxconn   4000  user    haproxy  group    haproxy  daemon defaults  mode          tcp  log           global  retries         3  timeout connect     10s  timeout client     1m  timeout server     1m frontend kubernetes  bind *:8443  mode tcp  option tcplog  default_backend kubernetes-apiserver backend kubernetes-apiserver  mode tcp  balance roundrobin  server k8s-master1 192.168.150.11:6443 check maxconn 2000  server k8s-master2 192.168.150.12:6443 check maxconn 2000  server k8s-master3 192.168.150.13:6443 check maxconn 2000\n修改内核参数\nvi /etc/sysctl.conf# 设置net.ipv4.ip_nonlocal_bind = 1# 设置net.ipv4.ip_forward = 1\n使修改生效\nsysctl -p  \n生成service\ncat &gt; /usr/lib/systemd/system/haproxy.service &lt;&lt;EOF[Unit]Description=HAProxy Load BalancerAfter=syslog.target network.target[Service]#支持多配置文件读取，类似于从侧面是实现配置文件的include功能。ExecStartPre=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf -c -qExecStart=/usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf -p /run/haproxy.pidExecReload=/bin/kill -USR2 $MAINPID[Install]WantedBy=multi-user.targetEOF\n设置服务\nsystemctl start haproxy.servicesystemctl stop haproxy.servicesystemctl restart haproxy.servicesystemctl status haproxy.servicesystemctl enable haproxy.service\n\n","categories":["Haproxy","Keepalived"],"tags":["Haproxy","Keepalived"]},{"title":"Kibana中文指南","url":"/20241121/ELK/7cded6e25ccb/","content":"Kibana中文指南\n\n\nThe End\n\n","categories":["ELK"],"tags":["ELK"]},{"title":"KubeSphere部署","url":"/20241121/Kubernetes/4ffd8977969d/","content":"KubeSphere部署1.基础环境依赖\n可以基于linux裸操作系统部署\n可以在K8S的基础上部署（本次部署）\n\n\n已经提前准备好Docker、harbor（镜像仓库）和K8S基础环境\n\n2.安装前准备\n(1).安装socat 到所有节点\n\n\n其功能与有瑞士军刀之称的 Netcat 类似，可以在两个流之间建立通道）helm依赖于socat,将socat-1.7.3.2-2.el7.x86_64拷贝的所有节点，并安装\n\nrpm -ivh socat-1.7.3.2-2.el7.x86_64socat -V# [root@master01 ~]# socat -V# socat by Gerhard Rieger and contributors - see www.dest-unreach.org# socat version 1.7.3.2 on Aug  4 2017 04:57:10#    running on Linux version #1 SMP Thu Nov 8 23:39:32 UTC 2018, release 3.10.0-957.el7.x86_64, machine x86_64# features:  #define WITH_STDIO 1  #define WITH_FDNUM 1  #define WITH_FILE 1  #define WITH_CREAT 1  #define WITH_GOPEN 1  #define WITH_TERMIOS 1  #define WITH_PIPE 1  #define WITH_UNIX 1\n\n(2).安装helm\n\n\n（helm分为客户端（helm）和服务端（tiller））（helm可以理解为k8s的yum，可以用chart的方式在线安装deployment）\n\n\n拷贝helm-v2.16.9-linux-amd64.tar到所有节点，解压后将linux-amd64下的helm拷贝到系统的&#x2F;usr&#x2F;local&#x2F;bin&#x2F;下，这样就可以使用helm命令tar -zxvf helm-v2.16.2-linux-amd64.tar.gzcp helm /usr/local/bin/\n拷贝tiller_v2.16.2到一个master节点，通过 docker load命令将其中的镜像解压到本地docker load &lt; tiller_v2.16.2\n重新tag tiller:v2.16.2镜像，并推送到镜像仓库docker tag 192.168.11.6:8083/kubesphere/tiller:v2.16.2  192.168.201.6:81/kubesphere/tiller:v2.16.2docker push 192.168.201.6:81/kubesphere/tiller:v2.16.2\n将rbac-config.yaml拷贝到一个master节点，通过kubectl create命令创建tiller使用的service account: tiller并分配合适的角色给它（不执行此步的话tiller容器起不来）kubectl create -f rbac-config.yaml\n将tiller-deploy.yaml拷贝到一个master节点，编辑将上面push的tiller镜像地址填入vim  tiller-deploy.yaml修改2处地址spec:      containers:      - env:        - name: TILLER_NAMESPACE          value: kube-system        - name: TILLER_HISTORY_MAX          value: &quot;0&quot;        image: 192.168.201.6:81/kubesphere/tiller:v2.16.2        imagePullPolicy: IfNotPresent ---      dnsPolicy: ClusterFirst      imagePullSecrets:      - name: 192.168.201.6      restartPolicy: Always      schedulerName: default-scheduler      securityContext:        seLinuxOptions: &#123;&#125;      serviceAccount: tiller      serviceAccountName: tiller      terminationGracePeriodSeconds: 30\n通过kubectl create命令创建tiller的deploy和podkubectl apply -f  tiller-deploy.yaml  -n kube-system\n查询pod已经正常运行，helm也获取到了serverkubectl get pod -n kube-systemhelm version\n\n\n(3).安装NFS文件系统\n\n\n注意：NFS文件系统只负责存储资源共享，和k8s没有其他关系，后面要部署的存储持久化也只是调用NFS的服务地址而已\n\n\n拷贝nfs-utils-1.3.0-0.66.el7.x86_64和rpcbind-0.2.0-49.el7.x86_64的rpm安装包到nfs服务器\n安装#服务器安装rpm -ivh nfs-utils-1.3.0-0.66.el7.x86_64 #服务器客户端都安装rpm -ivh rpcbind-0.2.0-49.el7.x86_64\n在nfs服务器上修改exports配置文件，填写需要共享的路径和权限（*代表所有服务器都可以访问）vim /etc/exports# [root@master01 ~]#vi /etc/exports# /nfs/data *(rw,no_root_squash,sync)\n启动并设置自启动nfs和rpcbind服务systemctl  enable  nfssystemctl  enable  rpcbindsystemctl  start  nfssystemctl  start  rpcbind\n在任意非nfs服务器系统上执行 showmount -e &lt;nfs服务器ip&gt;，可以得到共享信息即可showmount -e 192.168.201.6# [root@master02 ~]# showmount -e 192.168.201.6# Export list for 192.168.201.6:# /nfs/data     *\n\n\n(4).持久化存储部署（基于nfs文件系统的动态pv）\n\n\n拷贝并导入nfs-client-provisioner镜像到任意一个节点\ndocker load &lt; nfs-client-provisioner_v3.1.0-k8s1.11\ntag并上传到harbor\ndocker tag  192.168.11.6:8083/kubesphere/nfs-client-provisioner:v3.1.0-k8s1.11 192.168.201.6:81/kubesphere/nfs-6.55/kubesphere/nfs-client-provisioner:v3.1.0-k8s1.11docker push 192.168.201.6:81/kubesphere/nfs-client-provisioner:v3.1.0-k8s1.11\n因为有了helm，可以用helm离线方式安装nfs-client-provisioner，上传nfs-client-provisioner-1.2.8.tgz，并解压缩，编辑values.yaml，更改仓库地址及nfs地址\ntar  -zxvf  nfs-client-provisioner-1.2.8.tgzcd  nfs-client-provisionervim  valuse.yaml\n# Default values for nfs-client-provisioner.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1strategyType: Recreateimage:\trepository: 192.168.201.6:81/kubesphere/nfs-client-provisioner\ttag: v3.1.0-k8s1.11\tpullPolicy: IfNotPresentnfs:\tserver: 192.168.201.6\tpath: /nfs/data\t  mountOptions:# For creating the StorageClass automatically:storageClass:\tcreate: true\t# Set a provisioner name. If unset, a name will be generated.\t# provisionerName:\t# Set StorageClass as the default StorageClass\t# Ignored if storageClass.create is false\tdefaultClass: false\ncd nfs-client-provisionerhelm install ./\n\n在values.yaml所在文件夹内执行helm install .&#x2F; 成功安装后查看pod已经running，查看sc资源已经有了nfs-client，代表成功\nkubectl get sc# [root@master01 ~]# kubectl get sc# NAME                   PROVISIONER                                       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE# nfs-client (default)   cluster.local/fancy-gnat-nfs-client-provisioner   Delete          Immediate           true                   2d5h\nkubectl get pod -n default# [root@master01 nfs-client-provisioner]# kubectl get pod -n default# NAME                                                 READY   STATUS    RESTARTS   AGE# fancy-gnat-nfs-client-provisioner-598546c7d9-bcsft   1/1     Running   0          2d5h\n按照kubesphere教程要求设置默认存储(重要)\nkubectl patch storageclass nfs-client -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27;\n\n到此准备工作全部完成，可以开始离线安装kubesphere了\n\n\n\n3.在k8s集群上部署\n在线部署\n\n执行以下命令以开始安装：\nkubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yamlkubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml\n检查安装日志：\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f\n\n\n离线部署\n\n\n将7.2G的全包上传到服务器中，解压缩后得到如下tar包\ntar  -zxvf  kubesphere-images-v2.1.1.tar.gz\n将tar包内的镜像全部load到当前服务器可能会失败，多尝试几次\ndocker load &lt; ks_minimal_images.tardocker load &lt; openpitrix_images.tardocker load &lt; ks_logging_images.tardocker load &lt; ks_devops_images.tardocker load &lt; istio_images.tardocker load &lt; ks_notification_images.tardocker load &lt; example_images.tar\n\nload后本地会加载大量的镜像，不要慌，kubeshpere提供工具自动tag并上传到harbor（镜像仓库）\n\n\n使用kubesphere工具tag并上传镜像\n\n\n\n(1).将ks-installer.tar.gz拷贝到服务器并解压缩，进入ks-installer&#x2F;scrip目录后编辑create_project_harbor.sh，将harbor地址填入（用户不能用admin）\n\nvim create_project_harbor.sh# url=&quot;http://192.168.201.6:81&quot;# user=&quot;admin&quot;# passwd=&quot;Harbor12345&quot;\n\n新版的harbor仓库接口可能更换了，脚本不好使可以去web界面手动创建项目，大概30-40个项目。\n\n\n(2).执行create_project_harbor.sh，这一步会在harbor中创建一系列的项目\n\n(3).执行.&#x2F;push-image-list.sh 192.168.201.6:81（后面的IP为harbor地址）将镜像推送到harbor，因为镜像很多，需要大概半小时（根据网络速度不同）\n\n\n\n注意，这里的kubesphere&#x2F;jenkins-uc镜像，程序没有自动tar上传，但是安装kubesphere时用到了，请自行tag和推送\n\ndocker tag kubesphere/jenkins-uc:v2.1.1  192.168.201.6:81/kubesphere/jenkins-uc:v2.1.1docker push  192.168.201.6:81/kubesphere/jenkins-uc:v2.1.1 \n上传完毕后请去harbor检查镜像情况\n\n(4).编辑ks-installer目录下的kubesphere-minimal.yaml，修改harbor地址和修改ks-installer镜像地址\n\n\n这里采取最小化安装，安装完成后需要开启其他功能，可以自己选择增加\n\nvi kubesphere-minimal.yaml 修改相应的仓库地址\n---apiVersion: v1kind: Namespacemetadata:  name: kubesphere-system---apiVersion: v1data:  ks-config.yaml: |    ---    persistence:      storageClass: &quot;&quot;    etcd:      monitoring: False      endpointIps: 192.168.201.6,192.168.201.7,192.168.201.8      port: 2379      tlsEnable: True    common:      mysqlVolumeSize: 20Gi      minioVolumeSize: 20Gi      etcdVolumeSize: 20Gi      openldapVolumeSize: 2Gi      redisVolumSize: 2Gi    metrics_server:      enabled: False    console:      enableMultiLogin: True      port: 30880    monitoring:      prometheusReplicas: 1      prometheusMemoryRequest: 400Mi      prometheusVolumeSize: 20Gi      grafana:        enabled: False    logging:      enabled: True      elasticsearchMasterReplicas: 1      elasticsearchDataReplicas: 1      logsidecarReplicas: 2      elasticsearchMasterVolumeSize: 4Gi      elasticsearchDataVolumeSize: 20Gi      logMaxAge: 7      elkPrefix: logstash      containersLogMountedPath: &quot;&quot;      kibana:        enabled: False    openpitrix:      enabled: False    devops:      enabled: True      jenkinsMemoryLim: 2Gi      jenkinsMemoryReq: 1500Mi      jenkinsVolumeSize: 8Gi      jenkinsJavaOpts_Xms: 512m      jenkinsJavaOpts_Xmx: 512m      jenkinsJavaOpts_MaxRAM: 2g      sonarqube:        enabled: False        postgresqlVolumeSize: 8Gi    servicemesh:      enabled: False    notification:      enabled: False    alerting:      enabled: Falsekind: ConfigMapmetadata:  name: ks-installer  namespace: kubesphere-system---apiVersion: v1kind: ServiceAccountmetadata:  name: ks-installer  namespace: kubesphere-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  creationTimestamp: null  name: ks-installerrules:- apiGroups:  - &quot;&quot;  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apps  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - extensions  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - batch  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - rbac.authorization.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apiregistration.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - apiextensions.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - tenant.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - certificates.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - devops.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - monitoring.coreos.com  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - logging.kubesphere.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - jaegertracing.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - storage.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;- apiGroups:  - admissionregistration.k8s.io  resources:  - &#x27;*&#x27;  verbs:  - &#x27;*&#x27;---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: ks-installersubjects:- kind: ServiceAccount  name: ks-installer  namespace: kubesphere-systemroleRef:  kind: ClusterRole  name: ks-installer  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: ks-installer  namespace: kubesphere-system  labels:    app: ks-installspec:  replicas: 1  selector:    matchLabels:      app: ks-install  template:    metadata:      labels:        app: ks-install    spec:      serviceAccountName: ks-installer      containers:      - name: installer        image: 192.168.201.6:81/kubesphere/ks-installer:v2.1.1        imagePullPolicy: &quot;Always&quot;\n\n(5).确认helm 存储持久化 kubelet的正确，确认 node全部正常，kube-system的容器全部正常\n\nhelm versionkubectl get nodeskubectl get sckubectl get pod -n kube-system\n\n(6).执行安装命令\n\nkubectl apply -f kubesphere-minimal.yaml\n命令执行成功，pod为Running时，观察日志\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f\n\n日志可能会报一些错误，观察错误模块，到对应的容器内查看并解决错误,node或者master可能会拉取镜像到本地不成功，这时候需要手动拉取\n\n4.验证看到以下界面完成安装，等待所有pod完成后登录验证http://localhost:30880\n\n账号：admin密码：P@88w0rd\n\n########################################################              Welcome to KubeSphere!           ########################################################Console: http://localhost:30880Account: adminPassword: P@88w0rdNOTES：  1. After logging into the console, please check the     monitoring status of service components in     the &quot;Cluster Status&quot;. If the service is not     ready, please wait patiently. You can start     to use when all components are ready.  2. Please modify the default password after login.#####################################################\n5.遇到的问题\n(1).拉取镜像失败,手动进行pull后重新tagdocker pull 192.168.201.6:81/kubesphere/node-exporter:ks-v0.16.0docker tag 192.168.201.6:81/kubesphere/node-exporter:ks-v0.16.0 kubesphere/node-exporter:ks-v0.16.0\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之CKA大纲与题库","url":"/20241121/Kubernetes/b9b92a92c0bf/","content":"Kubernetes之CKA大纲与题库CKA大纲\n\n\nCKA题库\n\n\nThe End\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Health Check","url":"/20241121/Kubernetes/97869502a545/","content":"Kubernetes之Health Check\n描述\n\n强大的自愈能力是 Kubernetes 这类容器编排引擎的一个重要特性。自愈的默认实现方式是自动重启发生故障的容器。除此之外，用户还可以利用 Liveness 和 Readiness 探测机制设置更精细的健康检查，进而实现如下需求：\n\n1.零停机部署。\n2.避免部署无效的镜像。\n3.更加安全的滚动升级。\n\n默认健康检查默认情况下，每个容器启动时都会执行一个进程，由Dockerfile中的CMD或ENTRYPOINT指定。如果进程退出时的返回码不为0，则认为容器发生了故障，K8S会根据重启策略（restartPolicy）重启容器。\n\nPod 的 restartPolicy 设置为 OnFailure，默认为 Always\n\n例如下面这个例子，它模拟了容器发生故障的场景，注意下面配置文件中的args选项的定义：\napiVersion: v1kind: Podmetadata:  name: edc-healthcheck-demo  labels:    test: healthcheckspec:  restartPolicy: OnFailure  containers:    - name: healthcheck      image: busybox      imagePullPolicy: IfNotPresent      args:      - /bin/sh      - -c      - sleep 10; exit 1\n\n其中 sleep 10; exit 1代表启动10秒之后发生故障就非正常退出（返回码不为0），然后通过kubectl创建Pod：\n执行 kubectl apply 创建 Pod，命名为 health-check。\nkubectl apply -f health-check.yaml# pod &quot;health-check&quot; created\n\nkubectl get pod health-check# NAME          READY  STATUS   RESTART  AGE# health-check  1/1    Running  3        2\n可看到容器当前已经重启了 3 次。\n在上面的例子中，容器进程返回值非零，Kubernetes 则认为容器发生故障，需要重启。\n但有不少情况是发生了故障，但进程并不会退出。比如访问 Web 服务器时显示 500 内部错误，可能是系统超载，也可能是资源死锁，此时 httpd 进程并没有异常退出，在这种情况下重启容器可能是最直接最有效的解决方案，那我们如何利用 Health Check 机制来处理这类场景呢？\n答案就是 Liveness 探测\nLivenessProbe探测用于判断容器是否存活（Running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应的处理，如果一个容器不包含LivenessProbe探针，那么kubelet认为该容器LivenessProbe探针返回的值永远是Success\n一句话Liveness：如果检测有问题（如果健康检查失败），重启pod！至于怎么检测，你说了算（自定义判断容器是否健康的条件）\n\n1.ExecAction\n\n\n在容器内部执行一个命令，如果该命令返回码为0，则表明容器健康。\n\napiVersion: v1kind: Podmetadata:  labels:   test: liveness  name: liveness-execspec:  containers:  - name: liveness    image: gcr.io/google_containers/busybox    args:    - /bin/sh    - -c    - echo ok &gt; /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600    livenessProbe:      exec:        command:        - cat        - /tmp/health      initialDelaySeconds: 15      timeoutSeconds: 1\n\n\n2.TCPSocketAction\n\n\n通过容器的IP地址和端口号执行TCP检查，如果能够建立TCP连接，则表明容器健康\n\napiVersion: v1kind: Podmetadata:  labels:   test: liveness  name: liveness-execspec:  containers:  - name: nginx    image: nginx    ports:    - containerPort: 80    livenessProbe:      tcpSocket:        port: 80      initialDelaySeconds: 30      timeoutSeconds: 1\n\n\n3.HTTPGetAction\n\n\n通过容器的IP地址、端口号及路径调用HTTP Get方法，如果相应的状态码大于等于200且小于400，则认为容器健康\n\napiVersion: v1kind: Podmetadata:  labels:   test: liveness  name: liveness-execspec:  containers:  - name: nginx    image: nginx    ports:    - containerPort: 80    livenessProbe:      httpGet:        path: /_status/healthz        port: 80      initialDelaySeconds: 30      timeoutSeconds: 1\n\nLiveness提供了一些重要的参数\ninitialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒,看运行的服务而定。periodSeconds：      执行探测的频率，默认是10秒，最小1秒。timeoutSeconds：     探测超时时间，默认1秒，最小1秒。successThreshold：   探测失败后，最少连续探测成功多少次才被认定为成功，默认是1，对于liveness必须是1，最小值是1。failureThreshold：   探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1.\n\n\n\n\nReadinessProbe探测用于判断容器服务是否可用（Ready状态），达到Ready状态的Pod才可以接收请求。对于被service管理的Pod，Service与Pod Endpoint的关联关系也将基于Pod是否Ready进行设置。如果在运行过程中Ready状态变为False，则系统自动将其从service的后端Endpoint里表中隔离出去，后续再把恢复到Ready状态的Pod加回到后端Endpoint列表。这样就能保证客户端在访问service时不会被转发到服务不可用的Pod实例上\n\nreadinessProbe的配置语法与livenessProbe完全一致\n\nLiveness与Readiness比较下面对 Liveness 探测和 Readiness 探测做个比较\n\n1.Liveness 探测和 Readiness 探测是两种 Health Check 机制，如果不特意配置，Kubernetes 将对两种探测采取相同的默认行为，即通过判断容器启动进程的返回值是否为零来判断探测是否成功。\n\n2.两种探测的配置方法完全一样，支持的配置参数也一样。不同之处在于探测失败后的行为：Liveness 探测是重启容器；Readiness 探测则是将容器设置为不可用，不接收 Service 转发的请求。\n\n3.Liveness 探测和 Readiness 探测是独立执行的，二者之间没有依赖，所以可以单独使用，也可以同时使用。\n\n4.用 Liveness 探测判断容器是否需要重启以实现自愈；用 Readiness   探测判断容器是否已经准备好对外提供服务。Readiness 可用于指定容器启动1分后，判断容器各服务是否已正常启动（如启动脚本执行后写指定内容至特定文件）\n\n\n\nLiveness与Readiness是独立执行的，二者无依赖，可以单独使用也可以同时使用\n\nLiveness 和 Readiness 共用的情况\napiVersion: v1kind: Podmetadata:  name: goproxy  labels:    app: goproxyspec:  containers:  - name: goproxy    image: cnych/goproxy    ports:    - containerPort: 8080    readinessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 5      periodSeconds: 10    livenessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 15      periodSeconds: 20\n\n\n在Rolling Update中的应用现在有一个正常运行的多副本应用，我们要对其进行滚动更新即Rolling Update，K8S会逐步用新Pod替换旧Pod，结果就有可能发生这样的一个场景：当所有旧副本被替换之后，而新的Pod由于人为配置错误一直无法启动，因此整个应用将无法处理请求，无法对外提供服务，后果很严重！\n因此，Readiness探测还提供了用于避免滚动更新中出现这种情况的一些解决办法，比如maxSurge和maxUnavailable两个参数，用来控制副本替换的数量。\napiVersion: apps/v1kind: Deploymentmetadata:  name: edc-webapi-deployment  namespace: aspnetcorespec:  strategy:    rollingupdate:      maxSurge: 25%      maxUnavailable: 25%  replicas: 10  selector:    matchLabels:      name: edc-webapi  template:    metadata:      labels:        name: edc-webapi    spec:      containers:      - name: edc-webapi-container        image: edisonsaonian/k8s-demo:1.2        ports:        - containerPort: 80        imagePullPolicy: IfNotPresent        readinessProbe:          httpGet:            scheme: HTTP            path: /api/health            port: 80          initialDelaySeconds: 10          periodSeconds: 5---apiVersion: v1kind: Servicemetadata:  name: edc-webapi-service  namespace: aspnetcorespec:  type: NodePort  ports:    - nodePort: 31000       port: 8080      targetPort: 80  selector:    name: edc-webapi\n\n\n\n(1) maxSurge : 25% &#x3D;&gt; 控制滚动更新过程中副本总数超过预期（这里预期是10个副本 replicas: 10）的上限，可以是数值也可以是百分比，然后向上取整。这里写的百分比，默认值是25%；\n\n如果预期副本数为10，那么副本总数的最大值为RoundUp(10 + 10 * 25%)&#x3D;13个。\n\n(2) maxUnavailable : 25% &#x3D;&gt; 控制滚动更新过程中不可用的副本（这里预期是10个副本 replicas: 10）占预期的最大比例，可以是数值也可以是百分比，然后向下取整，同样地默认值也是25%；\n\n如果预期副本总数为10，那么可用的副本数至少要为10-roundDown(10 * 25%)&#x3D;10-2&#x3D;8个。\n综上看来，maxSurge的值越大，初始创建的新副本数量就越多；maxUnavaliable值越大，初始销毁的旧副本数量就越多；\n如果滚动更新失败，可以通过 kubectl rollout undo 回滚到上一个版本。\nkubectl rollout history deployment edc-webapi-deployment\n\n\n本章我们讨论了 Kubernetes 健康检查的两种机制：Liveness 探测和 Readiness 探测，并实践了健康检查在 Scale Up 和 Rolling Update 场景中的应用\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Kubeadm安装k8s","url":"/20241121/Kubernetes/9f4d917c38de/","content":"Kubernetes之Kubeadm安装k8s1.初始化服务器相关环境# 关闭防火墙 master及node节点设置systemctl stop firewalldsystemctl disable firewalld\n\n# 关闭swap master及node节点设置# 临时关闭swapoff -a sed -i &#x27;/swap/s/^/#/&#x27; /etc/fstabcat /etc/fstab \n\n# 关闭selinux master及node节点设置setenforce 0sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# 或者vim /etc/selinux/configSELINUX=disabled\n\n# 同步时间 master及node节点设置yum -y install ntpdatentpdate cn.pool.ntp.org# 安装常用包 master及node节点设置yum install vim bash-completion net-tools gcc wget curl lrzsz -y\n\n# host文件添加主机名和对应IP master及node节点设置cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.150.11 k8s-master1192.168.150.12 k8s-master2192.168.150.13 K8s-master3192.168.150.14 k8s-node1EOF\n\n# 解决路由异常问题 master及node节点设置# 配置内核参数，将桥接的IPv4流量传递到iptables的链cat &gt;&gt; /etc/sysctl.d/k8s.cof &lt;&lt;EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system\n\ncat &gt;ssh-kye.sh &lt;&lt;EOF#!/bin/sh###################################################################### 此脚本在master节点使用# 设置免密登录#####################################################################pass=root123iplist=&#x27;192.168.150.11 192.168.150.12 192.168.150.13 192.168.150.14&#x27;yum -y install expectecho &quot;y&quot;|ssh-keygen -t rsa -P &quot;&quot; -f /root/.ssh/id_rsafor i in \\ $&#123;iplist&#125;;doexpect -c  &quot;spawn ssh-copy-id -i /root/.ssh/id_rsa.pub root@\\$&#123;i&#125;expect &#123;    &quot;yes/no*&quot; &#123; send &quot;yes&quot;\\r;exp_continue&#125;    &quot;password*&quot; &#123; send &quot;\\$&#123;pass&#125;&quot;\\r;exp_continue&#125;    timeout &#123; &#125;&#125;&quot;doneexit 0EOF\n\n# 设置主机名称 master及node节点设置ssh 192.168.150.11 &quot;hostnamectl set-hostname k8s-master1&quot; &amp;&amp;ssh 192.168.150.12 &quot;hostnamectl set-hostname k8s-master2&quot; &amp;&amp;ssh 192.168.150.13 &quot;hostnamectl set-hostname k8s-master3&quot;  &amp;&amp;ssh 192.168.150.14 &quot;hostnamectl set-hostname k8s-node1&quot;\n\n#  添加阿里云YUM软件源cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF\n\n\n\n2.安装相关服务器及docker# master及node节点安装# 安装docker# 1.更新yum包yum update# 2.卸载旧版本yum remove docker# 3.安装依赖yum install -y yum-utils device-mapper-persistent-data lvm2# 4.设置yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# 5.安装dockeryum -y install docker-ce docker-ce-cli containerd.io# 6.启动dockersystemctl daemon-reloadsystemctl restart dockersystemctl enable docker# 7.验证安装是否成功docker version\n\n# 方式1 替换国内源 master及node节点设置cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125;EOF# 方式2 设置cgroupdriver为systemd(推荐)cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;registry-mirrors&quot;: [&quot;https://v16stybc.mirror.aliyuncs.com&quot;],  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  # 本地仓库需要添加此项(默认80端口可不写)  &quot;insecure-registries&quot;:[&quot;192.168.150.21:81&quot;]  &#125;EOF# 重启dockersystemctl daemon-reloadsystemctl restart docker\n\n\n\n3.安装kubelet、kubeadm、kubectl# 方式1 选择版本号下载 master及node节点设置yum -y install kubeadm-1.17.3 kubelet-1.17.3 kubectl-1.17.3# 方式2 默认最新版下载yum -y install kubeadm kubelet kubectl# 方式3 选择本地rpm包安装# 设置kubelet开机自启 master及node节点设置systemctl daemon-reloadsystemctl restart kubeletsystemctl enable kubelet\n\n4.初始化kubernetes\n方式1 kubeadm安装 \n\n# master执行(单个master)kubeadm init \\--apiserver-advertise-address=192.168.150.11 \\--image-repository registry.aliyuncs.com/google_containers \\--kubernetes-version v1.17.3 \\--service-cidr=10.96.0.0/12 \\--pod-network-cidr=10.244.0.0/16 \\--apiserver-cert-extra-sans=192.168.150.11\n\n# 释： --kubernetes-version：指定kubeadm版本 --pod-network-cidr：指定pod所属网络 --service-cidr：指定service网段 --ignore-preflight-errors=Swap/all：忽略 swap/所有 报错# 注：　　因为kubeadm需要拉取必要的镜像，这些镜像需要“科学上网”；所以可以先在docker hub或其他镜像仓库拉取kube-proxy、kube-scheduler、kube-apiserver、kube-controller-manager、etcd、pause镜像；并加上 --ignore-preflight-errors=all 忽略所有报错即可。\n\n# 自定义kubeadm配置文件（多个master集群）kubeadm init --config=kubeadm-config.yamlcat &gt; kubeadm-config.yaml &lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfiguration# 版本号kubernetesVersion: v1.17.3apiServer:  #填写所有kube-apiserver节点的hostname、IP、VIP  certSANs:  - k8s-master1  - k8s-master2  - k8s-master3  - k8s-node1  - k8s-node2  - k8s-node3  - 192.168.150.21  - 192.168.150.22  - 192.168.150.23  - 192.168.150.24  - 192.168.150.25  - 192.168.150.26  # VIP地址  - 192.168.150.20   # 地址controlPlaneEndpoint: &quot;192.168.150.20:6443&quot;networking:  podSubnet: &quot;10.244.0.0/16&quot;EOF\n\n# 配置授权信息 master1执行mkdir -p\t$HOME/.kubecp -i /etc/kubernetes/admin.conf\t$HOME/.kube/configchown $(id -u):$(id -g)\t$HOME/.kube/config\n\n# 复制文件到其他节点 master1执行for host in k8s-master2 k8s-master3; do  echo &quot;---$host ---&quot;  ssh $host &quot;mkdir -p $HOME/.kube&quot;  scp /etc/kubernetes/admin.conf $host:$HOME/.kube/config  ssh $host &quot;chown $(id -u):$(id -g) $HOME/.kube/config&quot;done\n\n\n方式2 sealos安装 \n\n./sealos init --passwd root123 --master 192.168.2.30 --master 192.168.2.31 --master 192.168.2.32 --node 192.168.2.33 --node 192.168.2.34 --node 192.168.2.35 --node 192.168.2.36 --pkg-url /data/kube1.17.3.tar.gz --version v1.17.3\n\n5.查看集群状态# 验证Kubernetes集群是否安装成功kubectl  get pods -n kube-system# 查看节点是否都Readykubectl get nodes\n\n6.增加集群master与node节点# 添加masterkubeadm join 192.168.150.20:6443 --token iwn4dd.5t69f9bcntalegep \\    --discovery-token-ca-cert-hash sha256:5e1a771521879a48a51fb73f8011f0d0c6802b6a7eac8e5d36f50bab55c4cd3d \\    --control-plane\n\n# 增加集群master与node节点(初始化完成后有加入节点的信息，注意查看!!!)# 以下命令问样例：kubeadm join 192.168.150.10:6443 --token 74ru5i.6lfau1y9lcil9nr3 \\    --discovery-token-ca-cert-hash sha256:0f064460f74982d03a4219371323d905fb877b23654801d7ff2dc44f2c97e973# token 过期时：# 列出tokenkubeadm token listTOKEN                     TTL         EXPIRES                     USAGES                  DESCRIPTION                                                EXTRA GROUPSriwrv7.wnpsc5610pre0od9   19h         2020-11-10T13:33:00+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token# 若无token，则创建tokenkubeadm token create# 获取ca证书sha256编码hash值openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;# 增加control-plane命令(集群master)kubeadm join $&#123;masterUrl&#125; --token --token 获取的token值 \\   --discovery-token-ca-cert-hash sha256:获取ca证书sha256编码hash值 \\  --control-plane  # 增加node节点kubeadm join 192.168.150.10:6443 --token 获取的token值  \\\t--discovery-token-ca-cert-hash sha256:获取ca证书sha256编码hash值\n\n#!/usr/bin/env bash############################################################### 脚本名称: get_token.sh# 脚本功能: 获取加入节点命令信息################################################################ 生成tokenkubeadm token create# 获取tokenToken=$(kubeadm token list|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1)# 获取masterUrlmasterUrl=$(kubectl cluster-info|grep master |awk -F&quot;https://&quot; &#x27;&#123;print $2&#125;&#x27;)# 显示tokenecho &quot;Token=$&#123;Token&#125;&quot;if [ ! -f /etc/kubernetes/pki/ca.crt ];then  echo &quot;not find /etc/kubernetes/pki/ca.crt , Please Check!&quot;  exitfi# 生成密钥Pubkey=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;)certificate_key=$(kubeadm init phase upload-certs --upload-certs|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1)# 返回信息# 返回信息echo &quot;添加control-plane命令&quot;echo &quot;kubeadm join $&#123;masterUrl&#125; --token $&#123;Token&#125; \\\\--discovery-token-ca-cert-hash sha256:$&#123;Pubkey&#125; \\\\--control-plane --certificate-key $&#123;certificate_key&#125;&quot;   echo &quot;添加node命令&quot;  echo &quot;kubeadm join $&#123;masterUrl&#125; --token $&#123;Token&#125; \\ --discovery-token-ca-cert-hash sha256:$&#123;Pubkey&#125;&quot;exit\n\n\n\n7.配置CNI网络插件(flannel,weave,calico)# 配置flannel网络插件,可能失败多试几次kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 或者kubectl apply -f  *.yml文件\n\n# 配置weave网络插件sysctl net.bridge.bridge-nf-call-iptables=1kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#x27;\\n&#x27;)&quot;\n\n# 配置calico，需要 kubeadm init 时设置 --pod-network-cidr=192.168.0.0/16kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml kubectl apply -f http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml\n\n安装完成后权限配置，使api网关微服务正常工作\nkubectl create clusterrolebinding k8s-api-admin --clusterrole=cluster-admin --user=root --user=kubelet --group=system:serviceaccounts\n\n\n8.其他命令# 停止k8s相关服务systemctl stop kube-apiserversystemctl stop kube-schedulersystemctl stop flanneldsystemctl stop kube-controller-manager# 重启k8s相关服务systemctl restart kube-apiserversystemctl restart kube-schedulersystemctl restart flanneldsystemctl restart kube-controller-manager#更改ROLES状态kubectl label node k8s-master2 node-role.kubernetes.io/worker=worker# 重置kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/# 执行命令及scp命令for host in k8s-master1 k8s-master2 k8s-master3 k8s-node1; do  echo &quot;---$host---&quot;  ssh $host &quot;mkdir -p $HOME/.kube&quot;  scp /etc/kubernetes/admin.conf $host:/etc/kubernetes/  ssh $host &quot;chown $(id -u):$(id -g) $HOME/.kube/config&quot;done# 删除node节点kubectl delete node k8s-master2# 重启podkubectl get pod &#123;podname&#125; -n &#123;namespace&#125; -o yaml | kubectl replace --force -f -# 查看状态kubectl describe pod kuboard-59bdf4d5fb-k7d4f -n kube-system# 查看目前所有的podkubectl get pod -A# 查看目前所有的replica setkubectl get rs -A# 查看目前所有的deploymentkubectl get deployment # 查看my-nginx pod的详细状态kubectl describe po my-nginx# 查看my-nginx replica set的详细状态kubectl describe rs my-nginx# 查看my-nginx deployment的详细状态kubectl describe deployment my-nginx \n\n9.集群清理# kubeadm安装kubeadm reset -frm -rf ~/.kube/rm -rf /etc/kubernetes/rm -rf /etc/systemd/system/kubelet.service.drm -rf /etc/systemd/system/kubelet.servicerm -rf /usr/bin/kube*rm -rf /etc/cnirm -rf /opt/cnirm -rf /var/lib/etcd# sealos安装sealos clean --all -f \n\n\n10.问题排查问题1\n# 如果在运行 kubeadm init 命令时，遇到以下的警告[preflight] WARNING: ebtables not found in system path[preflight] WARNING: ethtool not found in system path那么或许在你的节点上缺失 ebtables、ethtool 或者类似的可执行文件。 你可以使用以下命令安装它们：对于 Ubuntu/Debian 用户，运行 apt install ebtables ethtool 命令对于 CentOS/Fedora 用户，运行 yum install ebtables ethtool 命令# 报错：&quot;open /run/flannel/subnet.env: no such file or directory&quot;cat &gt;/run/flannel/subnet.env &lt;&lt;EOFFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF\n\n问题2\n# 报错信息Normal   Scheduled    &lt;unknown&gt;              default-scheduler   Successfully assigned kubesphere-monitoring-system/prometheus-k8s-0 to k8s-node2  Warning  FailedMount  5m10s                  kubelet, k8s-node2  Unable to attach or mount volumes: unmounted volumes=[secret-kube-etcd-client-certs], unattached volumes=[prometheus-k8s-token-cndz7 config config-out tls-assets prometheus-k8s-db prometheus-k8s-rulefiles-0 secret-kube-etcd-client-certs]: timed out waiting for the condition  Warning  FailedMount  2m54s (x2 over 7m24s)  kubelet, k8s-node2  Unable to attach or mount volumes: unmounted volumes=[secret-kube-etcd-client-certs], unattached volumes=[prometheus-k8s-db prometheus-k8s-rulefiles-0 secret-kube-etcd-client-certs prometheus-k8s-token-cndz7 config config-out tls-assets]: timed out waiting for the condition  Warning  FailedMount  73s (x12 over 9m27s)   kubelet, k8s-node2  MountVolume.SetUp failed for volume &quot;secret-kube-etcd-client-certs&quot; : secret &quot;kube-etcd-client-certs&quot; not found`\n# 解决kubectl -n kubesphere-monitoring-system create  secret generic kube-etcd-client-certs \\ --from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt \\ --from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ --from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key\n\n问题3\n# 报错# &quot;cannot use 0.0.0.0 as the bind address for API Server&quot;\n# 解决# 发现是没有设置默认网关的原因，按照以下命令，结合自己的网段，设置默认网关即可。route add default gw 192.168.2.1\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Master也能当作Node使用的方法","url":"/20241121/Kubernetes/3ad8d6c826e0/","content":"Kubernetes之Master也能当作Node使用的方法出于安全考虑，默认配置下 Kubernetes 不会将 Pod 调度到 Master 节点\n1.让 Master 可以调度，当作 Node 使用kubectl taint node k8s-master01 node-role.kubernetes.io/master-\n如果想让 Pod 也能调度到在 Master（本样例即 k8s-master01）上，可以执行如下命令使其作为一个工作节点：执行后将输出如下信息（其中报错可忽略）\nnode/k8s-master01 untainterror: taint &quot;node-role.kubernetes.io/master:&quot; not found\n2.将 Master 恢复成 不可调度，Master Only 状态如果想禁止 Master（本样例即 k8s-master01）部署 pod，则可执行如下命令：\nkubectl taint node k8s-master01 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之NodeSelector定向调度","url":"/20241121/Kubernetes/02204bb78fb6/","content":"Kubernetes之NodeSelector定向调度Kubernetes Master上的Scheduler服务（kube-scheduler进程）负责实现Pod的调度，整个调度过程通过执行一系列复杂的算法，最终为每个Pod都计算出一个最佳的目标节点，这一过程是自动完成的，通常我们无法知道Pod最终会被调度到哪个节点上。在实际情况下，也可能需要将Pod调度到指定的一些Node上，可以通过Node的标签（Label）和Pod的nodeSelector属性相匹配，来达到上述目的。\n1.首先通过kubectl label命令给目标Node打上一些标签：\nkubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n\n\n这里，我们为k8s-node-1节点打上一个zone&#x3D;north标签，表明它是“北方”的一个节点：\n\nkubectl label nodes k8s-node1 zone=north\n\n\n上述命令行操作也可以通过修改资源定义文件的方式，并执行kubectl replace -f xxx.yaml命令来完成。\n\napiVersion: vlkind: Replicationcontrollermetadata:  name: redis-master  labels:   name: redis-masterspec:  replicas: 1  selector:   name: redis-master  template:   metadata:     labels:       name: redis-master   spec:     containers:     -name: master       image: kubeguide/redis-master       ports:       - containerPort: 6379     nodeSelector: # 节点设置       zone: north # 改成要选择的标签\n\n\n运行kubectl create -f命令创建Pod，scheduler就会将该Pod调度到拥有zone&#x3D;north标签的Node上\n\n如果我们给多个Node都定义了相同的标签（例如zone=north），则scheduler会根据调度算法从这组Node中挑选一个可用的Node进行Pod调度。通过基于Node标签的调度方式，我们可以把集群中具有不同特点的Node都贴上不同的标签，例如“role=frontend”“role=backend”“role=database”等标签，在部署应用时就可以根据应用的需求设置NodeSelector来进行指定Node范的调度。需要注意的是，如果我们指定了Pod的nodeSelector条件，且在集群中不存在包含相应标签的Node，则即使在集群中还有其他可供使用的Node，这个Pod也无法被成功调度。除了用户可以自行给Node添加标签，Kubernetes也会给Node预定义一些标签，包括：\n\nkubernetes.io&#x2F;hostname\nbeta.kubernetes.io&#x2F;os（从1.14版本开始更新为稳定版，到1.18版本删除）\nbeta.kubernetes.io&#x2F;arch（从1.14版本开始更新为稳定版，到1.18版本删除）\nkubernetes.io&#x2F;os（从1.14版本开始启用）\nkubernetes.io&#x2F;arch（从1.14版本开始启用）\n\n用户也可以使用这些系统标签进行Pod的定向调度。NodeSelector通过标签的方式，简单实现了限制Pod所在节点的方法。亲和性调度机制则极大扩展了Pod的调度能力，主要的增强功能如下。\n\n更具表达力（不仅仅是“符合全部”的简单情况）。\n可以使用软限制、优先采用等限制方式，代替之前的硬限制，这样调度器在无法满足优先需求的情况下，会退而求其次，继续运行该Pod。\n可以依据节点上正在运行的其他Pod的标签来进行限制，而非节点本身的标签。这样就可以定义一种规则来描述Pod之间的亲和或互斥关系。\n\n亲和性调度功能包括节点亲和性（NodeAffinity）和Pod亲和性（PodAffinity）两个维度的设置。节点亲和性与NodeSelector类似，增强了上述前两点优势；Pod的亲和与互斥限制则通过Pod标签而不是节点标签来实现，也就是上面第4点内容所陈述的方式，同时具有前两点提到的优点。 NodeSelector将会继续使用，随着节点亲和性越来越能够表达nodeSelector的功能，最终NodeSelector会被废弃。\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Pod的自动水平伸缩(HPA)","url":"/20241121/Kubernetes/7884bfdbe7a7/","content":"Kubernetes之Pod的自动水平伸缩(HPA)HPA（Horizontal Pod Autoscaler ）\npod的自动水平伸缩\n有了HPA，我们就不用为上面的问题而烦恼，HPA会帮我们自动完成pod的扩缩容。\n当资源需求过高时，会自动创建出pod副本；当资源需求低时，会自动收缩pod副本数。\n注意：首先必须确保集群中已经安装metrics-server的组件，否则无法获取集群内资源数据，无法进行以下操作。\n原理通过集群内的资源监控系统（metrics-server），来获取集群中资源的使用状态。\n根据CPU、内存、以及用户自定义的资源指标数据的使用量或连接数为参考依据，来制定一个临界点，一旦超出这个点，HPA就会自动创建出pod副本。\n版本通过kubectl api-versions可以看到，目前有3个版本：\nautoscaling&#x2F;v1            #(默认)只支持通过cpu为参考依据，来改变pod副本数autoscaling&#x2F;v2beta1       #支持通过cpu、内存、连接数以及用户自定义的资源指标数据为参考依据。autoscaling&#x2F;v2beta2       #同上，小的变动\nkubectl api-versions | grep autoscalautoscaling/v1autoscaling/v2beta1autoscaling/v2beta2\n\n查看使用的版本：\nkubectl explain hpaKIND:     HorizontalPodAutoscalerVERSION:  autoscaling/v1\n\n示例使用 Deployment “foo”设定，使用默认的自动伸缩策略，指定目标CPU使用率，使其Pod数量在2到10之间。\nkubectl autoscale deployment foo --min=2 --max=10\n\n使用RC“foo”设定，使其Pod的数量介于1和5之间，CPU使用率维持在80％。\nkubectl autoscale rc foo --max=5 --cpu-percent=80\n\n示例2例如：我有个deployment叫myapp现在只有一个副本数，最多只能8个副本数，当pod的cpu平均利用率超过百分之50或内存平均值超过百分之50时，pod将自动增加副本数以提供服务。SVC、Deployment资源清单：\napiVersion: v1kind: Servicemetadata:  name: svc-hpa  namespace: defaultspec:  selector:    app: myapp  type: NodePort  ##注意这里是NodePort，下面压力测试要用到。  ports:  - name: http    port: 80---apiVersion: apps/v1kind: Deploymentmetadata:  name: myapp  namespace: defaultspec:  replicas: 1  selector:    matchLabels:      app: myapp  template:    metadata:      name: myapp-demo      namespace: default      labels:        app: myapp    spec:      containers:      - name: myapp        image: ikubernetes/myapp:v1        imagePullPolicy: IfNotPresent        ports:        - name: http          containerPort: 80        resources:          requests:            cpu: 50m            memory: 50Mi          limits:            cpu: 50m            memory: 50Mi\n\nHPA资源清单如下：\napiVersion: autoscaling/v2beta1kind: HorizontalPodAutoscalermetadata:  name: myapp-hpa-v2  namespace: defaultspec:  minReplicas: 1         ##至少1个副本  maxReplicas: 8         ##最多8个副本  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: myapp  metrics:  - type: Resource    resource:      name: cpu      targetAverageUtilization: 50  ##注意此时是根据使用率，也可以根据使用量：targetAverageValue  - type: Resource    resource:      name: memory      targetAverageUtilization: 50  ##注意此时是根据使用率，也可以根据使用量：targetAverageValue\n\n使用ab工具模拟压力测试：\nab -c 1000 -n 5000000 http://192.168.1.103:31727/index.html\n\n等待数分钟后，查看hpa及pod数量：\nkubectl get hpaNAME           REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS   AGEmyapp-hpa-v2   Deployment/myapp   5%/50%, 72%/50%   1         8         2          44mkubectl get podsNAME                     READY   STATUS    RESTARTS   AGEmyapp-558db64459-pwzsd   1/1     Running   0          16mmyapp-558db64459-x9c4k   1/1     Running   0          23s\n此示例来自 Smbands\n官方参数\n\n\nName\nShorthand\nDefault\nUsage\n\n\n\nallow-missing-template-keys\n\ntrue\nIf true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats.\n\n\ncpu-percent\n\n-1\nThe target average CPU utilization (represented as a percent of requested CPU) over all the pods. If it’s not specified or negative, a default autoscaling policy will be used.\n\n\ndry-run\n\nfalse\nIf true, only print the object that would be sent, without sending it.\n\n\nfilename\nf\n[]\nFilename, directory, or URL to files identifying the resource to autoscale.\n\n\ngenerator\n\nhorizontalpodautoscaler&#x2F;v1\nThe name of the API generator to use. Currently there is only 1 generator.\n\n\ninclude-extended-apis\n\ntrue\nIf true, include definitions of new APIs via calls to the API server. [default true]\n\n\nmax\n\n-1\nThe upper limit for the number of pods that can be set by the autoscaler. Required.\n\n\nmin\n\n-1\nThe lower limit for the number of pods that can be set by the autoscaler. If it’s not specified or negative, the server will apply a default value.\n\n\nname\n\n\nThe name for the newly created object. If not specified, the name of the input resource will be used.\n\n\nno-headers\n\nfalse\nWhen using the default or custom-column output format, don’t print headers (default print headers).\n\n\noutput\no\n\nOutput format. One of: json|yaml|wide|name|custom-columns&#x3D;…|custom-columns-file&#x3D;…|go-template&#x3D;…|go-template-file&#x3D;…|jsonpath&#x3D;…|jsonpath-file&#x3D;… See custom columns [http://kubernetes.io/docs/user-guide/kubectl-overview/#custom-columns], golang template [http://golang.org/pkg/text/template/#pkg-overview] and jsonpath template [http://kubernetes.io/docs/user-guide/jsonpath].\n\n\noutput-version\n\n\nDEPRECATED: To use a specific API version, fully-qualify the resource, version, and group (for example: ‘jobs.v1.batch&#x2F;myjob’).\n\n\nrecord\n\nfalse\nRecord current kubectl command in the resource annotation. If set to false, do not record the command. If set to true, record the command. If not set, default to updating the existing annotation value only if one already exists.\n\n\nrecursive\nR\nfalse\nProcess the directory used in -f, –filename recursively. Useful when you want to manage related manifests organized within the same directory.\n\n\nsave-config\n\nfalse\nIf true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.\n\n\nshow-all\na\nfalse\nWhen printing, show all resources (default hide terminated pods.)\n\n\nshow-labels\n\nfalse\nWhen printing, show all labels as the last column (default hide labels column)\n\n\nsort-by\n\n\nIf non-empty, sort list types using this field specification. The field specification is expressed as a JSONPath expression (e.g. ‘{.metadata.name}’). The field in the API resource specified by this JSONPath expression must be an integer or a string.\n\n\ntemplate\n\n\nTemplate string or path to template file to use when -o&#x3D;go-template, -o&#x3D;go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].\n\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Taints污点与Tolerations容忍详解","url":"/20241121/Kubernetes/90e02eb7b359/","content":"Kubernetes之Taints污点与Tolerations容忍详解1.Taints污点和Tolerations容忍概述节点和Pod亲和力，是将Pod吸引到一组节点【根据拓扑域】（作为优选或硬性要求）。污点（Taints）则相反，它们允许一个节点排斥一组Pod。\n容忍（Tolerations）应用于pod，允许（但不强制要求）pod调度到具有匹配污点的节点上。\n污点（Taints）和容忍（Tolerations）共同作用，确保pods不会被调度到不适当的节点。一个或多个污点应用于节点；这标志着该节点不应该接受任何不容忍污点的Pod。\n说明：我们在平常使用中发现pod不会调度到k8s的master节点，就是因为master节点存在污点\n2.Taints污点Taints污点的组成使用kubectl taint命令可以给某个Node节点设置污点，Node被设置污点之后就和Pod之间存在一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node上已经存在的Pod驱逐出去。\n每个污点的组成如下：\n# key值=value值:影响(NoSchedule,PreferNoSchedule,NoExecute)key=value:effect\n\n每个污点有一个key和value作为污点的标签，effect描述污点的作用。当前taint effect支持如下选项：\n\nNoSchedule：表示K8S将不会把Pod调度到具有该污点的Node节点上\nPreferNoSchedule：表示K8S将尽量避免把Pod调度到具有该污点的Node节点上\nNoExecute：表示K8S将不会把Pod调度到具有该污点的Node节点上，同时会将Node上已经存在的Pod驱逐出去\n\n\n污点taint的NoExecute详解\n\ntaint 的 effect 值 NoExecute，它会影响已经在节点上运行的 pod：\n\n如果 pod 不能容忍 effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐\n如果 pod 能够容忍 effect 值为 NoExecute 的 taint，且在 toleration 定义中没有指定 tolerationSeconds，则 pod 会一直在这个节点上运行。\n如果 pod 能够容忍 effect 值为 NoExecute 的 taint，但是在toleration定义中指定了 tolerationSeconds，则表示 pod 还能在这个节点上继续运行的时间长度。\n\nTaints污点设置\n污点（Taints）查看\n\n# kubectl describe node 节点名称(node,master)kubectl describe node k8s-master# Name:               k8s-master1# Roles:              master# Labels:             beta.kubernetes.io/arch=amd64#                     beta.kubernetes.io/os=linux#                     kubernetes.io/arch=amd64#                     kubernetes.io/hostname=k8s-master1#                     kubernetes.io/os=linux#                     node-role.kubernetes.io/master=# Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock#                     node.alpha.kubernetes.io/ttl: 0#                     projectcalico.org/IPv4Address: 192.168.2.30/24#                     projectcalico.org/IPv4IPIPTunnelAddr: 100.105.225.0#                     volumes.kubernetes.io/controller-managed-attach-detach: true# CreationTimestamp:  Sun, 18 Apr 2021 13:49:56 -0400# Taints:             node-role.kubernetes.io/master:NoSchedule# *******\n\n污点（Taints）添加# kubectl taint nodes 节点名称 key=value:effectkubectl taint nodes k8s-node01 check=zhang:NoSchedule\n\n污点（Taints）删除# kubectl taint nodes 节点名称 key=value:effect-kubectl taint nodes k8s-node01 check=zhang:NoSchedule-# 或者kubectl taint nodes 节点名称 key:effect-kubectl taint nodes k8s-node01 check:NoExecute-\n\n\n实例\n\napiVersion: apps/v1Beta1kind: Deploymentmetadata:  name: nginx-deployspec:  replicas: 1    selector:      matchLabels:        app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        images: nginx:laste        ports:        - containerPort: 80           tolerations:               #containers同级    - key: &quot;key1&quot;              #能容忍的污点key      operator: &quot;Equal&quot;        #Equal等于表示key=value ， Exists不等于，表示当值不等于下面value正常      value: &quot;value1&quot;          #值      effect: &quot;NoExecute&quot;      #effect策略，见上面      tolerationSeconds: 3600  #原始的pod多久驱逐，注意只有effect: &quot;NoExecute&quot;才能设置，不然报错\n3.Tolerations容忍设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。\n但我们可以在Pod上设置容忍（Tolerations），意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。\n重要说明：\n\n其中key、value、effect要与Node上设置的taint保持一致\noperator的值为Exists时，将会忽略value；只要有key和effect就行\ntolerationSeconds：表示pod 能够容忍 effect 值为 NoExecute 的 taint；当指定了 tolerationSeconds【容忍时间】，则表示 pod 还能在这个节点上继续运行的时间长度。\n\n当不指定key值时当不指定key值和effect值时，且operator为Exists，表示容忍所有的污点【能匹配污点所有的keys，values和effects】\ntolerations:- operator: &quot;Exists&quot;\n\n\n当不指定effect值时当不指定effect值时，则能匹配污点key对应的所有effects情况\ntolerations:- key: &quot;key&quot;  operator: &quot;Exists&quot;\n\n当有多个Master存在时当有多个Master存在时，为了防止资源浪费，可以进行如下设置：\nkubectl taint nodes Node-name node-role.kubernetes.io/master=:PreferNoSchedule\n\n字段名              字段类型            描述key                 string\t        此容忍针对的污点的 key。如果为空，则匹配所有的污点，此时operator 必须为 Exists。当 key 为空，operator 为 Exists 时，表示容忍匹配所有 key / value 的污点operator\t        string\t        operator （操作符）表示 key 与 value 的关系。可选值为 Exists 和 Equal，默认为 Equal。 Exists 相当于 value 的通配符，此时 Pod 可以容忍包含该 key 的所有污点value\t            string\t        此容忍针对的污点的 value。如果 operator 为 Exists，value 应该为空，否则，value 为一个常规字符串effect\t            string\t        此容忍针对的污点的 effect。如果为空，则表示该容忍匹配污点的所有 effect。可选值为 NoSchedule、PreferNoSchedule、NoExecutetolerationSeconds\tinteger\t        TolerationSeconds 代表了容忍的持续时间，当该字段被填写时，effect 必须为 NoExecute。默认情况下，该字段为空，代表 Pod 可以一直容忍该污点（不会被驱逐）。0 以及负数将被认为是 0 （立刻驱逐）。\n\n4.多个Taints污点和多个Tolerations容忍怎么判断可以在同一个node节点上设置多个污点（Taints），在同一个pod上设置多个容忍（Tolerations）。Kubernetes处理多个污点和容忍的方式就像一个过滤器：从节点的所有污点开始，然后忽略可以被Pod容忍匹配的污点；保留其余不可忽略的污点，污点的effect对Pod具有显示效果：特别是：\n\n如果有至少一个不可忽略污点，effect为NoSchedule，那么Kubernetes将不调度Pod到该节点\n如果没有effect为NoSchedule的不可忽视污点，但有至少一个不可忽视污点，effect为PreferNoSchedule，那么Kubernetes将尽量不调度Pod到该节点\n如果有至少一个不可忽视污点，effect为NoExecute，那么Pod将被从该节点驱逐（如果Pod已经在该节点运行），并且不会被调度到该节点（如果Pod还未在该节点运行）\n\n5.污点和容忍的使用场景污点和容忍使用起来非常灵活，可以用于：\n\n避免 Pod 被调度到某些特定的节点\n从节点上驱逐本不应该在该节点运行的 Pod\n\n具体使用场景可能有:\n\n专属的节点： 如果想将一组节点专门用于特定的场景，可以为这些节点添加污点（例如 kubectl taint nodes nodename dedicated&#x3D;groupName:NoSchedule）然后向对应的 Pod 添加容忍。带有这些容忍的 Pod 将可以使用这一组专属节点，同时也可以使用集群中的其他节点。如果想进一步限制这些 Pod 只能使用这一组节点，那么应该为这一组节点添加一个标签（例如 dedicated&#x3D;groupName），并为这一组 Pod 添加 node affinity（或 node selector）以限制这些 Pod 只能调度到这一组节点上。\n带有特殊硬件的节点：集群中，如果某一组节点具备特殊的硬件（例如 GPU），此时非常有必要将那些不需要这类硬件的 Pod 从这组节点上排除掉，以便需要这类硬件的 Pod 可以得到资源。此时可以为这类节点添加污点（例如：kubectl taint nodes nodename special&#x3D;true:NoSchedule 或者 kubectl taint nodes nodename special&#x3D;true:PreferNoSchedule）并为需要这类硬件的 Pod 添加匹配的容忍。\n基于污点的驱逐 当节点出现问题时，可以使用污点以 Pod 为单位从节点上驱逐 Pod。\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之Volume的hostPath挂载","url":"/20241121/Kubernetes/15558d4f25f8/","content":"Kubernetes之Volume的hostPath挂载挂载示例：\nkind: DeploymentapiVersion: apps/v1metadata:  name: emcanalyser-dispatch  namespace: ztbiz-frassessment  labels:    app: emcanalyser-dispatch  annotations:    deployment.kubernetes.io/revision: &#x27;26&#x27;    kubesphere.io/description: emcanalyser-dispatch    kubesphere.io/maxSurgePod: &#x27;2&#x27;    kubesphere.io/minAvailablePod: &#x27;1&#x27;spec:  replicas: 1  selector:    matchLabels:      app: emcanalyser-dispatch  template:    metadata:      creationTimestamp: null      labels:        app: emcanalyser-dispatch      annotations:        kubesphere.io/containerSecrets: &#x27;&#x27;        kubesphere.io/restartedAt: &#x27;2021-04-28T17:04:42.190Z&#x27;        logging.kubesphere.io/logsidecar-config: &#x27;&#123;&#125;&#x27;    spec:      volumes:        - name: volume-0n4jhp          configMap:            name: frassessment-config            defaultMode: 420                    # volume-emc-data 定义主机内路径 #            - name: volume-emc-data          hostPath:            path: /data/emc-data            type: DirectoryOrCreate        #################################               containers:        - name: container-2b1k4t          image: &#x27;192.168.1.1:80/frassessment/emcanalyserdispatch:1.0&#x27;          ports:            - name: http-80              containerPort: 80              protocol: TCP          resources: &#123;&#125;          volumeMounts:            - name: volume-0n4jhp              mountPath: /app/appsettings.json              subPath: appsettings.json                          # volume-emc-data 定义pod内路径 #                   - name: volume-emc-data              readOnly: true              mountPath: /app/emcfile            #################################                          terminationMessagePath: /dev/termination-log          terminationMessagePolicy: File          imagePullPolicy: Always      restartPolicy: Always      terminationGracePeriodSeconds: 30      dnsPolicy: ClusterFirst      serviceAccountName: default      serviceAccount: default      securityContext: &#123;&#125;      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:              - matchExpressions:                  - key: node-role.frassessment.io/worker                    operator: In                    values:                      - calculate      schedulerName: default-scheduler      tolerations:        - key: frassessment.io/calculate          operator: Exists          effect: NoExecute  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 25%      maxSurge: 25%  revisionHistoryLimit: 10  progressDeadlineSeconds: 600   ","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之etcd集群备份恢复","url":"/20241121/Kubernetes/f9f43250d454/","content":"Kubernetes之etcd集群备份恢复安装etcdctl命令etcdctl下载链接\n下载后 上传至服务器\nwget https://github.com/etcd-io/etcd/releases/download/v$&#123;ETCD_VER&#125;/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz例如:wget https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz\n\n解压并加入环境变量\ntar -zxvf etcd-v3.4.14-linux-amd64.tar.gzcd etcd-v3.4.14-linux-amd64cp etcdctl /usr/bin\n\n验证etcdctl是否可用\netcdctl version\netcdctl version: 3.4.14API version: 3.4\n\n查看etcd高可用集群健康状态ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --write-out=table --endpoints=127.0.0.1:2379 endpoint health\n+-------------------+--------+-------------+-------+|     ENDPOINT      | HEALTH |    TOOK     | ERROR |+-------------------+--------+-------------+-------+| 192.168.1.1:2379  |   true | 29.123678ms |       || 192.168.1.2:2379  |   true | 30.169546ms |       || 192.168.1.3:2379  |   true | 30.654512ms |       |+-------------------+--------+-------------+-------+\n\n查看etcd高可用集群列表[ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --write-out=table --endpoints=127.0.0.1:2379 member list\n+------------------+---------+-------------+---------------------------+---------------------------+------------+|        ID        | STATUS  |    NAME     |        PEER ADDRS         |       CLIENT ADDRS        | IS LEARNER |+------------------+---------+-------------+---------------------------+---------------------------+------------+| 4a22b8f319402aba | started | k8s-master2 | https://192.168.1.2:2380  | https://192.168.1.2:2379  |    false   || b0e52dad330ab3ee | started | k8s-master3 | https://192.168.1.3:2380  | https://192.168.1.3:2379  |    false   || efe5d6122ccb1c7e | started | k8s-master1 | https://192.168.1.1:2380  | https://192.168.1.1:2379  |    false   |+------------------+---------+-------------+---------------------------+---------------------------+------------+\n\n备份etcd集群方式1\netcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  snapshot save 文件名称.db\n\n方式2\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --endpoints=127.0.0.1:2379 snapshot save 文件名称.db\n\n恢复etcd集群方式1\netcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  snapshot restore 文件名称.db\n\n方式2\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --endpoints=127.0.0.1:2379 snapshot restore 文件名称.db\n\n配置定时任务crontab -e\n\n添加以下定时任务(设置每天晚上11点半执行备份任务保存7天)\n或者将命令做成脚本，定时执行脚本\n30 23 * * * etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  snapshot save /opt/k8s-etcd-back/data/k8s-etcd-52-xingqi$(date +&quot;\\%w&quot;).db\n\nEnd\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之etcd集群的v2 api数据恢复","url":"/20241121/Kubernetes/1bb2763857cc/","content":"Kubernetes之etcd集群的v2 api数据恢复1.将备份文件或者之前的etcd数据解压到etcd集群的某一个节点（这里是master1）的工作目录&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;# 停止集群所有节点的etcd服务，并清空(或者重命名)工作目录/var/lib/etcd/ll /var/lib/etcd/member_bak\n\n\n\n2.使用“–force-new-cluster”参数启动 重新创建一个新的etcd服务：# 例如:# 删除了“--initial-cluster”参数，添加了“--force-new-cluster”参数[root@test-master-001 kubernetes]# /usr/local/bin/etcd \\  --data-dir=/var/lib/etcd/ \\  --name=test-master-001 \\  --advertise-client-urls=http://192.168.200.101:2381 \\  --listen-client-urls=http://192.168.200.101:2381,http://127.0.0.1:2381 \\  --initial-advertise-peer-urls=http://192.168.200.101:2382 \\  --listen-peer-urls=http://192.168.200.101:2382 \\  --initial-cluster-state=new \\  --initial-cluster-token=etcd-cluster \\  --force-new-cluster\n\n# 修改/etc/kubernetes/manifests/etcd.yaml参数spec:  containers:  - command:    - etcd    - --advertise-client-urls=https://192.168.11.51:2379    - --cert-file=/etc/kubernetes/pki/etcd/server.crt    - --client-cert-auth=true    - --data-dir=/var/lib/etcd    - --initial-advertise-peer-urls=https://192.168.11.51:2380    - --key-file=/etc/kubernetes/pki/etcd/server.key    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.11.51:2379    - --listen-metrics-urls=http://127.0.0.1:2381    - --listen-peer-urls=https://192.168.11.51:2380    - --name=master1    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt    - --peer-client-cert-auth=true    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt    - --snapshot-count=10000    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt    - --force-new-cluster # 此处为添加\n\n3.使用etcdctl member update命令修改test-master-001节点的advertised peer URLs：# 在etcd正常的节点 登录到docker容器内执行etcd命令docker ps |grep etcddocker exec -it 容器ID sh# 列出成员etcdctl --endpoints=http://192.168.200.101:2381 member listetcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  member list# 更新数据etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  member update 6f7eb0614101 http://192.168.200.101:2382# Updated member with ID 6f7eb0614101 in cluster\n\n4.使用etcdctl member add命令添加第二个etcd节点# 方式1etcdctl --endpoints=http://192.168.200.101:2381 member add test-master-002 http://192.168.200.102:2382  -w tableAdded member named test-master-002 with ID ff69528fcc000b88 to clusterETCD_NAME=&quot;test-master-002&quot;ETCD_INITIAL_CLUSTER=&quot;test-master-001=http://192.168.200.101:2382,test-master-002=http://192.168.200.102:2382&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot;\n\netcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key  --endpoints https://127.0.0.1:2379 --insecure-skip-tls-verify  -w table member list# 方式2etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --endpoints=https://192.168.11.51:2379 --insecure-skip-tls-verify member add master2 --peer-urls=&quot;https://192.168.11.52:2380&quot;\n\n# 移除成员etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --endpoints=https://192.168.11.51:2379 --insecure-skip-tls-verify member remove 29be125b3f05ac68\n\n\n\n5.同理添加第三个etcd节点：6.etcdctl member list命令看到etcd集群恢复成功至此，成功使用备份文件恢复了etcd集群v2 api的数据，可见etcd的健壮和易用。\n来自 运维有道\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之helm清除Kubernetes的软件包","url":"/20241121/Kubernetes/f513c0dfc939/","content":"Kubernetes之helm清除Kubernetes的软件包之前卸载的时候，看helm ls 看有openldap，也给删了，不知道有没有清除干净 再次安装的时候，就报错了，原因是没有卸载干净。\nhelm list# NAME          \tREVISION\tUPDATED                 \tSTATUS  \tCHART                       \tAPP VERSION                 \tNAMESPACE        # fancy-gnat    \t1       \tTue Jan 19 10:19:13 2021\tDEPLOYED\tnfs-client-provisioner-1.2.8\t3.1.0                       \tdefault          # ks-minio      \t1       \tTue Jan 19 10:39:21 2021\tDEPLOYED\tminio-2.5.16                \tRELEASE.2019-08-07T01-59-21Z\tkubesphere-system# ks-openldap   \t1       \tTue Jan 19 14:55:00 2021\tDEPLOYED\topenldap-ha-0.1.0           \t1.0                         \tkubesphere-system# ks-openpitrix \t1       \tTue Jan 19 13:05:36 2021\tDEPLOYED\topenpitrix-0.1.0            \tv0.4.8                      \topenpitrix-system# metrics-server\t1       \tTue Jan 19 10:23:55 2021\tDEPLOYED\tmetrics-server-2.5.0        \t0.3.1-0217                  \tkube-system \n用helm delete Name --purge删除\nhelm delete ks-openldap --purge\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之kubeadm init参数说明","url":"/20241121/Kubernetes/92c76d906ff7/","content":"Kubernetes之kubeadm init参数说明  --apiserver-advertise-address string   设置 apiserver 绑定的 IP.\n  --apiserver-bind-port int32            设置apiserver 监听的端口. (默认 6443)\n  --apiserver-cert-extra-sans strings    api证书中指定额外的Subject Alternative Names (SANs) 可以是IP 也可以是DNS名称。 证书是和SAN绑定的。\n  --cert-dir string                      证书存放的目录 (默认 &quot;/etc/kubernetes/pki&quot;)\n  --certificate-key string               kubeadm-cert secret 中 用于加密 control-plane 证书的key\n  --config string                        kubeadm 配置文件的路径.\n  --cri-socket string                    CRI socket 文件路径，如果为空 kubeadm 将自动发现相关的socket文件; 只有当机器中存在多个 CRI  socket 或者 存在非标准 CRI socket 时才指定.\n  --dry-run                              测试，并不真正执行;输出运行后的结果.\n  --feature-gates string                 指定启用哪些额外的feature 使用 key=value 对的形式。\n\n  -h, –help                                 帮助文档      –ignore-preflight-errors strings      忽略前置检查错误，被忽略的错误将被显示为警告. 例子: ‘IsPrivilegedUser,Swap’. Value ‘all’ ignores errors from all checks.      –image-repository string              选择拉取 control plane images 的镜像repo (default “k8s.gcr.io”)      –kubernetes-version string            选择K8S版本. (default “stable-1”)      –node-name string                     指定node的名称，默认使用 node 的 hostname.      –pod-network-cidr string              指定 pod 的网络， control plane 会自动将 网络发布到其他节点的node，让其上启动的容器使用此网络      –service-cidr string                  指定service 的IP 范围. (default “10.96.0.0&#x2F;12”)      –service-dns-domain string            指定 service 的 dns 后缀, e.g. “myorg.internal”. (default “cluster.local”)      –skip-certificate-key-print           不打印 control-plane 用于加密证书的key.      –skip-phases strings                  跳过指定的阶段（phase）      –skip-token-print                     不打印 kubeadm init 生成的 default bootstrap token      –token string                         指定 node 和control plane 之间，简历双向认证的token ，格式为 [a-z0-9]{6}.[a-z0-9]{16} - e.g. abcdef.0123456789abcdef      –token-ttl duration                   token 自动删除的时间间隔。 (e.g. 1s, 2m, 3h). 如果设置为 ‘0’, token 永不过期 (default 24h0m0s)      –upload-certs                         上传 control-plane 证书到 kubeadm-certs Secret.\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之kubeadm初始化k8s集群延长证书过期时间","url":"/20241121/Kubernetes/cf45721301a0/","content":"Kubernetes之kubeadm初始化k8s集群延长证书过期时间# 前言# kubeadm初始化k8s集群，签发的CA证书有效期默认是10年，签发的apiserver证书有效期默认是1年，#到期之后请求apiserver会报错，使用openssl命令查询相关证书是否到期。# 以下延长证书过期的方法适合kubernetes1.14、1.15、1.16、1.17、1.18版本# 查看证书有效时间openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text  |grep Not# 显示如下，通过下面可看到ca证书有效期是10年，从2020到2030年：# Not Before: Apr 22 04:09:07 2020 GMT# Not After : Apr 20 04:09:07 2030 GMTopenssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text  |grep Not# 显示如下，通过下面可看到apiserver证书有效期是1年，从2020到2021年：# Not Before: Apr 22 04:09:07 2020 GMT# Not After : Apr 22 04:09:07 2021 GMT# 延长证书过期时间# 1.把update-kubeadm-cert.sh文件上传到master1、master2、master3节点# update-kubeadm-cert.sh文件所在的github地址如下：# https://github.com/luckylucky421/kubernetes1.17.3# 把update-kubeadm-cert.sh文件clone和下载下来，拷贝到master1，master2，master3节点上# 2.在每个节点都执行如下命令# 1）给update-kubeadm-cert.sh证书授权可执行权限chmod +x update-kubeadm-cert.sh# 2）执行下面命令，修改证书过期时间，把时间延长到10年./update-kubeadm-cert.sh all# 3）在master1节点查询Pod是否正常,能查询出数据说明证书签发完成kubectl  get pods -n kube-system# 显示如下，能够看到pod信息，说明证书签发正常：# ......# calico-node-b5ks5                  1/1     Running   0          157m# calico-node-r6bfr                  1/1     Running   0          155m# calico-node-r8qzv                  1/1     Running   0          7h1m# coredns-66bff467f8-5vk2q           1/1     Running   0          7h30m# ......# 验证证书有效时间是否延长到10年openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text  |grep Not# 显示如下，通过下面可看到ca证书有效期是10年，从2020到2030年：# Not Before: Apr 22 04:09:07 2020 GMT# Not After : Apr 20 04:09:07 2030 GMT# [root@master01 ~]#openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text  |grep Not# 显示如下，通过下面可看到apiserver证书有效期是10年，从2020到2030年：# Not Before: Apr 22 11:15:53 2020 GMT# Not After : Apr 20 11:15:53 2030 GMTopenssl x509 -in /etc/kubernetes/pki/apiserver-etcd-client.crt  -noout -text  |grep Not# 显示如下，通过下面可看到etcd证书有效期是10年，从2020到2030年：# Not Before: Apr 22 11:32:24 2020 GMT# Not After : Apr 20 11:32:24 2030 GMTopenssl x509 -in /etc/kubernetes/pki/front-proxy-ca.crt  -noout -text  |grep Not# 显示如下，通过下面可看到fron-proxy证书有效期是10年，从2020到2030年：# Not Before: Apr 22 04:09:08 2020 GMT# Not After : Apr 20 04:09:08 2030 GMT","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之kubelet和docker的Cgroups驱动配置","url":"/20241121/Kubernetes/cfcdca5a076b/","content":"Kubernetes之kubelet和docker的Cgroups驱动配置cgroups 的全称是 Linux Control Groups，主要作用是限制、记录和隔离进程组（process groups）使用的物理资源（cpu、memory、IO 等）systemd是系统自带的cgroup管理器, 系统初始化就存在的, 和cgroups联系紧密,为每一个进程分配cgroups, 用它管理就行了. 如果设置成cgroupfs就存在2个cgroup控制管理器, 实验证明在资源有压力的情况下,会存在不稳定的情况.\n一、查看docker和kubelet的配置\n默认下，docker和kubelet的Cgroups驱动都是cgroupfs。\n# 1、查看docker的Cgroups驱动：docker info#　2、查看kubelet的Cgroups驱动：cat /etc/kubernetes/kubelet\n\n\n二、配置成推荐的systemd驱动\ncat /etc/docker/daemon.json &#123;  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;200m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;&#125;cat /etc/kubernetes/kubelet KUBELET_ARGS=&quot;--stderrthreshold=3 --v=0 --kubeconfig=/etc/kubernetes/admin.kubeconfig --address=10.30.0.3 --port=10250 --kube-reserved=cpu=1,memory=1Gi,ephemeral-storage=2Gi --system-reserved=cpu=1,memory=1Gi,ephemeral-storage=2Gi --eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;10% --hostname-override=10.30.0.3 --allow-privileged=true --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.1 --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --feature-gates=PersistentLocalVolumes=true,LocalStorageCapacityIsolation=true --pod-manifest-path=/etc/kubernetes/manifests --cluster-domain=internal-bigdata.com --cluster-dns=169.169.0.10 --root-dir=/data/kubelet --logtostderr=false --log-dir=/data/kubeletLogs/kubelet --cgroup-driver=systemd&quot;","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之node节点设置不可调度或者删除node节点","url":"/20241121/Kubernetes/734004e6e6d6/","content":"Kubernetes之node节点设置不可调度或者删除node节点\n在master 执行\n\n\n1.不可调度\n\nkubectl cordon k8s-node-1kubectl uncordon k8s-node-1       #取消\n\n2.驱逐已经运行的业务容器\n\nkubectl drain --ignore-daemonsets k8s-node-1  \n\n3.如果想删除node 节点，则进行这个步骤\n\nkubectl delete node k8s-node-1","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之pod创建过程","url":"/20241121/Kubernetes/1d164325b5c6/","content":"Kubernetes之pod创建过程\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之为Namespace配置CPU和内存配额","url":"/20241121/Kubernetes/9772db72d7d7/","content":"Kubernetes之为Namespace配置CPU和内存配额k8s通过RBAC将不同团队（or 项目）限制在不同的namespace下，通过resourceQuota来限制该namespace能够使用的资源。\n配额资源分为以下三种。\n\n计算资源配额：cpu，memory\n\n存储资源配置：requests.storage（真～总量），pvc，某storage class下的限制（例如针对fast类型的sc的限制）\n\n对象数量配置：cm，service，pod的个数。\n\n\n创建一个namespacekubectl create namespace test\n\n创建quota\napiVersion: v1kind: ResourceQuotametadata:  name: compute-resources  namespace: testspec:  hard:    requests.cpu: &quot;1&quot;    requests.memory: 1Gi    limits.cpu: &quot;2&quot;    limits.memory: 2Gi    pods: &quot;2&quot;  # pod数量配额    #requests.storage: 30Gi\n\n创建ResourceQuota对象\nkubectl create -f https://k8s.io/docs/tasks/administer-cluster/quota-mem-cpu.yaml --namespace=test\n\n\n\n注意:当开启了resource quota时，用户创建pod，必须指定cpu、内存的 requests or limits ，否则创建失败。resourceQuota搭配 limitRanges口感更佳：limitRange可以配置创建Pod的默认limit&#x2F;request。\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之亲和性和反亲和性详解亲和性和反亲和性概述","url":"/20241121/Kubernetes/aa25958fc052/","content":"Kubernetes之亲和性和反亲和性详解亲和性和反亲和性概述需要说明的是，node并没有anti-affinity这种东西，因为NotIn和DoesNotExist能提供类似的功能。\n\n亲和性：应用A与应用B两个应用频繁交互，所以有必要利用亲和性让两个应用的尽可能的靠近，甚至在一个node上，以减少因网络通信而带来的性能损耗。\n反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，以提高HA。\n\n目前主要的node affinity：\n\nrequiredDuringSchedulingIgnoredDuringExecution 硬策略表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。\n\nrequiredDuringSchedulingRequiredDuringExecution 硬策略表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。\n\npreferredDuringSchedulingIgnoredDuringExecution 软策略表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。\n\npreferredDuringSchedulingRequiredDuringExecution 软策略表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。\n\n\n软策略和硬策略的区分是有用处的，硬策略适用于 pod 必须运行在某种节点，否则会出现问题的情况，比如集群中节点的架构不同，而运行的服务必须依赖某种架构提供的功能；软策略不同，它适用于满不满足条件都能工作，但是满足条件更好的情况，比如服务最好运行在某个区域，减少网络传输等。这种区分是用户的具体需求决定的，并没有绝对的技术依赖。\n限制:\n\n同时指定nodeSelector and nodeAffinity，pod必须都满足\nnodeAffinity有多个nodeSelectorTerms ，pod只需满足一个\nnodeSelectorTerms多个matchExpressions ，pod必须都满足\n由于IgnoredDuringExecution，所以改变labels不会影响已经运行pod\n\n总的来说，node亲和性与nodeSelector类似，是它的扩展。\n\n下面是一个官方的示例：\n\napiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/e2e-az-name            operator: In            values:            - e2e-az1            - e2e-az2      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: gcr.io/google_containers/pause:2.0\n\n这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。\n这里的匹配逻辑是label在某个列表中，可选的操作符有： \n\nIn: label的值在某个列表中\nNotIn：label的值不在某个列表中\nExists：某个label存在\nDoesNotExist：某个label不存在\nGt：label的值大于某个值（字符串比较）\nLt：label的值小于某个值（字符串比较）\n\n如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可；如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之删除Terminating状态的命名空间","url":"/20241121/Kubernetes/14a7947affc2/","content":"Kubernetes之删除Terminating状态的命名空间1.描述使用kubectl  delete -f xxx.yaml，再次执行 kubectl apply -f xxx.yaml，提示：\nError from server (Forbidden): error when creating &quot;kubesphere-complete-setup.yaml&quot;: configmaps &quot;ks-installer&quot; is forbidden: unable to create new content in namespace kubesphere-system because it is being terminatedError from server (Forbidden): error when creating &quot;kubesphere-complete-setup.yaml&quot;: serviceaccounts &quot;ks-installer&quot; is forbidden: unable to create new content in namespace kubesphere-system because it is being terminatedError from server (Forbidden): error when creating &quot;kubesphere-complete-setup.yaml&quot;: deployments.apps &quot;ks-installer&quot; is forbidden: unable to create new content in namespace kubesphere-system because it is being terminated\n\n查看命名空间\nkubectl  get ns# NAME                STATUS        AGE# default             Active        15h# kube-node-lease     Active        15h# kube-public         Active        15h# kube-system         Active        15h# kubesphere-system   Terminating   28m\n发现kubesphere-system一直处于Terminating 状态。无法删除命名空间！！\n2.解决办法查看kubesphere-system的namespace描述\nkubectl get ns kubesphere-system  -o json &gt; kubesphere-system.json\n\n# 编辑json文件，删除spec字段的内存，因为k8s集群时需要认证的。vi kubesphere-system.json\n将&quot;spec&quot;: &#123;        &quot;finalizers&quot;: [            &quot;kubernetes&quot;        ]    &#125;,\n改成&quot;spec&quot;: &#123;  &#125;,\n\n或者\nfor NS in $(kubectl get ns 2&gt;/dev/null | grep Terminating | cut -f1 -d &#x27; &#x27;); do  kubectl get ns $NS -o json &gt; /tmp/$NS.json  sed -i &#x27;&#x27; &quot;s/\\&quot;kubernetes\\&quot;//g&quot; /tmp/$NS.json  kubectl replace --raw &quot;/api/v1/namespaces/$NS/finalize&quot; -f /tmp/$NS.jsondone\n\n\n新开一个窗口运行kubectl proxy跑一个API代理在本地的8081端口\nkubectl proxy --port=8081# Starting to serve on 127.0.0.1:8081\n最后运行curl命令进行删除\ncurl -k -H &quot;Content-Type:application/json&quot; -X PUT --data-binary @kubesphere-system.json http://127.0.0.1:8081/api/v1/namespaces/kubesphere-system/finalize\n注意：命令中的kubesphere-system就是命名空间。\n再次查看命名空间\nkubectl get ns# NAME              STATUS   AGE# default           Active   15h# kube-node-lease   Active   15h# kube-public       Active   15h# kube-system       Active   15h\n发现kubesphere-system命名空间已经消失了\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之利用NFS动态提供K8s后端存储卷","url":"/20241121/Kubernetes/e67feda0a11a/","content":"Kubernetes之利用NFS动态提供K8s后端存储卷\n本文将介绍使用nfs-client-provisioner这个应用，利用NFS Server给Kubernetes作为持久存储的后端，并且动态提供PV。前提条件是有已经安装好的NFS服务器，并且NFS服务器与Kubernetes的Slave节点都能网络连通。 所有下文用到的文件来自于git clone https://github.com/kubernetes-incubator/external-storage.git的nfs-client目录。\n\nnfs-client-provisioner\nnfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储\n\n\nPV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上）\nPV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上）\n\n安装部署\n修改deployment文件并部署 deploy&#x2F;deployment.yaml\n\n需要修改的地方只有NFS服务器所在的IP地址（10.10.10.60），以及NFS服务器共享的路径（&#x2F;ifs&#x2F;kubernetes），两处都需要修改为你实际的NFS服务器和共享目录\nkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: nfs-client-provisionerspec:  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: fuseim.pri/ifs            - name: NFS_SERVER              value: 10.10.10.60            - name: NFS_PATH              value: /ifs/kubernetes      volumes:        - name: nfs-client-root          nfs:            server: 10.10.10.60            path: /ifs/kubernetes\n\n\n修改StorageClass文件并部署 deploy&#x2F;class.yaml\n\n此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致。\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: managed-nfs-storageprovisioner: fuseim.pri/ifs\n\n\n\n授权如果您的集群启用了RBAC，或者您正在运行OpenShift，则必须授权provisioner。 如果你在非默认的“default”名称空间&#x2F;项目之外部署，可以编辑deploy&#x2F;auth&#x2F;clusterrolebinding.yaml或编辑oadm policy“指令。\n如果启用了RBAC需要执行如下的命令来授权。\nkubectl create -f deploy/auth/serviceaccount.yaml# serviceaccount &quot;nfs-client-provisioner&quot; createdkubectl create -f deploy/auth/clusterrole.yaml# clusterrole &quot;nfs-client-provisioner-runner&quot; createdkubectl create -f deploy/auth/clusterrolebinding.yaml# clusterrolebinding &quot;run-nfs-client-provisioner&quot; createdkubectl patch deployment nfs-client-provisioner -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;nfs-client-provisioner&quot;&#125;&#125;&#125;&#125;&#x27;\n\n\n测试\n测试创建PVC\n\nkubectl create -f deploy/test-claim.yaml\n\n\n测试创建POD\n\nkubectl create -f deploy/test-pod.yaml\n\n\n删除测试POD\n\nkubectl delete -f deploy/test-pod.yaml\n\n\n删除测试PVC\n\nkubectl delete -f deploy/test-claim.yaml\n\n\n在NFS服务器上的共享目录下查看NFS的PV卷回收以后是否名字以archived开头。\n实例\nNFS服务器配置\n\ncat /etc/exports#/media/docker        *(no_root_squash,rw,sync,no_subtree_check)\n\n\nnfs-deployment.yaml示例NFS服务器的地址是ubuntu-master,共享出来的路径是&#x2F;media&#x2F;docker，其他不需要修改。\n\ncat nfs-deployment.yaml\nkind: DeploymentapiVersion: extensions/v1beta1metadata:  name: nfs-client-provisionerspec:  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: fuseim.pri/ifs            - name: NFS_SERVER              value: ubuntu-master            - name: NFS_PATH              value: /media/docker      volumes:        - name: nfs-client-root          nfs:            server: ubuntu-master            path: /media/docker\n\n\nStorageClass示例可以修改Class的名字，我的改成了default。\n\ncat class.yaml\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: defaultprovisioner: fuseim.pri/ifs\n\n\n查看StorageClass\n\nkubectl get sc# NAME             PROVISIONER               AGE# default          fuseim.pri/ifs            2d\n\n\n设置这个default名字的SC为Kubernetes的默认存储后端\n\nkubectl patch storageclass default -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27;# storageclass.storage.k8s.io &quot;default&quot; patchedkubectl get sc# NAME                PROVISIONER               AGE# default (default)   fuseim.pri/ifs            2d\n\n\n\n创建PVC\n\ncat test-claim.yaml\nkind: PersistentVolumeClaimapiVersion: v1metadata:  name: test-claimspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1Mi\n\nkubectl apply -f test-claim.yaml # persistentvolumeclaim &quot;test-claim&quot; createdkubectl get pvc|grep test# test-claim                  Bound     pvc-fe3cb938-3f15-11e8-b61d-08002795cb26   1Mi        RWX            default        10skubectl get pv|grep test# pvc-fe3cb938-3f15-11e8-b61d-08002795cb26   1Mi        RWX            Delete           Bound     default/test-claim                  default                  58s\n\n\n\n启动测试POD\n\nPOD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件。\ncat test-pod.yaml\nkind: PodapiVersion: v1metadata:  name: test-podspec:  containers:  - name: test-pod    image: gcr.io/google_containers/busybox:1.24    command:      - &quot;/bin/sh&quot;    args:      - &quot;-c&quot;      - &quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1&quot;    volumeMounts:      - name: nfs-pvc        mountPath: &quot;/mnt&quot;  restartPolicy: &quot;Never&quot;  volumes:    - name: nfs-pvc      persistentVolumeClaim:        claimName: test-claim\n\n启动POD，一会儿POD就是completed状态，说明执行完毕。\nkubectl apply -f test-pod.yaml # pod &quot;test-pod&quot; createdkubectl get pod|grep test# test-pod                                                  0/1       Completed   0          40s\n\n我们去NFS共享目录查看有没有SUCCESS文件。\ncd default-test-claim-pvc-fe3cb938-3f15-11e8-b61d-08002795cb26ls# SUCCESS\n\n说明部署正常，并且可以动态分配NFS的共享卷。\n[本文来自: https://jimmysong.io/kubernetes-handbook/practice/using-nfs-for-persistent-storage.html]\n","categories":["Kubernetes"],"tags":["Kubernetes","nfs"]},{"title":"Kubernetes之各类yaml详解","url":"/20241121/Kubernetes/6cb27800cb92/","content":"Kubernetes之各类yaml详解deployment.yaml文件详解apiVersion: extensions/v1beta1   #接口版本kind: Deployment                 #接口类型metadata:  name: cango-demo               #Deployment名称  namespace: cango-prd           #命名空间  labels:    app: cango-demo              #标签spec:  replicas: 3  strategy:    rollingUpdate:  ##由于replicas为3,则整个升级,pod个数在2-4个之间      maxSurge: 1      #滚动升级时会先启动1个pod      maxUnavailable: 1 #滚动升级时允许的最大Unavailable的pod个数  template:             metadata:      labels:        app: cango-demo  #模板名称必填    sepc: #定义容器模板，该模板可以包含多个容器      containers:                                                                           - name: cango-demo                                                 #镜像名称          #镜像地址          image: swr.cn-east-2.myhuaweicloud.com/cango-prd/cango-demo:0.0.1-SNAPSHOT           command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/path/to/special-key&quot; ]    #启动命令          args:                                                                #启动参数            - &#x27;-storage.local.retention=$(STORAGE_RETENTION)&#x27;            - &#x27;-storage.local.memory-chunks=$(STORAGE_MEMORY_CHUNKS)&#x27;            - &#x27;-config.file=/etc/prometheus/prometheus.yml&#x27;            - &#x27;-alertmanager.url=http://alertmanager:9093/alertmanager&#x27;            - &#x27;-web.external-url=$(EXTERNAL_URL)&#x27;    #如果command和args均没有写，那么用Docker默认的配置。    #如果command写了，但args没有写，那么Docker默认的配置会被忽略而且仅仅执行.yaml文件的command（不带任何参数的）。    #如果command没写，但args写了，那么Docker默认配置的ENTRYPOINT的命令行会被执行，但是调用的参数是.yaml中的args。    #如果如果command和args都写了，那么Docker默认的配置被忽略，使用.yaml的配置。          imagePullPolicy: IfNotPresent  #如果不存在则拉取          livenessProbe:       #表示container是否处于live状态。如果LivenessProbe失败，LivenessProbe将会通知kubelet对应的container不健康了。随后kubelet将kill掉container，并根据RestarPolicy进行进一步的操作。默认情况下LivenessProbe在第一次检测之前初始化值为Success，如果container没有提供LivenessProbe，则也认为是Success；            httpGet:              path: /health #如果没有心跳检测接口就为/              port: 8080              scheme: HTTP            initialDelaySeconds: 60 ##启动后延时多久开始运行检测            timeoutSeconds: 5            successThreshold: 1            failureThreshold: 5          readinessProbe:            httpGet:              path: /health #如果没有心跳检测接口就为/              port: 8080              scheme: HTTP            initialDelaySeconds: 30 ##启动后延时多久开始运行检测            timeoutSeconds: 5            successThreshold: 1            failureThreshold: 5          resources:              ##CPU内存限制            requests:             #限制资源(下限)              cpu: 2              memory: 2048Mi            limits:              #限制资源(上限)              cpu: 2              memory: 2048Mi          env:                    ##通过环境变量的方式，直接传递pod=自定义Linux OS环境变量            - name: LOCAL_KEY     #本地Key              value: value            - name: CONFIG_MAP_KEY  #局策略可使用configMap的配置Key，              valueFrom:                configMapKeyRef:                  name: special-config   #configmap中找到name为special-config                  key: special.type      #找到name为special-config里data下的key          ports:            - name: http              containerPort: 8080 #对service暴露端口          volumeMounts:     #挂载volumes中定义的磁盘          - name: log-cache            mount: /tmp/log          - name: sdb       #普通用法，该卷跟随容器销毁，挂载一个目录            mountPath: /data/media              - name: nfs-client-root    #直接挂载硬盘方法，如挂载下面的nfs目录到/mnt/nfs            mountPath: /mnt/nfs          - name: example-volume-config  #高级用法第1种，将ConfigMap的log-script,backup-script分别挂载到/etc/config目录下的一个相对路径path/to/...下，如果存在同名文件，直接覆盖。            mountPath: /etc/config                 - name: rbd-pvc                #高级用法第2中，挂载PVC(PresistentVolumeClaim) #使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容，  volumes:  # 定义磁盘给上面volumeMounts挂载  - name: log-cache    emptyDir: &#123;&#125;  - name: sdb  #挂载宿主机上面的目录    hostPath:      path: /any/path/it/will/be/replaced  - name: example-volume-config  # 供ConfigMap文件内容到指定路径使用    configMap:      name: example-volume-config  #ConfigMap中名称      items:      - key: log-script           #ConfigMap中的Key        path: path/to/log-script  #指定目录下的一个相对路径path/to/log-script      - key: backup-script        #ConfigMap中的Key        path: path/to/backup-script  #指定目录下的一个相对路径path/to/backup-script  - name: nfs-client-root         #供挂载NFS存储类型    nfs:      server: 10.42.0.55          #NFS服务器地址      path: /opt/public           #showmount -e 看一下路径  - name: rbd-pvc                 #挂载PVC磁盘    persistentVolumeClaim:      claimName: rbd-pvc1         #挂载已经申请的pvc磁盘\n\nPod yaml文件详解# yaml格式的pod定义文件完整内容：apiVersion: v1       #必选，版本号，例如v1kind: Pod       #必选，Podmetadata:       #必选，元数据  name: string       #必选，Pod名称  namespace: string    #必选，Pod所属的命名空间  labels:      #自定义标签    - name: string     #自定义标签名字  annotations:       #自定义注释列表    - name: stringspec:         #必选，Pod中容器的详细定义  containers:      #必选，Pod中容器列表  - name: string     #必选，容器名称    image: string    #必选，容器的镜像名称    imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像    command: [string]    #容器的启动命令列表，如不指定，使用打包时使用的启动命令    args: [string]     #容器的启动命令参数列表    workingDir: string     #容器的工作目录    volumeMounts:    #挂载到容器内部的存储卷配置    - name: string     #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名      mountPath: string    #存储卷在容器内mount的绝对路径，应少于512字符      readOnly: boolean    #是否为只读模式    ports:       #需要暴露的端口库号列表    - name: string     #端口号名称      containerPort: int   #容器需要监听的端口号      hostPort: int    #容器所在主机需要监听的端口号，默认与Container相同      protocol: string     #端口协议，支持TCP和UDP，默认TCP    env:       #容器运行前需设置的环境变量列表    - name: string     #环境变量名称      value: string    #环境变量的值    resources:       #资源限制和请求的设置      limits:      #资源限制的设置        cpu: string    #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数        memory: string     #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数      requests:      #资源请求的设置        cpu: string    #Cpu请求，容器启动的初始可用数量        memory: string     #内存清楚，容器启动的初始可用数量    livenessProbe:     #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可      exec:      #对Pod容器内检查方式设置为exec方式        command: [string]  #exec方式需要制定的命令或脚本      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port        path: string        port: number        host: string        scheme: string        HttpHeaders:        - name: string          value: string      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式         port: number       initialDelaySeconds: 0  #容器启动完成后首次探测的时间，单位为秒       timeoutSeconds: 0   #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒       periodSeconds: 0    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次       successThreshold: 0       failureThreshold: 0       securityContext:         privileged:false    restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod    nodeSelector: obeject  #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定    imagePullSecrets:    #Pull镜像时使用的secret名称，以key：secretkey格式指定    - name: string    hostNetwork:false      #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络    volumes:       #在该pod上定义共享存储卷列表    - name: string     #共享存储卷名称 （volumes类型有很多种）      emptyDir: &#123;&#125;     #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值      hostPath: string     #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录        path: string     #Pod所在宿主机的目录，将被用于同期中mount的目录      secret:      #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部        scretname: string          items:             - key: string          path: string      configMap:     #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部        name: string        items:        - key: string          path: string\n\nService yaml文件详解apiVersion: v1kind: Servicematadata:                                #元数据  name: string                           #service的名称  namespace: string                      #命名空间    labels:                                #自定义标签属性列表    - name: string  annotations:                           #自定义注解属性列表      - name: stringspec:                                    #详细描述  selector: []                           #label selector配置，将选择具有label标签的Pod作为管理                                          #范围  type: string                           #service的类型，指定service的访问方式，默认为                                          #clusterIp  clusterIP: string                      #虚拟服务地址        sessionAffinity: string                #是否支持session  ports:                                 #service需要暴露的端口列表  - name: string                         #端口名称    protocol: string                     #端口协议，支持TCP和UDP，默认TCP    port: int                            #服务监听的端口号    targetPort: int                      #需要转发到后端Pod的端口号    nodePort: int                        #当type = NodePort时，指定映射到物理机的端口号  status:                                #当spce.type=LoadBalancer时，设置外部负载均衡器的地址    loadBalancer:                        #外部负载均衡器          ingress:                           #外部负载均衡器         ip: string                       #外部负载均衡器的Ip地址值        hostname: string                 #外部负载均衡器的主机名Apply on KubeSail\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之命令补齐","url":"/20241121/Kubernetes/a85e20a04aa8/","content":"yum安装依赖(centos)\nyum -y install bash-completion\napt安装依赖(ubuntu)\napt-get install -y bash-completion\n添加环境变量\nsource /usr/share/bash-completion/bash_completionsource &lt;(kubectl completion bash)echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc\n\n\nThe End\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之处理逻辑","url":"/20241121/Kubernetes/3f3400cf5c3c/","content":"Kubernetes之处理逻辑\n\n\nThe End\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之备份与还原","url":"/20241121/Kubernetes/22cd3d5a9a29/","content":"一.k8s模型Kubernetes 集群能支撑庞大而又复杂的应用系统，许多用户和团队共享集群，难免会相互影响或冲突。为了避免用户之间的冲突，Kubernetes 引入命名空间概念，在同一个命名空间下各种资源的不能重名，在不同的命名空间下允许重名，实现资源安全隔离。用户在分配给他的命名空间下操作，不用担心影响到别人，也不用担心受别人影响，因为每个用户或者每个团队都有独立的命名空间。\nKubernetes 拥有和保留系统级的命名空间 Kube-system 和 kube-public，未经授权不允许普通用户使用系统命名空间。默认命名空间 default 是公共的，如果没有指定命名空间，用户新建的资源都将建立在 default 命名空间。\nKubernetes 的资源类型包括：\n\n服务 service\n\n部署 deploy\n\n配置 configmap\n\n加密配置 secret\n\n任务 job\n\n定时任务 cronjob\n\n副本集 replicaset\n\n驻留任务集 daemonset\n\n有状态集 statefulset\n\n\n此处不一一列举。\nKubernetes 的资源配置告诉 Kubernetes 系统，部署（deploy）哪些应用，对外提供哪些服务（service），应用配置参数（configmap）存在哪儿，敏感参数（secret）需要加密吗， 运行一次就结束的任务（job）如何调度，像闹钟一样的定时任务（cronjob）怎么安排，一个节点运行一个且只运行一个的驻留任务（daemonset）支持吗，运行过后希望保留数据及状态的任务（statefulset）。资源配置赋予 Kubernetes 丰富的资源创建能力，能适应复杂多变的应用环境。\n同一个资源类型下，可以配置多个不同的资源实例，而同一个资源实例允许运行多个完全相同的副本，拓展了系统服务能力。例如：供客户使用的 nginx&#x2F;http 服务和供内部运营人员使用 nginx&#x2F;http 服务，可以分别配置、分别部署。供客户使用的 nginx&#x2F;http 服务可以多副本运行，并且按应用负载自适应扩展或者缩减副本数量。\n二.k8s集群层级模型如上所述，Kubernetes 系统可以归纳为多层级的分层模型，从上到下分别是：\n\nKubernete 平台\n\n命名空间 namespace\n\n资源类型 resource type\n\n资源实例 resource instance。\n\n\n上一级和下一级之间是一对多的关系，而下一级从属于唯一的上级。例如，资源类型为deploy 的应用实例 nginx-web 从属于类型 deploy，而 deploy 从属于某个命名空间 ns-cmft。命名空间可以容纳 deploy、service 和 job 等多种类型的资源。资源实例的副本（实例），是运行时的概念，可以动态创建、动态销毁。\nKubernetes 资源配置构成容器编排的身体骨架，而血肉在运行时填充进去并塑造形状。或者说，资源配置就像是一个具体 Kubernetes 运行时实例的 DNA、基因组。如果所有资源配置确定了，系统的内部结构也基本确定了。这个 Kubernetes 基因组的另一部分是容器镜像，以及容器运行时产生的数据。\n三.数据恢复模型数据恢复是从已备份的数据副本恢复到正在运行的 Kubernetes 系统。\n这里的数据副本是指此前备份导出来的 yaml 格式文本文件。Yaml 文件是人类肉眼可读的，也是可以修改的。如果管理员想恢复某个历史时点备份的资源文件，且修改某个错误值，那么从备份的 yaml 文件恢复是比较好的选择。\n数据恢复可以是全量恢复，或者部分恢复。全量恢复是从某个时刻的全量备份数据恢复至 Kubernetes 系统。部分恢复只恢复部分数据。\n相比数据备份的豪气、大方，数据恢复则要小心谨慎得多。数据恢复一般以部分恢复为主，只对发生故障，而且确认备份副本数据正确有效时才会恢复。在系统发生不可逆转的全面崩溃时，会优先考虑从物理备份恢复，只有在物理备份不可用时，此时才考虑逻辑备份（本文所说的 yaml 备份）。\n有时物理备份与逻辑备份配合使用恢复系统也是不错的选项。\n部分恢复应当把经过仔细审查通过的 yaml 文件复制到专门的恢复目录（restore），以便于按顺序批量执行，恢复系统数据。\n数据恢复应当记录详细的日志，以便事后查询、审计。\n四.数据备份与恢复脚本各个目录位置备份路径&#x2F;opt&#x2F;k8s-backup&#x2F;脚本放置路径&#x2F;opt&#x2F;k8s-backup&#x2F;bin名称空间备份路径&#x2F;opt&#x2F;k8s-backup&#x2F;日期&#x2F;namespace备份日志路径&#x2F;opt&#x2F;k8s-backup&#x2F;k8s-backup.log\n1.手动备份备份脚本默认是全量备份，也就是备份 K8s 集群下的所有命名空间下的系统资源数据。有些生产系统，命名空间比较多，运行的服务、容器和 Pod 也多，系统资源的数据量比较大， 备份时间也会比较长。优先备份是指选择一部分优先级高的重要资源数据、或者经常变化的资源数据，并提供频率更高的备份。\n同一用户下的资源数据经常配置在相同的命名空间，按命名空间来识别高优先级的资源数据是一条简便途径。本文的方法能用脚本传入参数，只备份指定命名空间的资源数据，而不会备份其他命名空间下的资源数据，可以用作优先备份。\n在命令行下输入备份脚本，默认开始全量备份：\ncd /opt/k8s-backup/bin./k8s_backup.sh\n\n在命令行输入带参数的备份脚本，开始部分备份（优先备份）：\ncd /opt/k8s-backup/bin./k8s_backup.sh test test2\n\n2.自动备份备份工作最好加入定时任务，定时由系统自动触发备份。将备份脚本加入到 Linux&#x2F;Unix 的 crontab 定时任务：crontab -e输入定时任务计划，每天凌晨 1:05 触发自动备份运行一次。\n5 1 * * * sh -x /opt/k8s-backup/k8s_backup.sh\n\n3.备份注意备份机可以是 K8s 集群内的任意一台主机，也可以是集群外的主机。只要网络互通，备份机能访问到 K8s 集群的 Kube-apiserver，备份机可以部署在任何地方。备份机需要持有 K8s 集群的数字证书，也就是 kubeconfig 文件，否则无法访问 K8s 集群，也就不能执行任何备份操作。\n备份的备份备份数据需不需要备份，要不要考虑备份数据的安全性。备份数据的备份是管理员要考虑的事项之一。如果要提高备份数据的安全性，可以拷贝一份备份数据，存放到安全的地方。\n4.数据恢复数据恢复会对 K8s 集群正常运行产生影响，需要谨慎执行。数据恢复没有采用全面恢复的策略，而是设置专用的数据恢复目录，只有恢复目录下的 yaml 文件才会被恢复。从备份目录复制 yaml 文件到恢复目录，文件名应替换为待恢复的文件。\ncd  /opt/k8s-backup/datarm -rf restorecp -r 202xXXyy restore\n\n名称空间创建(存在则无需创建)!!!不要恢复kube-system名称空间下的资源!!!\ncd /opt/k8s-backup/202xXXyy/namespacekubectl create -f 名称空间.yaml\n\n执行恢复脚本数据恢复以批处理方式进行，对数据恢复专用目录下的所有 yaml 文件逐个执行，先删除旧资源条目，再创建新的资源条目。数据恢复的次序是按照 yaml 文件名升序先后排列。\ncd /opt/k8s-backup/bin./k8s_restore.sh\n\n恢复检测数据恢复以后，最好再查询一次已恢复的资源项，验证资源项是否正确恢复了。检测的 方式可以是重新做一次备份，导出 yaml 文件，再做数据比对。命令 diff 比较两个文本文件内容，并列出两个文件的差异行。\n如果不想导出太多 yaml 文件，也可以导出单个资源项的 yaml 文件，然后与之前的文件做比对。例如：\nkubectl -n wisecloud-controller get cm test-config -o yaml\n\n\n5.脚本源文件备份脚本\n#!/usr/bin/bash## show usageecho &quot;usage: $0 [namespace]&quot;## define variableBACKUP_PATH=/opt/k8s-backupBACKUP_PATH_BIN=$BACKUP_PATH/binBACKUP_PATH_DATA=$BACKUP_PATH/data/`date +%Y%m%d%H%M%S`BACKUP_PATH_LOG=$BACKUP_PATH/logBACKUP_PATH_NS=$BACKUP_PATH/data/`date +%Y%m%d%H%M%S`/namespaceBACKUP_LOG_FILE=$BACKUP_PATH_LOG/k8s-backup.log## set K8s typeCONFIG_TYPE=&quot;service deploy configmap secret job cronjob replicaset daemonset statefulset&quot;## make dirmkdir -p $BACKUP_PATH_BINmkdir -p $BACKUP_PATH_DATAmkdir -p $BACKUP_PATH_LOGmkdir -p $BACKUP_PATH_NScd $BACKUP_PATH_DATA## set namespace listns_list=`kubectl get ns | awk &#x27;&#123;print $1&#125;&#x27; | grep -v NAME|grep -v kube-system` if [ $# -ge 1 ]; then ns_list=&quot;$@&quot;fi## define countersCOUNT0=0COUNT1=0COUNT2=0COUNT3=0## print hintecho &quot;`date` Backup kubernetes config in namespaces [&quot;echo &quot;$&#123;ns_list&#125;&quot;echo &#x27;] now.&#x27;echo &quot;`date` Backup kubernetes config for [type: $&#123;CONFIG_TYPE&#125;].&quot;echo &quot;`date` If you want to read the record of backup, please input command &#x27; tail -100f $&#123;BACKUP_LOG_FILE&#125; &#x27;&quot;## ask and answermessage=&quot;This will backup resources of kubernetes cluster to yaml files.&quot;echo $&#123;message&#125; 2&gt;&amp;1 &gt;&gt; $BACKUP_LOG_FILEecho $&#123;message&#125;read -n 1 -p &quot;Do you want to continue? [yes/no] &quot; input_char &amp;&amp; printf &quot;\\n&quot;if [ &quot;$&#123;input_char&#125;&quot; != &#x27;y&#x27;  ]; thenmessage=&quot;`date` Exit by user&#x27;s selection.&quot;echo $message 2&gt;&amp;1 &gt;&gt; $BACKUP_LOG_FILEecho $messageexit 1fi## loop for namespacesfor ns in $ns_list; do COUNT0=`expr $COUNT0 + 1` echo  &quot;`date`  Backup  No.$&#123;COUNT0&#125;  namespace  [namespace:  $&#123;ns&#125;].&quot;  2&gt;&amp;1  &gt;&gt;$BACKUP_LOG_FILE kubectl get ns $&#123;ns&#125; -o yaml &gt;&gt;$BACKUP_PATH_DATA/namespace/$&#123;ns&#125;_namespace.yaml COUNT2=0 ## loop for types for type in $CONFIG_TYPE; do echo &quot;`date` Backup type [namespace: $&#123;ns&#125;, type: $&#123;type&#125;].&quot; 2&gt;&amp;1 &gt;&gt;$BACKUP_LOG_FILE item_list=`kubectl -n $ns get $type | awk &#x27;&#123;print $1&#125;&#x27; | grep -v NAME  | grep -v &quot;No &quot;` COUNT1=0 ## loop for items for item in $item_list; do file_name=$BACKUP_PATH_DATA/$&#123;ns&#125;_$&#123;type&#125;_$&#123;item&#125;.yamlecho &quot;`date` Backup kubernetes config yaml [namespace: $&#123;ns&#125;, type: $&#123;type&#125;, item: $&#123;item&#125;] to file: $&#123;file_name&#125;&quot; 2&gt;&amp;1 &gt;&gt; $BACKUP_LOG_FILE kubectl -n $ns get $type $item -o yaml &gt; $file_name COUNT1=`expr $COUNT1 + 1` COUNT2=`expr $COUNT2 + 1` COUNT3=`expr $COUNT3 + 1`echo &quot;`date` Backup No.$COUNT3 file done.&quot;  2&gt;&amp;1 &gt;&gt; $BACKUP_LOG_FILE done;done;echo &quot;`date`  Backup $&#123;COUNT2&#125;  files done  in [namespace:  $&#123;ns&#125;].&quot;  2&gt;&amp;1  &gt;&gt;$BACKUP_LOG_FILEdone;## show statsmessage=&quot;`date` Backup $&#123;COUNT3&#125; yaml files in all.&quot;echo $&#123;message&#125;echo $&#123;message&#125; 2&gt;&amp;1 &gt;&gt; $BACKUP_LOG_FILEecho &quot;`date` kubernetes Backup completed, all done.&quot; 2&gt;&amp;1 &gt;&gt;$BACKUP_LOG_FILEexit\n\n\n\n还原脚本\n#!/usr/bin/bash## define variableBACKUP_PATH=/opt/k8s-backupBACKUP_PATH_BIN=$BACKUP_PATH/binBACKUP_PATH_DATA=$BACKUP_PATH/data/restore BACKUP_PATH_LOG=$BACKUP_PATH/logBACKUP_PATH_NS=$BACKUP_PATH_DATA/namespaceRESTORE_LOG_FILE=$BACKUP_PATH_LOG/k8s-restore.log## make dirmkdir -p $BACKUP_PATH_BINmkdir -p $BACKUP_PATH_DATAmkdir -p $BACKUP_PATH_LOGcd $BACKUP_PATH_DATA## print hintmessage=&quot;`date` Kubernetes Restore start now. All yaml files which located in path $&#123;BACKUP_PATH_DATA&#125; will be applied.&quot;echo $&#123;message&#125; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILEecho $&#123;message&#125;echo &quot;`date` If you want to read the record of restore, please input command &#x27; tail -100f $&#123;RESTORE_LOG_FILE&#125; &#x27;&quot;## list yaml filesfile_list=`ls -n $&#123;BACKUP_PATH_DATA&#125;/*.yaml | awk &#x27;&#123;print $9&#125;&#x27;`file_count=`echo $&#123;file_list&#125; | wc -w`## ask and answermessage=&quot;WARNING!!! This will create $&#123;file_count&#125; yaml files to kubernetes cluster. While same name resources will be deleted. Please consider it carefully!&quot;echo $&#123;message&#125; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILEecho $&#123;message&#125;read -n 1 -p &quot;Do you want to continue? [yes/no/show] &quot; input_char &amp;&amp; printf &quot;\\n&quot;if [ &quot;$&#123;input_char&#125;&quot; == &#x27;s&#x27;   ]; then       message=&quot;`date` Show yaml files list.&quot;       echo $message 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       echo $message\t   ls -n $&#123;BACKUP_PATH_NS&#125;/*.yaml       ls -n $&#123;BACKUP_PATH_DATA&#125;/*.yaml       exit 1elif [ &quot;$&#123;input_char&#125;&quot; != &#x27;y&#x27;   ]; then       message=&quot;`date` Exit by user&#x27;s selection.&quot;       echo $message 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       echo $message       exit 2fi## create namespacefor ns in `ls $&#123;BACKUP_PATH_NS&#125;`; do\t\tnames=`echo $&#123;ns&#125;|awk -F&#x27;_&#x27; &#x27;&#123;print$1&#125;&#x27;`\t\techo &quot;kubectl delete -f $&#123;ns&#125;&quot;  2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE\t\tcmd_delete_ns=&quot;kubectl delete -f $&#123;BACKUP_PATH_NS&#125;/$&#123;ns&#125;&quot;\t\tcmd_create_ns=&quot;kubectl create -f $&#123;BACKUP_PATH_NS&#125;/$&#123;ns&#125;&quot;\t\t## run delete\t\t#echo &quot;`date` Run shell: $&#123;cmd_delete_ns&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE\t\t#if [ $&#123;ns&#125;&#x27;x&#x27; != &#x27;kube-systemx&#x27; ];then\t\t#  $&#123;cmd_delete_ns&#125;\t\t#fi\t\t#result=&quot;failed&quot;\t\t#if [ $? -eq 0 ]; then\t\t#  result=&quot;ok&quot;\t\t#fi\t\t\t\t## run create\t\techo &quot;`date` Run shell: $&#123;cmd_create_ns&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE\t\t$&#123;cmd_create_ns&#125;\t\tresult=&quot;failed&quot;\t\tif [ $? -eq 0 ]; then\t\t  result=&quot;ok&quot;\t\tfi\t\techo &quot;`date` Create namespace $&#123;result&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE done## loop for file listCOUNT=0for file_yaml in $file_list; do       COUNT=`expr $COUNT + 1`       echo &quot;`date` Restore No.$&#123;COUNT&#125; yaml file: $&#123;file_yaml&#125;...&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       cmd_delete=&quot;kubectl delete -f $&#123;file_yaml&#125;&quot;       cmd_create=&quot;kubectl create -f $&#123;file_yaml&#125;&quot;       ## run delete       echo &quot;`date` Run shell: $&#123;cmd_delete&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       $&#123;cmd_delete&#125;       result=&quot;failed&quot;       if [ $? -eq 0 ]; then                result=&quot;ok&quot;       fi       echo &quot;`date` Delete resource $&#123;result&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       ## run create       echo &quot;`date` Run shell: $&#123;cmd_create&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE       $&#123;cmd_create&#125;       result=&quot;failed&quot;       if [ $? -eq 0 ]; then       result=&quot;ok&quot;       fi     echo &quot;`date` Create resource $&#123;result&#125;.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE done;## show statsmessage=&quot;`date` Restore $&#123;COUNT&#125; yaml files in all.&quot;echo $&#123;message&#125;echo $&#123;message&#125; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILEecho &quot;`date` Kubernetes Restore completed, all done.&quot; 2&gt;&amp;1 &gt;&gt; $RESTORE_LOG_FILE exit 0;\n\nEnd\n详细内容请到CNCF此文脚本增加了名称空间备份yaml\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之安装 Kuboard v2","url":"/20241121/Kubernetes/3b6599c83b50/","content":"安装 Kuboard v2Kuboard 是 Kubernetes 的一款图形化管理界面\n1.安装\n在线安装\n\nkubectl apply -f https://kuboard.cn/install-script/kuboard.yamlkubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml\n\n\n离线安装\n\n修改yaml文件为本地仓库地址\n2.查看 Kuboard 运行状态：kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system# NAME                       READY   STATUS        RESTARTS   AGE# kuboard-54c9c4f6cb-6lf88   1/1     Running       0          45s\n\n3.获取Token管理员用户\necho $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk &#x27;&#123;print $1&#125;&#x27;) -o go-template=&#x27;&#123;&#123;.data.token&#125;&#125;&#x27; | base64 -d)\n只读用户\necho $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-viewer | awk &#x27;&#123;print $1&#125;&#x27;) -o go-template=&#x27;&#123;&#123;.data.token&#125;&#125;&#x27; | base64 -d)\n\n4.访问Kuboard\nNodePort\nKuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567；您可以按如下方式访问 Kuboard。\n http:&#x2F;&#x2F;任意一个Worker节点的IP地址:32567&#x2F;\n\nport-forward\nkubectl port-forward service/kuboard 8080:80 -n kube-system\n 在浏览器打开链接 http://localhost:8080\n\n\n5.卸载\n在线卸载\nkubectl delete -f https://kuboard.cn/install-script/kuboard.yamlkubectl delete -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml\n\n本地化yaml卸载\nkubectl delete -f kuboard.yamlkubectl delete -f metrics-server.yaml\n\n6.yaml文件metrics-server.yaml\n---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: system:aggregated-metrics-reader  labels:    rbac.authorization.k8s.io/aggregate-to-view: &quot;true&quot;    rbac.authorization.k8s.io/aggregate-to-edit: &quot;true&quot;    rbac.authorization.k8s.io/aggregate-to-admin: &quot;true&quot;rules:- apiGroups: [&quot;metrics.k8s.io&quot;]  resources: [&quot;pods&quot;, &quot;nodes&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: metrics-server:system:auth-delegatorroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:auth-delegatorsubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: metrics-server-auth-reader  namespace: kube-systemroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: extension-apiserver-authentication-readersubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: system:metrics-serverrules:- apiGroups:  - &quot;&quot;  resources:  - pods  - nodes  - nodes/stats  - namespaces  verbs:  - get  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: system:metrics-serverroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:metrics-serversubjects:- kind: ServiceAccount  name: metrics-server  namespace: kube-system---apiVersion: apiregistration.k8s.io/v1kind: APIServicemetadata:  name: v1beta1.metrics.k8s.iospec:  service:    name: metrics-server    namespace: kube-system    port: 443  group: metrics.k8s.io  version: v1beta1  insecureSkipTLSVerify: true  groupPriorityMinimum: 100  versionPriority: 100---apiVersion: v1kind: ServiceAccountmetadata:  name: metrics-server  namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata:  name: metrics-server  namespace: kube-system  labels:    k8s-app: metrics-serverspec:  selector:    matchLabels:      k8s-app: metrics-server  template:    metadata:      name: metrics-server      labels:        k8s-app: metrics-server    spec:      serviceAccountName: metrics-server      volumes:      # mount in tmp so we can safely use from-scratch images and/or read-only containers      - name: tmp-dir        emptyDir: &#123;&#125;      hostNetwork: true      containers:      - name: metrics-server\t  \timage: eipwork/metrics-server:v0.3.7        # image: 仓库地址:版本        # command:        # - /metrics-server        # - --kubelet-insecure-tls        # - --kubelet-preferred-address-types=InternalIP         args:          - --cert-dir=/tmp          - --secure-port=4443          - --kubelet-insecure-tls=true          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,externalDNS        ports:        - name: main-port          containerPort: 4443          protocol: TCP        securityContext:          readOnlyRootFilesystem: true          runAsNonRoot: true          runAsUser: 1000        imagePullPolicy: Always        volumeMounts:        - name: tmp-dir          mountPath: /tmp      nodeSelector:        beta.kubernetes.io/os: linux---apiVersion: v1kind: Servicemetadata:  name: metrics-server  namespace: kube-system  labels:    kubernetes.io/name: &quot;Metrics-server&quot;    kubernetes.io/cluster-service: &quot;true&quot;spec:  selector:    k8s-app: metrics-server  ports:  - port: 443    protocol: TCP    targetPort: 4443\nkuboard.yaml\napiVersion: apps/v1kind: Deploymentmetadata:  name: kuboard  namespace: kube-system  annotations:    k8s.kuboard.cn/displayName: kuboard    k8s.kuboard.cn/ingress: &quot;true&quot;    k8s.kuboard.cn/service: NodePort    k8s.kuboard.cn/workload: kuboard  labels:    k8s.kuboard.cn/layer: monitor    k8s.kuboard.cn/name: kuboardspec:  replicas: 1  selector:    matchLabels:      k8s.kuboard.cn/layer: monitor      k8s.kuboard.cn/name: kuboard  template:    metadata:      labels:        k8s.kuboard.cn/layer: monitor        k8s.kuboard.cn/name: kuboard    spec:      containers:      - name: kuboard        image: eipwork/kuboard:latest\t\t# image: 镜像地址:版本        imagePullPolicy: Always      tolerations:      - key: node-role.kubernetes.io/master        effect: NoSchedule        operator: Exists---apiVersion: v1kind: Servicemetadata:  name: kuboard  namespace: kube-systemspec:  type: NodePort  ports:  - name: http    port: 80    targetPort: 80    nodePort: 32567  selector:    k8s.kuboard.cn/layer: monitor    k8s.kuboard.cn/name: kuboard---apiVersion: v1kind: ServiceAccountmetadata:  name: kuboard-user  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: kuboard-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: kuboard-user  namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata:  name: kuboard-viewer  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: kuboard-viewerroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: viewsubjects:- kind: ServiceAccount  name: kuboard-viewer  namespace: kube-system# ---# apiVersion: extensions/v1beta1# kind: Ingress# metadata:#   name: kuboard#   namespace: kube-system#   annotations:#     k8s.kuboard.cn/displayName: kuboard#     k8s.kuboard.cn/workload: kuboard#     nginx.org/websocket-services: &quot;kuboard&quot;#     nginx.com/sticky-cookie-services: &quot;serviceName=kuboard srv_id expires=1h path=/&quot;# spec:#   rules:#   - host: kuboard.yourdomain.com#     http:#       paths:#       - path: /#         backend:#           serviceName: kuboard#           servicePort: http","categories":["Kubernetes"],"tags":["Kubernetes","Kuboard"]},{"title":"Kubernetes之强制删除pod","url":"/20241121/Kubernetes/156dd32a5b2e/","content":"Kubernetes之强制删除pod\n在dashboard界面删除容器，发现无法删除。使用命令查看发现该pod一直处于terminating的状态\n\nKubernetes强制删除一直处于Terminating状态的pod。\n\n1.使用命令获取pod的名字\n\nkubectl get po -n NAMESPACE |grep Terminating\n\n2、使用kubectl中的强制删除命令\n\nkubectl delete pod podName -n NAMESPACE --force --grace-period=0","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之报错already present on machine","url":"/20241121/Kubernetes/73181d76ad9f/","content":"Kubernetes之报错already present on machinepod节点报错already present on machine，应该是数据损坏，处理思路：\n\n将其他matser或者node打上其他标签；\n将报错pod个数调度到0；\n修改pod的亲和性改为刚刚打得标签，让这个pod调度到其他节点；\n\n 现在报错： \nWarning  Failed          7m52s                  kubelet, smyk8s-h3c-master-01  Error: Error response from daemon: error creating overlay mount to /data/docker/overlay2/cb7546231a100f02b414080eb065e88769876b79e72736d1e4ec941c69b9b3a4-init/merged: no such file or directoryWarning  Failed          7m28s (x2 over 7m39s)  kubelet, smyk8s-h3c-master-01  (combined from similar events): Error: Error response from daemon: error creating overlay mount to /data/docker/overlay2/6272d607de946411c5ea81c1277345830ff49af7d94fc5ada83a683fe02e937a-init/merged: no such file or directoryNormal   Pulled          90s (x39 over 9m16s)   kubelet, smyk8s-h3c-master-01  Container image &quot;osixia/openldap:1.3.0&quot; already present on machine\n\n解决方案\n根据版主(kubesphere论坛)的指导总结下解决过程：\n1.这个pod运行只在master上的，master节点有3个，其中master1的数据被删无法启动，需要调度到master2和master3上启动，这里是对master2和master3打个自定义标签node-role.kubernetes.io&#x2F;openldap\nkubectl label node master-02 node-role.kubernetes.io/openldapkubectl label node master-03 node-role.kubernetes.io/openldap\n\n 实例关闭 \nkubectl -n kubesphere-system scale sts openldap --replicas=0\n\n 修改sts \nkubectl -n kubesphere-system edit sts openldap#在亲和性上修改为：nodeAffinity:  preferredDuringSchedulingIgnoredDuringExecution:  - preference:      matchExpressions:      - key: node-role.kubernetes.io/openldap # 修改此处        operator: In        values:        - &quot;&quot;    weight: 100#保存退出，#启动实例等待openldap服务启动完成验证分配节点是否正常kubectl -n kubesphere-system scale sts openldap --replicas=2kubectl get po -n kubesphere-system -o wide |grep open#openldap启动完成后重启account服务kubectl rollout restart deployment ks-account -n kubesphere-system\n\n 2.重置管理员密码 \n#查看account服务的pod namekubectl get po -n kubesphere-system  |grep ks-account#进入容器kubectl exec -it ks-account-5d8c49d4bc-4rz29 -n kubesphere-system sh#执行初始化账号packet=&#x27;PUT /kapis/iam.kubesphere.io/v1alpha2/users/admin HTTP/1.1\\r\\nHost: ks-account.kubesphere-system.svc:9090\\r\\nUser-Agent: curl/7.54.0\\r\\nAccept: */*\\r\\nContent-Type: application/json\\r\\nContent-Length: 105\\r\\n\\r\\n&#123;&quot;username&quot;: &quot;admin&quot;,&quot;email&quot;:&quot;admin@kubesphere.io&quot;,&quot;cluster_role&quot;: &quot;cluster-admin&quot;,&quot;password&quot;:&quot;P@88w0rd&quot;&#125;&#x27;; echo -ne $packet | nc ks-account.kubesphere-system.svc 80","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之挂载nfs到pod中","url":"/20241121/Kubernetes/c2b4c7d6da77/","content":"Kubernetes之挂载nfs到pod中用界面或者命令编辑pod的yml文件\nkubectl edit pod pod名称 -n 名称空间\n\n修改配置文件内容，示例\nkind: DeploymentapiVersion: apps/v1metadata:  name: ***  namespace: ***  labels:    app: ***  annotations:    ...spec:        spec:      volumes:        - name: demcache          nfs:            server: 192.168.11.149            path: /data5/nfs_dir/DemCache    containers:        - name: ***          image: ***          ...          volumeMounts:            - name: demcache              readOnly: true              mountPath: /DemCache  \n\n完整yaml示例\nkind: DeploymentapiVersion: apps/v1metadata:  name: ***  namespace: ***  labels:    app: ***  annotations:    deployment.kubernetes.io/revision: &#x27;134&#x27;    kubesphere.io/alias-name: ***    kubesphere.io/maxSurgePod: &#x27;2&#x27;    kubesphere.io/minAvailablePod: &#x27;1&#x27;spec:  replicas: 1  selector:    matchLabels:      app: ***  template:    metadata:      creationTimestamp: null      labels:        app: ***      annotations:        kubesphere.io/containerSecrets: &#x27;&#x27;        kubesphere.io/restartedAt: &#x27;2022-03-03T01:07:01.567Z&#x27;        logging.kubesphere.io/logsidecar-config: &#x27;&#123;&#125;&#x27;    spec:      volumes:        - name: volume-joohn4          persistentVolumeClaim:            claimName: public-disk        - name: volume-thekd1          configMap:            name: test            defaultMode: 420        - name: demcache          nfs:            server: 192.168.11.149            path: /data/nfs/DemCache      containers:        - name: container-2nvr7g          image: &#x27;192.168.11.6:80/test/test:0.0.1&#x27;          ports:            - name: http-5000              containerPort: 80              protocol: TCP          resources:            limits:              cpu: 500m              memory: 1000Mi            requests:              cpu: 10m              memory: 10Mi          volumeMounts:            - name: volume-joohn4              mountPath: /data            - name: volume-thekd1              mountPath: /app/appsettings.json              subPath: appsettings.json            - name: demcache              readOnly: true              mountPath: /DemCache          terminationMessagePath: /dev/termination-log          terminationMessagePolicy: File          imagePullPolicy: Always      restartPolicy: Always      terminationGracePeriodSeconds: 30      dnsPolicy: ClusterFirst      serviceAccountName: default      serviceAccount: default      securityContext: &#123;&#125;      affinity: &#123;&#125;      schedulerName: default-scheduler  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 25%      maxSurge: 25%  revisionHistoryLimit: 10  progressDeadlineSeconds: 600\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之更换集群IP地址","url":"/20241121/Kubernetes/8d25ac14ec4e/","content":"Kubernetes之更换集群IP地址k8s的master更换ip后，通信问题出现了问题，我们只需要通过kubeadm init phase命令，重新生成config文件和签名文件就可以了\n操作如下：\n一.更改配置文件进入到&#x2F;etc&#x2F;kubernetes&#x2F;manifests,将etcd.yaml  kube-apiserver.yaml里的ip地址替换为新的ip\ncd /etc/kubernetes/manifests vim etcd.yaml           # 修改etcd原IP为新的IP地址vim kube-apiserver.yaml # 修改api-server原IP为新的IP地址\n\n二.生成新的config文件cd /etc/kubernetesmv admin.conf admin.conf.bakkubeadm init phase kubeconfig admin --apiserver-advertise-address &lt;新的ip&gt;\n\n三.删除老证书,生成新证书cd /etc/kubernetescd pkimv apiserver.key apiserver.key.bakmv apiserver.crt apiserver.crt.bakkubeadm init phase certs apiserver  --apiserver-advertise-address &lt;新的ip&gt;\n\n四.重启dockersystemctl restart docker systemctl restart kubelet\n\n五.将配置文件config输出kubectl get nodes --kubeconfig=admin.conf  #  此时已经是通信成功了\n\n六.替换config文件# 将kubeconfig默认配置文件替换为admin.conf,这样就可以直接使用kubectl get nodes# 再将admin.conf配置到其他master或者node的机器上,就可以通过api访问这台k8s机器了。cp admin.conf /root/.kube/configscp /root/.kube/config k8s-node:/root/.kube/config\n\n完\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之更改副本数","url":"/20241121/Kubernetes/5050d2913ad6/","content":"Kubernetes之更改副本数kubectl scale 参数说明Examples:  # Scale a replicaset named &#x27;foo&#x27; to 3.  kubectl scale --replicas=3 rs/foo    # Scale a resource identified by type and name specified in &quot;foo.yaml&quot; to 3.  kubectl scale --replicas=3 -f foo.yaml    # If the deployment named mysql&#x27;s current size is 2, scale mysql to 3.  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql    # Scale multiple replication controllers.  kubectl scale --replicas=5 rc/foo rc/bar rc/baz    # Scale statefulset named &#x27;web&#x27; to 3.  kubectl scale --replicas=3 statefulset/webOptions:      --all=false: Select all resources in the namespace of the specified resource types      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats.      --current-replicas=-1: Precondition for current size. Requires that the current size of the resource match this value in order to scale.  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to set a new size  -k, --kustomize=&#x27;&#x27;: Process the kustomization directory. This flag can&#x27;t be used together with -f or -R.  -o, --output=&#x27;&#x27;: Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-file.      --record=false: Record current kubectl command in the resource annotation. If set to false, do not record the command. If set to true, record the command. If not set, default to updating the existing annotation value only if one already exists.  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory.      --replicas=0: The new desired number of replicas. Required.      --resource-version=&#x27;&#x27;: Precondition for resource version. Requires that the current resource version match this value in order to scale.  -l, --selector=&#x27;&#x27;: Selector (label query) to filter on, supports &#x27;=&#x27;, &#x27;==&#x27;, and &#x27;!=&#x27;.(e.g. -l key1=value1,key2=value2)      --template=&#x27;&#x27;: Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].      --timeout=0s: The length of time to wait before giving up on a scale operation, zero means don&#x27;t wait. Any other values should contain a corresponding time unit (e.g. 1s, 2m, 3h).Usage:  kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) [options]Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).\n\n\n\nkubectl scale sts与kubectl scale deploy deploy 为无状态的集群 sts   为有状态的集群,像redis,kafka等\n\n\n\nkubectl -n kubesphere-system scale sts redis-ha-server --replicas=0kubectl -n kubesphere-system scale deploy redis-ha-haproxy --replicas=0kubectl -n kubesphere-system scale sts redis-ha-server --replicas=3kubectl -n kubesphere-system scale deploy redis-ha-haproxy --replicas=3#集群状态kubectl -n kubesphere-system exec -it redis-ha-server-1 redis-cli info replication#redis正常后kubectl -n kubesphere-system scale deploy ks-apigateway --replicas=0kubectl -n kubesphere-system scale deploy ks-apigateway --replicas=1kubectl -n kubesphere-system rollout restart deploy ks-accountkubectl -n kubesphere-system scale deploy ks-apigateway --replicas=3kubectl -n kubesphere-system scale deploy ks-apigateway --replicas=3","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之给节点打标签","url":"/20241121/Kubernetes/4bc9d8911810/","content":"Kubernetes之给节点打标签你可以约束一个 Pod 只能在特定的 Node(s) 上运行，或者优先运行在特定的节点上。有几种方法可以实现这点，推荐的方法都是用标签选择器(nodeSelector)来进行选择。通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 pod 分散到节点上，而不是将 pod 放置在可用资源不足的节点上等等），但在某些情况下，你可以需要更多控制 pod 停靠的节点，例如，确保 pod 最终落在连接了 SSD 的机器上，或者将来自两个不同的服务且有大量通信的 pod 放置在同一个可用区。\n给节点打上标签：\nkubectl label nodes &lt;node_name&gt; key1=val1 key2=val2kubectl label nodes 10.2.2.123 key1=val1kubectl label nodes 10.2.2.123 key2=val2kubectl label nodes 10.2.2.123 key1=val1 key2=val2\n\n删除节点某个标签：\nkubectl label nodes &lt;node_name&gt; key1- key2-kubectl label nodes 10.2.2.123 key1-\n\n查询节点已有的标签：\nkubectl get node --show-labels=true# NAME         STATUS                     ROLES                  AGE    VERSION   LABELS# 10.2.2.120   Ready,SchedulingDisabled   master                 4d1h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=10.2.2.120,kubernetes.io/role=master# 10.2.2.121   Ready                      metallb-speaker,node   4d1h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=10.2.2.121,kubernetes.io/role=node,node-role.kubernetes.io/metallb-speaker=true# 10.2.2.122   Ready                      metallb-speaker,node   4d1h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=10.2.2.122,kubernetes.io/role=node,node-role.kubernetes.io/metallb-speaker=true# 10.2.2.123   Ready                      node                   45m    v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,key1=val1,key2=val2,kubernetes.io/hostname=10.2.2.123,kubernetes.io/role=node\n\n把pod部署到指定标签（key1&#x3D;val1 key2&#x3D;val2）的节点：\napiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: nginxspec:  replicas: 1  template:    metadata:      labels:        app: nginx    spec:      nodeSelector:        key1: val1        key2: val2      containers:      - name: nginx        image: nginx:latest        ports:\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之节点删除后重新加入异常处理","url":"/20241121/Kubernetes/6fd8d48fca14/","content":"Kubernetes之节点删除后重新加入异常处理\n删除节点重新加入报错： \n\nerror execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://192.168.123.21:2379 with maintenance client: context deadline exceedederror execution phase check-etcd: error syncing endpoints with etc: dial tcp 172.31.182.152:2379: connect: connection refused\n\n\n  解决方法： \n\n1.在kubeadm-config删除的状态不存在的etcd节点：kubectl edit configmaps -n kube-system kubeadm-config# 删除apiEndpoints下不存在的节点:(本例为master1)    apiEndpoints:      master1: # 删掉        advertiseAddress: 172.16.11.10 # 删掉        bindPort: 6443 # 删掉      master2:        advertiseAddress: 172.16.11.14        bindPort: 6443      master3:        advertiseAddress: 172.16.11.15        bindPort: 6443\n\n# Please edit the object below. Lines beginning with a &#x27;#&#x27; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1data:  ClusterConfiguration: |    apiServer:      certSANs:      - 127.0.0.1      - apiserver.cluster.local      - 172.16.11.10      - 10.103.97.2      extraArgs:        authorization-mode: Node,RBAC        feature-gates: TTLAfterFinished=true      extraVolumes:      - hostPath: /etc/localtime        mountPath: /etc/localtime        name: localtime        pathType: File        readOnly: true      timeoutForControlPlane: 4m0s    apiVersion: kubeadm.k8s.io/v1beta2    certificatesDir: /etc/kubernetes/pki    clusterName: kubernetes    controlPlaneEndpoint: apiserver.cluster.local:6443    controllerManager:      extraArgs:        experimental-cluster-signing-duration: 876000h        feature-gates: TTLAfterFinished=true      extraVolumes:      - hostPath: /etc/localtime        mountPath: /etc/localtime        name: localtime        pathType: File        readOnly: true    dns:      type: CoreDNS    etcd:      local:        dataDir: /var/lib/etcd    imageRepository: k8s.gcr.io    kind: ClusterConfiguration    kubernetesVersion: v1.17.3    networking:      dnsDomain: cluster.local      podSubnet: 100.64.0.0/10      serviceSubnet: 10.96.0.0/12    scheduler:      extraArgs:        feature-gates: TTLAfterFinished=true      extraVolumes:      - hostPath: /etc/localtime        mountPath: /etc/localtime        name: localtime        pathType: File        readOnly: true  ClusterStatus: |    apiEndpoints:      master1:        advertiseAddress: 172.16.11.10        bindPort: 6443      master2:        advertiseAddress: 172.16.11.14        bindPort: 6443      master3:        advertiseAddress: 172.16.11.15        bindPort: 6443    apiVersion: kubeadm.k8s.io/v1beta2    kind: ClusterStatuskind: ConfigMapmetadata:  creationTimestamp: &quot;2022-02-12T09:09:18Z&quot;  name: kubeadm-config  namespace: kube-system  resourceVersion: &quot;1308389&quot;  selfLink: /api/v1/namespaces/kube-system/configmaps/kubeadm-config  uid: 6a9e5249-af69-4e01-9231-945e1a236a42\n\n2.删除etcd集群内的成员 因为我是用kubeadm搭建的集群，所有etcd在每个master节点都会以pod的形式存在一个，etcd是在每个控制平面都启动一个实例的，当删除k8s-001节点时，etcd集群未自动删除此节点上的etcd成员，因此需要手动删除。注意这里首先要进入etcd的pod。 \nkubectl exec -it etcd-master1 sh -n kube-system\n\n容器内执行\nexport ETCDCTL_API=3alias etcdctl=&#x27;etcdctl --endpoints=https://172.31.182.153:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key&#x27;/ # etcdctl member listceb6b1f4369e9ecc, started, cn-hongkong.i-j6caps6av1mtyxyofmrx, https://172.31.182.154:2380, https://172.31.182.154:2379d4322ce19cc3f8da, started, cn-hongkong.i-j6caps6av1mtyxyofmrw, https://172.31.182.152:2380, https://172.31.182.152:2379d598f7eabefcc101, started, cn-hongkong.i-j6caps6av1mtyxyofmry, https://172.31.182.153:2380, https://172.31.182.153:2379#删除不存在的节点/ # etcdctl member remove d4322ce19cc3f8daMember d4322ce19cc3f8da removed from cluster ed812b9f85d5bcd7/ # etcdctl member listceb6b1f4369e9ecc, started, cn-hongkong.i-j6caps6av1mtyxyofmrx, https://172.31.182.154:2380, https://172.31.182.154:2379d598f7eabefcc101, started, cn-hongkong.i-j6caps6av1mtyxyofmry, https://172.31.182.153:2380, https://172.31.182.153:2379/ # etcdctl member listcd4e1e075b1904b2, started, cn-hongkong.i-j6caps6av1mtyxyofmrw, https://172.31.182.152:2380, https://172.31.182.152:2379ceb6b1f4369e9ecc, started, cn-hongkong.i-j6caps6av1mtyxyofmrx, https://172.31.182.154:2380, https://172.31.182.154:2379d598f7eabefcc101, started, cn-hongkong.i-j6caps6av1mtyxyofmry, https://172.31.182.153:2380, https://172.31.182.153:2379/ # exit\n\n 最后每次kubeadm join失败后要kubeadm reset重置节点，在kubeadm join才会成功。\n\njoin加入后报错\n\nerror execution phase control-plane-prepare/download-certs\n\n控制平面认证的certs已过期，默认时间两个小时，需要重新生成上传\n在已存在的控制平面运行：\nkubeadm init phase upload-certs --upload-certs\n\n 生成的替换–certificate-key的值 \n\n例子：\n\nkubeadm join 172.31.182.153:6443 --token vauo7d.d40khbya379q7bk4 --discovery-token-ca-cert-hash sha256:139ff25e1af59d940089f85614bd02066dfbe6bee937b087f0cc7896e24d8e54 --control-plane --certificate-key 2d0f05294f03306f7867c27b11c2d73c5ebef4413a8369e5cc03bf9abe53b836\n\n\n有跳过的步骤可在–ignore-preflight-errors加入跳过的名称\n\nkubeadm join 192.168.11.52:6443 --token vauo7d.d40khbya379q7bk4 --discovery-token-ca-cert-hash sha256:139ff25e1af59d940089f85614bd02066dfbe6bee937b087f0cc7896e24d8e54 --control-plane --certificate-key 2d0f05294f03306f7867c27b11c2d73c5ebef4413a8369e5cc03bf9abe53b836 --ignore-preflight-errors all\n\n\n\n\n\n\n\nThe END\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"kubernetes之设置默认存储","url":"/20241121/Kubernetes/6405977cf4c9/","content":"Kubernetes之设置默认存储# 设置默认存储kubectl patch storageclass nfs-client -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27;\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之轻量化K3s安装","url":"/20241121/Kubernetes/c15f7b87f1f0/","content":"Kubernetes之轻量化K3s安装K3s 是轻量级的 Kubernetes。K3s 易于安装，仅需要 Kubernetes 内存的一半，所有组件都在一个小于 100 MB 的二进制文件中。\n它适用于：\n\nEdge\nIoT\nCI\nDevelopment\nARM\n嵌入 K8s\n无法获得 K8s 集群 PhD 的情况\n\n什么是 K3s？K3s 是一个完全兼容的 Kubernetes 发行版，具有以下增强功能：\n\n打包为单个二进制文件。\n使用基于 sqlite3 作为默认存储机制的轻量级存储后端。同时支持使用 etcd3、MySQL 和 Postgres。\n封装在简单的启动程序中，可以处理很多复杂的 TLS 和选项。\n默认情况下是安全的，对轻量级环境有合理的默认值。\n添加了简单但强大的 batteries-included 功能，例如：1.本地存储提供程序2.service load balancer3.Helm controller4.Traefik ingress controller\n所有 Kubernetes control plane 组件的操作都封装在单个二进制文件和进程中。因此，K3s 支持自动化和管理复杂的集群操作（例如证书分发等）。\n最大程度减轻了外部依赖性，K3s 仅需要现代内核和 cgroup 挂载。K3s 打包了所需的依赖，包括：1.containerd2.Flannel (CNI)3.CoreDNS4.Traefik (Ingress)5.Klipper-lb (Service LB)6.嵌入式网络策略控制器7.嵌入式 local-path-provisioner8.主机实用程序（iptables、socat 等）\n\n在 K3s v1.19.1 中，嵌入式 etcd 取代了实验性的 Dqlite。这是一个突破性的变化。请注意，不支持从实验性 Dqlite 升级到嵌入式 etcd。要在这种模式下运行 K3s，你必须拥有奇数个 Server 节点。我们建议从三个节点开始。\n一.安装docker或者containerdk3s默认安装containerd，可以不安装docker\n二.离线下载文件从 Releases 页面获取要运行的 K3s 版本的镜像 tar 文件\nhttps://github.com/k3s-io/k3s/releases/tag/v1.25.7%2Bk3s1版本v1.25.7+k3s1\n将二进制文件放在&#x2F;usr&#x2F;local&#x2F;bin下命名为k3s（所有服务器都要放）并添加执行权限\ncp k3s /usr/local/bin/k3schmod 777 /usr/local/bin/k3s\n\n三.镜像导入本地将 tar 文件放在 images 目录下(k3s会自动导入)\nmkdir -p /var/lib/rancher/k3s/agent/images/cp  k3s-airgap-images-amd64.tar  /var/lib/rancher/k3s/agent/images/\n\n四.安装k3s单机版1.获取安装脚本\nhttps://rancher-mirror.rancher.cn/k3s/k3s-install.sh 或者 https://get.k3s.io\n\n\n2.命名为k3s-install.sh并传入master上\nchmod 777 k3s-install.sh\n\n3.在主节点执行初始化命令\n基于containerd版\nINSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC=&#x27;server --token=SECRET&#x27; ./k3s-install.sh --cluster-init --disable traefik\n基于docker版\nINSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC=&#x27;server --token=SECRET&#x27; ./k3s-install.sh --cluster-init --disable traefik  --docker\n五.后期添加mater与agent节点1.引擎为containerd无需提前安装，引擎为docker时需要将其他节点安装docker\n2.将 tar 文件放在 images 目录下(k3s会自动导入)\nmkdir -p /var/lib/rancher/k3s/agent/images/cp  k3s-airgap-images-amd64.tar  /var/lib/rancher/k3s/agent/images/\n3.从 Releases 页面获取要运行的 K3s 版本的镜像 tar 文件\nhttps://github.com/k3s-io/k3s/releases/tag/v1.25.7%2Bk3s1版本v1.25.7+k3s1\n将二进制文件放在&#x2F;usr&#x2F;local&#x2F;bin下命名为k3s（所有服务器都要放）并添加执行权限\nchmod 777 /usr/local/bin/k3s\n4.找到查看token文件\ncat /var/lib/rancher/k3s/server/token\n\n5.执行添加master主节点命令(K3S_TOKEN更换为本次的token)\nINSTALL_K3S_SKIP_DOWNLOAD=true  K3S_TOKEN=K108b9c5bdcce1ea3f50b0d97f424737f4577abdf881b770f415fcfe819c00f6f4d::server:SECRET  ./k3s-install.sh  -s  -  server --server https://master1:6443\n\n\n6.执行添加agent从节点命令\nINSTALL_K3S_SKIP_DOWNLOAD=true  K3S_TOKEN=K108b9c5bdcce1ea3f50b0d97f424737f4577abdf881b770f415fcfe819c00f6f4d::server:SECRET  ./k3s-install.sh  -s  -  agent --server https://master1:6443\n\n\n等待所有节点Running状态时k3s添加节点成功安装成功。\n六.配置harbor镜像仓库创建配置文件 (所有master与agent都创建)vim &#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;registries.yaml\nmirrors:  &quot;192.168.11.6:8083&quot;:              # 仓库名称    endpoint:      - &quot;http://192.168.11.6:8083&quot;  # 仓库地址configs:  &quot;192.168.11.6:8083&quot;:    auth:      username: admin               # 这是私有镜像仓库的用户名      password: 12345678            # 这是私有镜像仓库的密码    tls:      insecure_skip_verify: true\n\n重启k3s服务\nsystemctl restart k3s\n\n拉取测试\ncrictl pull 192.168.11.6:8083/big-website/test:0.0.1\n安装可视化工具建议安装kuboard\n来自K3s\n\nThe End \n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之面试题汇总","url":"/20241121/Kubernetes/4f1530aff745/","content":"Kubernetes之面试题汇总\n\n\nThe End\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes介绍","url":"/20250409/Kubernetes/49c62625ea33/","content":"\nKubernetes\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes学习笔记","url":"/20241121/Kubernetes/f8f40dda76f7/","content":"Kubernetes学习笔记\n文件过大 请下载到本地后观看\n\nKubernetes学习笔记\n\n\n\nThe End\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes是什么","url":"/20241121/Kubernetes/4137eee5ef23/","content":"Kubernetes是什么\nKubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。\n\n通过Kubernetes你可以：\n\n快速部署应用\n快速扩展应用\n无缝对接新的应用功能\n节省资源，优化硬件资源的使用\n\n我们的目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。\nKubernetes 特点:\n可移植: 支持公有云，私有云，混合云，多重云（multi-cloud）\n可扩展: 模块化, 插件化, 可挂载, 可组合\n自动化: 自动部署，自动重启，自动复制，自动伸缩&#x2F;扩展\n\nKubernetes是Google 2014年创建管理的，是Google 10多年大规模容器管理技术Borg的开源版本。\n为什么要使用容器?传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新&#x2F;回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。\n新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统解耦的，所以它能在不同云、不同版本操作系统间进行迁移。\n容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间成一对一关系也使容器有更大优势，使用容器可以在build或release 的阶段，为应用创建容器镜像，因为每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境基础结构，这使得从研发到测试、生产能提供一致环境。类似地，容器比虚机轻量、更“透明”，这更便于监控和管理\n容器优势总结：\n快速创建&#x2F;部署应用：与VM虚拟机相比，容器镜像的创建更加容易。\n持续开发、集成和部署：提供可靠且频繁的容器镜像构建&#x2F;部署，并使用快速和简单的回滚(由于镜像不可变性)。\n开发和运行相分离：在build或者release阶段创建容器镜像，使得应用和基础设施解耦。\n开发，测试和生产环境一致性：在本地或外网（生产环境）运行的一致性。\n云平台或其他操作系统：可以在 Ubuntu、RHEL、 CoreOS、on-prem、Google Container Engine或其它任何环境中运行。\nLoosely coupled，分布式，弹性，微服务化：应用程序分为更小的、独立的部件，可以动态部署和管理。\n资源隔离\n资源利用：更高效\n\nKubernetes能做什么？可以在物理或虚拟机的Kubernetes集群上运行容器化应用，Kubernetes能提供一个以“容器为中心的基础架构”，满足在生产环境中运行应用的一些常见需求，如：\n\n多个进程（作为容器运行）协同工作。（Pod）\n存储系统挂载\nDistributing secrets\n应用健康检测\n应用实例的复制\nPod自动伸缩&#x2F;扩展\nNaming and discovering\n负载均衡\n滚动更新\n资源监控\n日志访问\n调试应用程序\n提供认证和授权\n\nKubernetes不是Kubernetes并不是传统的PaaS（平台即服务）系统。\n\nKubernetes不限制支持应用的类型，不限制应用框架。不限制受支持的语言runtimes (例如, Java, Python, Ruby)，满足12-factor applications 。不区分 “apps” 或者“services”。 Kubernetes支持不同负载应用，包括有状态、无状态、数据处理类型的应用。只要这个应用可以在容器里运行，那么就能很好的运行在Kubernetes上。\nKubernetes不提供中间件（如message buses）、数据处理框架（如Spark）、数据库(如Mysql)或者集群存储系统(如Ceph)作为内置服务。但这些应用都可以运行在Kubernetes上面。\nKubernetes不部署源码不编译应用。持续集成的 (CI)工作流方面，不同的用户有不同的需求和偏好的区域，因此，我们提供分层的 CI工作流，但并不定义它应该如何工作。\nKubernetes允许用户选择自己的日志、监控和报警系统。\nKubernetes不提供或授权一个全面的应用程序配置 语言&#x2F;系统（例如，jsonnet）。\nKubernetes不提供任何机器配置、维护、管理或者自修复系统。\n\n另一方面，大量的Paas系统都可以运行在Kubernetes上，比如Openshift、Deis、Gondor。可以构建自己的Paas平台，与自己选择的CI系统集成。\n由于Kubernetes运行在应用级别而不是硬件级，因此提供了普通的Paas平台提供的一些通用功能，比如部署，扩展，负载均衡，日志，监控等。这些默认功能是可选的。\n另外，Kubernetes不仅仅是一个“编排系统”；它消除了编排的需要。“编排”的定义是指执行一个预定的工作流：先执行A，之B，然C。相反，Kubernetes由一组独立的可组合控制进程组成。怎么样从A到C并不重要，达到目的就好。当然集中控制也是必不可少，方法更像排舞的过程。这使得系统更加易用、强大、弹性和可扩展。\nKubernetes是什么意思？K8S由来？Kubernetes的名字来自希腊语，意思是“舵手” 或 “领航员”。K8s是将8个字母“ubernete”替换为“8”的缩写。\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes 组件","url":"/20241121/Kubernetes/86e8739dd6ef/","content":"Kubernetes 组件一.Master 组件Master组件提供集群的管理控制中心。\n\nMaster组件可以在集群中任何节点上运行。但是为了简单起见，通常在一台VM&#x2F;机器上启动所有Master组件，并且不会在此VM&#x2F;机器上运行用户容器。请参考 构建高可用群集以来构建multi-master-VM。\n\n1.kube-apiserver\nkube-apiserver用于暴露Kubernetes API。任何的资源请求&#x2F;调用操作都是通过kube-apiserver提供的接口进行。请参阅构建高可用群集。\n\n2.ETCD\netcd是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划。\n\n3.kube-controller-manager\nkube-controller-manager运行管理控制器，它们是集群中处理常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成单个二进制文件，并在单个进程中运行。\n\n这些控制器包括：\n\n节点（Node）控制器。\n副本（Replication）控制器：负责维护系统中每个副本中的pod。\n端点（Endpoints）控制器：填充Endpoints对象（即连接Services＆Pods）。\nService Account和Token控制器：为新的Namespace 创建默认帐户访问API Token。\n\n4.cloud-controller-manager\n云控制器管理器负责与底层云提供商的平台交互。云控制器管理器是Kubernetes版本1.6中引入的，目前还是Alpha的功能。云控制器管理器仅运行云提供商特定的（controller loops）控制器循环。可以通过将–cloud-provider flag设置为external启动kube-controller-manager ，来禁用控制器循环。\n\ncloud-controller-manager 具体功能：\n\n节点（Node）控制器\n路由（Route）控制器\nService控制器\n卷（Volume）控制器\n\n5.kube-scheduler\nkube-scheduler 监视新创建没有分配到Node的Pod，为Pod选择一个Node。\n\n6.插件 addons\n插件（addon）是实现集群pod和Services功能的 。Pod由Deployments，ReplicationController等进行管理。Namespace 插件对象是在kube-system Namespace中创建。\n\n7.DNS\n虽然不严格要求使用插件，但Kubernetes集群都应该具有集群 DNS。群集 DNS是一个DNS服务器，能够为 Kubernetes services提供 DNS记录。由Kubernetes启动的容器自动将这个DNS服务器包含在他们的DNS searches中。\n\n8.用户界面\nkube-ui提供集群状态基础信息查看。更多详细信息，请参阅使用HTTP代理访问Kubernetes API\n\n9.容器资源监测\n容器资源监控提供一个UI浏览监控数据。\n\n10.Cluster-level Logging\nCluster-level logging，负责保存容器日志，搜索&#x2F;查看日志。\n\n二.节点（Node）组件\n节点组件运行在Node，提供Kubernetes运行时环境，以及维护Pod。\n\n1.kubelet\nkubelet是主要的节点代理，它会监视已分配给节点的pod。\n\n具体功能：\n\n安装Pod所需的volume。\n下载Pod的Secrets。\nPod中运行的 docker（或experimentally，rkt）容器。\n定期执行容器健康检查。\nReports the status of the pod back to the rest of the system, by creating a mirror pod if necessary.\nReports the status of the node back to the rest of the system.\n\n2.kube-proxy\nkube-proxy通过在主机上维护网络规则并执行连接转发来实现Kubernetes服务抽象。\n\n3.docker\ndocker用于运行容器。\n\n4.RKT\nrkt运行容器，作为docker工具的替代方案。\n\n5.supervisord\nsupervisord是一个轻量级的监控系统，用于保障kubelet和docker运行。\n\n6.fluentd\nfluentd是一个守护进程，可提供cluster-level logging.\n\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes部署NFS持久存储（静态和动态）","url":"/20241121/Kubernetes/0d951ad8aa14/","content":"Kubernetes部署NFS持久存储（静态和动态）参考文章 https://www.jianshu.com/p/5e565a8049fc\nNFS简介\nNFS是网络文件系统Network File System的缩写，NFS服务器可以让PC将网络中的NFS服务器共享的目录挂载到本地的文件系统中，而在本地的系统中来看，那个远程主机的目录就好像是自己的一个磁盘分区一样。\nkubernetes使用NFS共享存储有两种方式：\n\n1.手动方式静态创建所需要的PV和PVC。\n2.通过创建PVC动态地创建对应PV，无需手动创建PV。\n\n1.k8s集群前提准备在master节点上部署NFS服务器\n# master节点安装nfsyum -y install nfs-utils# 创建nfs目录mkdir -p /nfs/data/# 修改权限chmod -R 777 /nfs/data# 编辑export文件,这个文件就是nfs默认的配置文件vim /etc/exports/nfs/data *(rw,no_root_squash,sync)# 配置生效exportfs -r# 查看生效exportfs# 启动rpcbind、nfs服务systemctl restart rpcbindsystemctl enable rpcbindsystemctl restart nfssystemctl enable nfs# 查看 RPC 服务的注册状况rpcinfo -p localhost# showmount测试showmount -e 192.168.88.111\n所有node节点安装客户端，开机启动\nyum -y install nfs-utilssystemctl start nfs &amp;&amp; systemctl enable nfs\n\n2.静态申请PV卷添加pv卷对应目录,这里创建2个pv卷，则添加2个pv卷的目录作为挂载点。\n# 创建pv卷对应的目录mkdir -p /nfs/data/pv001# 配置exportrs(我觉得可以不用这步，因为父目录/nfs/data，已经设为共享文件夹)vim /etc/exports/nfs/data *(rw,no_root_squash,sync)/nfs/data/pv001 *(rw,no_root_squash,sync)# 配置生效exportfs -r# 重启rpcbind、nfs服务systemctl restart rpcbind &amp;&amp; systemctl restart nfs\n创建PV下面创建名为pv001的PV卷，配置文件 nfs-pv001.yaml 如下： vim nfs-pv001.yaml\napiVersion: v1kind: PersistentVolumemetadata:name: nfs-pv001labels:  pv: nfs-pv001spec:capacity:  storage: 1GiaccessModes:  - ReadWriteOncepersistentVolumeReclaimPolicy: RecyclestorageClassName: nfsnfs:  path: /nfs/data/pv001  server: 192.168.1.1\n配置说明：\n① capacity 指定 PV 的容量为 1G。② accessModes 指定访问模式为 ReadWriteOnce，支持的访问模式有：2.1ReadWriteOnce – PV 能以 read-write 模式 mount 到单个节点。2.2ReadOnlyMany – PV 能以 read-only 模式 mount 到多个节点。2.3ReadWriteMany – PV 能以 read-write 模式 mount 到多个节点。③ persistentVolumeReclaimPolicy 指定当 PV 的回收策略为 Recycle，支持的策略有：3.1Retain – 需要管理员手工回收。3.2Recycle – 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*。3.3Delete – 删除 Storage Provider 上的对应存储资源，例如 AWS EBS、GCE PD、AzureDisk、OpenStack Cinder Volume 等。④ storageClassName 指定 PV 的 class 为 nfs。相当于为 PV 设置了一个分类，PVC 可以指定 class 申请相应 class 的 PV。⑤ 指定 PV 在 NFS 服务器上对应的目录。\n\n创建对应pv：\nkubectl apply -f nfs-pv001.yaml# persistentvolume/nfs-pv001 createdkubectl get pv# NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE# nfs-pv001   1Gi        RWO            Recycle          Available           nfs                     4s\nSTATUS 为 Available，表示 pv就绪，可以被 PVC 申请。\n创建PVC接下来创建一个名为pvc001的PVC，配置文件 nfs-pvc001.yaml 如下：vim nfs-pvc001.yaml\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nfs-pvc001spec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi  storageClassName: nfs  selector:    matchLabels:      pv: nfs-pv001\n执行yaml文件创建 pvc：\nkubectl apply -f nfs-pvc001.yaml# persistentvolumeclaim/nfs-pvc001 createdkubectl get pvc# NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE# nfs-pvc001   Bound    pv001    1Gi        RWO            nfs            6skubectl get pv# NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE# nfs-pv001   1Gi        RWO            Recycle          Bound    default/pvc001   nfs                     9m12s\n从 kubectl get pvc 和 kubectl get pv 的输出可以看到 pvc001绑定到pv001，申请成功。注意pvc绑定到对应pv通过labels标签方式实现，也可以不指定，将随机绑定到pv。 接下来就可以在 Pod 中使用存储了，Pod 配置文件 nfs-pod001.yaml 如下：vim nfs-pod001.yaml \nkind: PodapiVersion: v1metadata:  name: nfs-pod001spec:  containers:    - name: myfrontend      image: nginx      volumeMounts:      - mountPath: &quot;/var/www/html&quot;        name: nfs-pv001  volumes:    - name: nfs-pv001      persistentVolumeClaim:        claimName: nfs-pvc001\n与使用普通 Volume 的格式类似，在 volumes 中通过 persistentVolumeClaim 指定使用nfs-pvc001申请的 Volume。\n执行yaml文件创建nfs-pdo001：\nkubectl apply -f nfs-pod001.yaml# pod/nfs-pod001 createdkubectl get pod# NAME                                      READY   STATUS    RESTARTS   AGE# nfs-client-provisioner-75bf876d88-sqqpv   1/1     Running   0          25m# nfs-pod001                                1/1     Running   0          12s\n验证 PV 是否可用：\nkubectl exec nfs-pod001 touch /var/www/html/index001.htmlls /nfs/data/pv001/# index001.html\n进入pod查看挂载情况\nkubectl exec -it nfs-pod001 /bin/bash# root@nfs-pod001:/# df -h# ......# 192.168.92.56:/nfs/data/pv001   47G  5.2G   42G  11%  /var/www/html# ......# root@nfs-pod001:/# \n\n删除pv删除pod，pv和pvc不会被删除，nfs存储的数据不会被删除。\nkubectl delete -f nfs-pod001.yaml# pod &quot;nfs-pod001&quot; deletedkubectl get pv# NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE# nfs-pv001   1Gi        RWO            Recycle          Bound    default/pvc001   nfs                     34mkubectl get pvc# NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE# nfs-pvc001   Bound    pv001    1Gi        RWO            nfs            25mls /nfs/data/pv001/# index001.html\n继续删除pvc，pv将被释放，处于 Available 可用状态，并且nfs存储中的数据被删除。\nkubectl delete -f nfs-pvc001.yaml# persistentvolumeclaim &quot;nfs-pvc001&quot; deletedkubectl get pv# NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM            STORAGECLASS   REASON   AGE# nfs-pv001   1Gi        RWO            Recycle          Available                    nfs                     35mls /nfs/data/pv001/\n继续删除pv\nkubectl delete -f nfs-pv001.yaml# persistentvolume &quot;pv001&quot; deleted\n\n\n动态申请PV卷External NFS驱动的工作原理 K8S的外部NFS驱动，可以按照其工作方式（是作为NFS server还是NFS client）分为两类：\n\n1.nfs-client:也就是我们接下来演示的这一类，它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。当用户创建对应的PVC来申请PV时，该provider就将PVC的要求与自身的属性比较，一旦满足就在本地挂载好的NFS目录中创建PV所属的子目录，为Pod提供动态的存储服务。\n2.nfs:与nfs-client不同，该驱动并不使用k8s的NFS驱动来挂载远端的NFS到本地再分配，而是直接将本地文件映射到容器内部，然后在容器内使用ganesha.nfsd来对外提供NFS服务；在每次创建PV的时候，直接在本地的NFS根目录中创建对应文件夹，并export出该子目录。 利用NFS动态提供Kubernetes后端存储卷 本文将介绍使用nfs-client-provisioner这个应用，利用NFS Server给Kubernetes作为持久存储的后端，并且动态提供PV。前提条件是有已经安装好的NFS服务器，并且NFS服务器与Kubernetes的Slave节点都能网络连通。将nfs-client驱动做一个deployment部署到K8S集群中，然后对外提供存储服务。 nfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储\n\n部署nfs-client-provisioner,在master上操作首先克隆仓库获取yaml文件\ngit clone https://github.com/kubernetes-incubator/external-storage.gitcp -R external-storage/nfs-client/deploy/ $HOMEcd deploy\n修改deployment.yaml文件这里修改的参数包括NFS服务器所在的IP地址（192.168.1.1），以及NFS服务器共享的路径（/nfs/data），两处都需要修改为你实际的NFS服务器和共享目录。cat deployment.yaml\napiVersion: v1kind: ServiceAccountmetadata:  name: nfs-client-provisioner---kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: nfs-client-provisionerspec:  replicas: 1  strategy:    type: Recreate  template:    metadata:      labels:        app: nfs-client-provisioner    spec:      serviceAccountName: nfs-client-provisioner      containers:        - name: nfs-client-provisioner          image: quay.io/external_storage/nfs-client-provisioner:latest          volumeMounts:            - name: nfs-client-root              mountPath: /persistentvolumes          env:            - name: PROVISIONER_NAME              value: fuseim.pri/ifs            - name: NFS_SERVER              value: 192.168.1.1            - name: NFS_PATH              value: /nfs/data      volumes:        - name: nfs-client-root          nfs:            server: 192.168.1.1            path: /nfs/data\n部署deployment.yaml\nkubectl apply -f deployment.yaml\n查看创建的POD\n kubectl get pod -o wide# NAME                                      READY   STATUS             RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES# nfs-client-provisioner-75bf876d88-578lg   1/1     Running            0          51m   10.244.2.131   k8s-node2   &lt;none&gt;           &lt;none&gt;\n创建StorageClassstorage class的定义，需要注意的是：provisioner属性要等于驱动所传入的环境变量PROVISIONER_NAME的值。否则，驱动不知道知道如何绑定storage class。 此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致。 （此yaml无需修改）cat class.yaml\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;parameters:  archiveOnDelete: &quot;false&quot;\n部署yaml文件\nkubectl apply -f class.yaml\n查看创建的storageclass\nkubectl get sc# NAME                  PROVISIONER      AGE# managed-nfs-storage   fuseim.pri/ifs   95m\n配置授权如果集群启用了RBAC，则必须执行如下命令授权provisioner。(k8s1.6+默认开启) 此yaml无需修改cat rbac.yaml\nkind: ServiceAccountapiVersion: v1metadata:  name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: nfs-client-provisioner-runnerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumes&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;persistentvolumeclaims&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]  - apiGroups: [&quot;storage.k8s.io&quot;]    resources: [&quot;storageclasses&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;events&quot;]    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: run-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    namespace: defaultroleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisionerrules:  - apiGroups: [&quot;&quot;]    resources: [&quot;endpoints&quot;]    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: leader-locking-nfs-client-provisionersubjects:  - kind: ServiceAccount    name: nfs-client-provisioner    # replace with namespace where provisioner is deployed    namespace: defaultroleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io\n\n部署yaml文件\nkubectl create -f rbac.yaml\n\n测试创建测试PVC\nkubectl create -f test-claim.yaml\n可以看到PVC状态为Bound，绑定的volume为pvc-a17d9fd5-237a-11e9-a2b5-000c291c25f3。\nkubectl get pvc# NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE# test-claim   Bound    pvc-a17d9fd5-237a-11e9-a2b5-000c291c25f3   1Mi        RWX            managed-nfs-storage   34m\n查看自动创建的PV\nkubectl get pv# NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS          REASON   AGE# pvc-a17d9fd5-237a-11e9-a2b5-000c291c25f3   1Mi        RWX            Delete           Bound    default/test-claim   managed-nfs-storage            34m\n然后，我们进入到NFS的export目录，可以看到对应该volume name的目录已经创建出来了。 其中volume的名字是namespace，PVC name以及uuid的组合：\ncd /nfs/data/ll# total 0# drwxrwxrwx 2 root root 21 Jan 29 12:03 default-test-claim-pvc-a17d9fd5-237a-11e9-a2b5-000c291c25f3","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Linux系统结构详解","url":"/20241121/CentOS/028b6eacef3a/","content":"Linux系统结构详解\n\n\nThe End\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Linux运维企业架构项目实战系列","url":"/20241121/CentOS/dea97a47466c/","content":"Linux运维企业架构项目实战系列1.项目实战1—LNMP的搭建、nginx的ssl加密、权限控制的实现LNMP的搭建、nginx的ssl加密、权限控制的实现\n2.项目实战2—LVS、nginx实现负载均衡系列实现基于LVS负载均衡集群的电商网站架构nginx 反向代理负载均衡、动静分离和缓存的实现Tengine实现反向代理负载均衡\n3.项目实战3—实现基于Keepalived+LVS的高可用集群网站架构实现基于Keepalived+LVS的高可用集群网站架构\n4.项目实战4—HAProxy实现高级负载均衡实战和ACL控制HAProxy实现高级负载均衡实战和ACL控制\n5.项目实战5—企业级缓存系统varnish应用与实战企业级缓存系统varnish应用与实战\n6.项目实战6—Mysql实现企业级日志管理、备份与恢复实战Mysql实现企业级日志管理、备份与恢复实战\n7.项目实战7—Mysql实现企业级数据库主从复制架构实战Mysql实现企业级数据库主从复制架构实战\n8.项目实战8—Tomcat系列Tomcat系列\n9.项目实战9—企业级分布式存储应用与实战MogileFS、FastDFS企业级分布式存储应用与实战MogileFS、FastDFS\n10.项目实战10—企业级自动化运维工具系列企业级自动化运维工具系列-ansible企业级自动化运维工具系列-puppet企业级自动化运维工具系列-puppet高阶用法\n11.项目实战11—企业级nosql数据库应用与实战-redis的主从和集群企业级nosql数据库应用与实战-redis的主从和集群\n12.项目实战12—企业级监控工具应用实战-zabbix系列企业级监控工具应用实战-zabbix安装与基础操作企业级监控工具应用实战-zabbix操作进阶\n13.项目实战13—企业级虚拟化Virtualization-KVM技术企业级虚拟化Virtualization-KVM技术\n14.项目实战14—ELK 企业内部日志分析系统系列ELK 企业内部日志分析系统ELK 经典用法—公司自定义日志和mysql模块ELK重难点总结和整体优化配置\n15.项目实战15—企业级堡垒机 jumpserver系列企业级堡垒机 jumpserver一步一步搭建企业级堡垒机 jumpserver快速入门\n16.项目实战16—Jenkins服务系列Jenkins服务搭建和部署Jenkins+Maven+Gitlab+Tomcat 自动化构建打包、部署\n17.项目实战17—超详细“零”基础kafka入门篇超详细“零”基础kafka入门篇\n18.项目实战18—Hadoop+Hbase分布式集群架构Hadoop+Hbase分布式集群架构\nThe End\n来自along\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Mondo一键部署、镜像恢复，快速部署","url":"/20241121/CentOS/c09dfdba7153/","content":"Mondo一键部署、镜像恢复，快速部署一.简介Mondo Rescue是一个开源免费的故障恢复和备份工具，用户可以轻松地创建系统（Linux或Windows）克隆或备份的ISO镜像，可以将这些镜像存放在CD、DVD、磁带、USB设备、硬盘和NFS上。Mondo Rescue还可以用来快速恢复或者重新部署工作镜像到其他系统中，万一碰到数据丢失，你还可以从备份介质恢复尽可能完整的系统数据。\nMondo Rescue（以下简称Mondo）可以说是Linux 下的Ghost，它可以将你的系统照相一样备份至磁带，CD-R，CD-RW，NFS或硬盘分区。Mondo广泛支持LVM、RAID、ext2、ext3、JFS、XFS、ReiserFS、VFAT等。这个软件可以让大家一步一步地将Linux系统备份出来，一旦今后出了问题，只要用创建好的MondoCD将系统还原至上次备份时的状态即可。架构：Mondo Rescue由Mondo和Mindi两个包组成。Mondo是主程序用来备份和还原。Mindi则类似一个迷你的Linux，它包括核心，模块，函数库及一些做系统维护的重要工具。目前，Mondo已经支持Asianux 2&#x2F;3系统平台。\nMondo rescue 支持 各版本linux系统，本文档只在centos和redhat这俩个linux版本里做过实验，这俩个版本也是当下比较用的普遍的\n二.系统注意事项（1） 镜像机要能上网，因为需要从网上装所需的包，没有网络的话，需要的软件包也早下载好了，可以离线安装，离线安装比较费事，尽量有网比较好\n（2） 镜像机磁盘不要用lvm的，使用linux的标准分区，文件系统ext3、ext4、xfs都可以。\n（3） 镜像机在装系统的时候，建立俩个分区就可以，一个&#x2F;boot 分区，给200m，一个&#x2F; 分区，&#x2F; 分区看着给，不要给太大，够装应用就可以，做好的镜像在还原到别的机器上的时候，会自动帮你扩展，给的太大，会出问题。千万不要创建swap分区，会出问题，swap分区系统装好了也可以创建的，不用担心\n（4） 分区挂载格式要用盘符去挂载不要用UUID去挂载，不然做好镜像去还原的时候会识别不了 分区挂载文件\n## /etc/fstab# Created by anaconda on Wed Jan 12 11:52:23 2022## Accessible filesystems, by reference, are maintained under &#x27;/dev/disk&#x27;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0#UUID=72488b94-4520-406c-83ec-678d619cdfe9 /boot                   xfs     defaults        0 0/dev/sda2              /boot                    xfs     defaults        0 0\n\n\n\n三.安装过程1.配置mondo源cd /etc/yum.repos.d/wget ftp://ftp.mondorescue.org/centos/7/x86_64/mondorescue.reposed -i &quot;s/gpgcheck=1/gpgcheck=0/g&quot; mondorescue.repocat mondorescue.repo\n\n2.安装mondoyum makecacheyum install mondo -y\n\n四.制作镜像 运行mondo\n 以root用户权限运行mondoarchive命令\n 选择Hard disk 备份ISO镜像到硬盘 如下图： \n\n 选择备份路径 \n\n 使用gzip方式压缩 \n\n 压缩率：选择第一个Maximum \n\n 设置ISO镜像的大小 ，修改设置为4480(DVD光盘大小)\n\n 设置镜像名称 (可以为任意名称)\n\n需要备份的路径，&#x2F;代表全系统 \n\n 需要排除的目录，排除&#x2F;proc|&#x2F;tmp \n\n 临时目录 (默认即可)\n\n\n 是否备份扩展属性 \n\n确认kernel系统内核 \n\n 备份后是否verify检查\n \n 确认是否Proceed开始备份 \n\n\n 收集Mindi启动必须的文件信息 \n\n\n无需操作，等待完成，点击OK完成\n\n 备份后的镜像文件在&#x2F;var&#x2F;cache&#x2F;mondo内\n五.恢复镜像 使用备份的iso文件进行恢复\n开机启动 \n 输入nuke，格式化分区并恢复所有文件 \n\n\n\n选择yes\n\n修改第二行&#x2F;dev&#x2F;centos&#x2F;root为&#x2F;dev&#x2F;mapper&#x2F;centos-root\n\n\n选择yes\n\n选择yes\n\n\n恢复文件，选择yes\n\n加载boot，选择yes\n\n默认选择no，自己配置选择yes(此处选择yes)\n\n\n以下需要修改几个文件(此处默认，未进行修改)\n\n\n\n\n\n\n恢复完成，输入exit进行重启\n\n完！\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Mysql主从配置","url":"/20241121/uncategorized/f01e0c6480b3/","content":"Mysql主从配置一.主从服务器分别作以下操作# 1、版本一致# 2、初始化表，并在后台启动mysql# 3、修改root的密码\n\n\n\n二.修改主服务器mastervi /etc/my.cnf[mysqld]log-bin=mysql-bin  //[必须]启用二进制日志server-id=222    //[必须]服务器唯一ID，默认是1，一般取IP最后一段\n\n\n\n三.修改从服务器slave#vi /etc/my.cnf[mysqld]log-bin=mysql-bin  //[不是必须]启用二进制日志server-id=226    //[必须]服务器唯一ID，默认是1，一般取IP最后一段\n\n\n\n四.重启两台服务器的mysql/etc/init.d/mysql restart/mysql//mysql/support-files/mysql.server restart\n\n\n\n五.在主服务器上建立帐户并授权slavemysql -uroot -pmysql&gt;GRANT REPLICATION SLAVE ON *.* to &#x27;boer&#x27;@&#x27;%&#x27; identified by &#x27;Boer@147258&#x27;; //一般不用root帐号，“%”表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.145.226，加强安全。# 8.0GRANT replication slave ON *.* TO &#x27;boer&#x27;@&#x27;%&#x27;;GRANT ALL privileges ON *.* TO &#x27;boer&#x27;@&#x27;%&#x27;;flush privileges;\n\n# 创建用户(8.0)create user &#x27;#userName&#x27;@&#x27;#host&#x27; identified by &#x27;#passWord&#x27;;create user &#x27;entegor&#x27;@&#x27;%&#x27; identified by &#x27;entegor@123&#x27;;# 授权GRANT REPLICATION SLAVE ON *.* TO &#x27;entegor&#x27;@&#x27;%&#x27;;  --8.0版本FLUSH PRIVILEGES;GRANT ALL PRIVILEGES ON *.* TO &#x27;entegor&#x27;@&#x27;%&#x27; WITH GRANT OPTION;  --8.0版本FLUSH PRIVILEGES;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; WITH GRANT OPTION;  --8.0版本FLUSH PRIVILEGES;\n\n\n\n六.登录主服务器的mysql，查询master的状态# 主库执行show master status;查看状态mysql&gt;show master status;+------------------+----------+--------------+------------------+| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000004 |      308 |              |                  |+------------------+----------+--------------+------------------+1 row in set (0.00 sec)注：执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化\n\n七.配置从服务器Slave# 从库执行change master to master_host=&#x27;192.168.145.222&#x27;,master_user=&#x27;mysync&#x27;,master_password=&#x27;q123456&#x27;,master_log_file=&#x27;mysql-bin.000004&#x27;,master_log_pos=308;  //注意不要断开，308数字前后无单引号。# 执行start slave;  //启动从服务器复制功能\n\n八.检查从服务器复制功能状态show slave status\\G   *************************** 1. row ***************************              Slave_IO_State: Waiting for master to send event              Master_Host: 192.168.2.222  //主服务器地址              Master_User: mysync   //授权帐户名，尽量避免使用root              Master_Port: 3306    //数据库端口，部分版本没有此行              Connect_Retry: 60              Master_Log_File: mysql-bin.000004              Read_Master_Log_Pos: 600     //#同步读取二进制日志的位置，大于等于Exec_Master_Log_Pos              Relay_Log_File: ddte-relay-bin.000003              Relay_Log_Pos: 251              Relay_Master_Log_File: mysql-bin.000004              Slave_IO_Running: Yes    //此状态必须YES              Slave_SQL_Running: Yes     //此状态必须YES                    ......\n\n# 注：Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则都是错误的状态(如：其中一个NO均属错误)。# 以上操作过程，主从服务器配置完成。\n\n九.主从服务器测试# 主服务器Mysql，建立数据库，并在这个库中建表插入一条数据： mysql&gt; CREATE DATABASE `hi_db` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; Query OK, 1 row affected (0.00 sec) mysql&gt; use hi_db; Database changed mysql&gt; create table hi_tb(id int(3),name char(10)); Query OK, 0 rows affected (0.00 sec)  mysql&gt; insert into hi_tb values(001,&#x27;bobu&#x27;); Query OK, 1 row affected (0.00 sec) mysql&gt; show databases;  +--------------------+  | Database      |  +--------------------+  | information_schema |  | hi_db         |  | mysql         |  | test         |  +--------------------+  4 rows in set (0.00 sec)\n\n# 从服务器Mysql查询：  mysql&gt; show databases;  +--------------------+  | Database        |  +--------------------+  | information_schema |  | hi_db         |    //I&#x27;M here，大家看到了吧  | mysql         |  | test      |  +--------------------+  4 rows in set (0.00 sec)\n\nmysql&gt; use hi_dbDatabase changedmysql&gt; select * from hi_tb;      //查看主服务器上新增的具体数据+------+------+| id  | name |+------+------+|  1 | bobu |+------+------+1 row in set (0.00 sec)\n\nMysql主从复制完成\nmysql正确关闭slave取消主从正确关闭slave步骤1.执行STOP SLAVE语句\nstop slave;\n\n2.使用 show status like &#39;%slave%&#39;;检查slave_open_temp_tables变量的值\nshow status like &#x27;%slave%&#x27;;mysql&gt; show status like &#x27;%slave%&#x27;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| Com_show_slave_hosts | 0 || Com_show_slave_status | 0 || Com_slave_start | 0 || Com_slave_stop | 0 || Slave_open_temp_tables | 0 |+-----------------------\n\n3.如果值为0，使用mysqladmin shutdown命令关闭从服务器\n4.如果值不为0，用START SLAVE重启从服务器线程 slave_open_temp_tables值显示，当前slave创建了多少临时表，注意由client显示创建的 即便是这样，在使用临时表的场景下，如果服务器宕机，将遇到不可预知的问题。 所以比较保险的做法是，创建实体表，虽然会由于分配的文件刷新到磁盘。 \nmysql 删除 主从信息原来配置的是主从同步，现在不需要了，想去掉从服务器1.进入mysql\nmysql -uroot -p\n2.关闭复制\nstop slave;\n3.重置slave\nreset slave;\n4.将master设置为空\nchange master to master_user=&#x27;&#x27;, master_host=&#x27; &#x27;, master_password=&#x27;&#x27;;\n","tags":["Mysql"]},{"title":"Mysql之my.cnf配置","url":"/20241121/Mysql/cf6c43d1d7ec/","content":"Mysql之my.cnf配置[mysqld]user=mysqlbasedir=/mysqldatadir=/mysql/datasocket=/tmp/mysql.socksecure-file-priv=/tmpserver_id=10lower_case_table_names=1log_bin_trust_function_creators=TRUEsql_mode=NO_ENGINE_SUBSTITUTIONsync_binlog=1  #规定二进制日志从内存写入磁盘的时间点,1为事务提交时写入磁盘log_bin=/binlog/mysql-bin  #指定二进制日志文件位置及前缀,等于同时设置了开启二进制日志功能innodb_buffer_pool_size=1300M  #数据缓冲区,测试机内存2G,我按照60%给的innodb_flush_log_at_trx_commit=1  #控制了redolog数据从内存写入磁盘的时间点,1为事务提交时写入磁盘innodb_log_buffer_size=32M  #设置redo缓冲区大小innodb_log_file_size=128M  #设置redolog重做日志的大小innodb_log_files_in_group=3  #redolog文件数量transaction_isolation=REPEATABLE-READ  #隔离级别RRinnodb_data_file_path=ibdata1:512M;ibdata2:512M:autoextend  #生成两个存放数据字典和undolog日志文件,一个512Mgtid-mode=on  #开启GTID功能enforce-gtid-consistency=true  #强制执行GTID启动数据库服务#log-slave-updates=1  #从库记录relay执行的日志到binlog,适用于从库下面还有从库的环境#slave-parallel-type=LOGICAL_CLOCK #LOGICAL_CLOCK：基于组提交的并行复制方式#slave-parallel-workers=8  #slave复制线程数#master_info_repository=TABLE  #master_info信息保存在表中，性能可以有50%~80%的提升#relay_log_info_repository=TABLE #relay.info信息保存在表中#relay_log_recovery=ON  #从库设置，宕机后从主库拿数据保证relay-log的完整性skip-name-resolve  #禁止DNS反向解析relay_log_purge=0  #从库关闭relay语句执行后删除功能,为了安全max_connections=3096  #最大连接数,可以根据测试结果设置back_log=16  #达到最大连接数后,允许的最大排队用户数量wait_timeout=60  #处于sleep连接状态的连接线程自动释放时间,默认60秒#interactive_timeout=7200 #谨慎设置，应与开发人员沟通后设置，如果开发需要长连接，请勿修改key_buffer_size=16M  #MyISAM索引缓冲区,还和ibtmp临时表有关,减小io和cpu负载,可以设置稍微大一点,根据show status like &quot;created_tmp%&quot;;disk使用量调整query_cache_size=128M  #查询缓存,但是命中率不高,一般不使用query_cache_type=1  #缓存类型,其代表全部query_cache_limit=50M  #限制单个查询使用的最大缓存数max_connect_errors=2000  #最大错误连接数,超过将无法登陆,建议大一点sort_buffer_size=2M  #排序缓冲区max_allowed_packet=256M  #允许最大传输包大小join_buffer_size=2M  #多表联合join缓冲区thread_cache_size=16  #连接线程缓存池,下次可以直接连接,减小cpu压力,但是较消耗内存binlog_cache_size=2M  #这个是每个会话分配的内存,酌情给,否则会占用大量内存,二进制日志缓存区大小max_binlog_cache_size=8M  #最大binlog日志缓存区大小，show global status like &#x27;bin%&#x27;;查看Binlog_cache_disk_use值比较大的时候，我们可以考虑适当的调高 binlog_cache_size对应的值max_binlog_size=512M  #规定二进制日志binlog文件最大文件大小,超出就会生成新文件expire_logs_days=15  #二进制日志保留期限,原则上2个全备周期+1read_buffer_size=2M  #读入缓冲区的大小read_rnd_buffer_size=2M  #随机读(查询操作)缓冲区大小bulk_insert_buffer_size=32M  #批量插入数据缓存大小,可以有效提高插入效率,默认为8M[mysql]socket=/tmp/mysql.sockprompt=mysql5729_db01 [\\\\d]&gt;[client]socket=/tmp/mysql.sock\n\nMysql配置参数介绍\nMySQL获取配置信息路径    1、命令行参数        mysqld_safe --datadir=/data/sql_data    2、配置文件        查看配置文件的命令：        [root@localhost ~]# mysqld --help --verbose | egrep -A 1 &#x27;Default options&#x27;        配置文件的有效路径            /etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnfMySQL配置参数的作用域    1、全局参数        set global 参数名=参数值;        set @@global.参数名:=参数值;    2、会话参数         set [session] 参数名=参数值;        set @@session.参数名:=参数值;内存配置相关参数    1、确定可以使用的内存的上限    2、确定MySQL的每个连接使用的内存        sort_buffer_size        join_buffer_size        read_buffer_size        read_rnd_buffer_size    3、确定需要为操作系统保留多少内存    4、如何为缓存池分配内存        Innodb_buffer_pool_size            注：设置缓存池的大小的考量标准为:总内存-(每个编程所以需要的内存*连接数)-系统保留内存        key_buffer_size        select sum(index_length) from information_schema.tables where engines=&#x27;myisam&#x27;    I/O相关配置参数    Innodo I/O相关配置    Innodb_log_file_size 单个事务日志的大小    Innodb_log_files_in_group 控制文件日子的个数    事务日志总大小 = Innodb_log_files_in_group * Innodb_log_file_size    Innodb_log_buffer_size = (32M or 128M)    Innodb_flush_log_at_trx_commint        0:每秒进行一次log写入cache,并flush log到磁盘        1[默认]:在每次事务提交执行log写入cache，并flush log到磁盘        2[建议]:每次事务提交，执行log数据写入到cache中，每秒执行一次flush log到磁盘    Innodb_flush_method=O_DIRECT    Innodb_file_per_table = 1    Innodb_doublewrite = 1MyISAM I/O相关配置    delay_key_write        OFF:每次写操作后刷新键缓冲中的脏块到磁盘        ON:只对在键表时指定了delay_key_write选项的表使用延迟刷新        ALL:对所有的MyISAM表都使用延迟建写入        安全相关配置参数    expire_logs_days 指定自动清理binlog的天数    max_allowed_packet 控制MySQL可以连接的包大小，建议设置为32M，如果使用了主从复制，参数应该设置成一致的    skip_name_resolve 禁用DNS查找    sysdate_is_now 确保sysdate()返回确保性日期    read_only 禁止非super权限的用户写操作  注：建议在主从复制中的从库开启此功能。以确保不能修改从库中的操作，只能从主库同步过来    skip_slave_start 禁用Salve自动恢复(从库中的设置使用)    sql_mode 设置MySQL所使用的SQL模式 （谨慎操作，可能会造成MySQL无法执行）        ① strict_trans_tables 给定的数据如果不能插入到数据库中，对事务引擎会终端操作，对非事务引擎是没有影响的        ② no_engine_subitiution 在create table中指定engines的时候，如果引擎不可用，不会使用默认引擎建立表        ③ no_zero_date 不能再表中插入0年0月0日的日期        ④ no_zero_in_date 不接受一部分的为0的日期        ⑤ noly_full_group_by     其他常用的配置参数    sync_binlog 控制MySQL如何向磁盘刷新binlog    tmp_table_size 和 max_heap_table_size 控制内存临时表大小(不宜设置的太大，以避免内存的溢出)    max_connections 控制允许的最大连接数(默认为100，有点小，根据自己的业务适当的调整大小)\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql之mysqldump备份与恢复","url":"/20241121/Mysql/fb5dad26e852/","content":"Mysql之mysqldump备份与恢复一.介绍​\tmysqldump是mysql用于转存储数据库的实用程序。它主要产生一个SQL脚本，其中包含从头重新创建数据库所必需的命令CREATE TABLE INSERT等。它的备份原理是通过协议连接到 MySQL 数据库，将需要备份的数据查询出来，将查询出的数据转换成对应的insert 语句，当我们需要还原这些数据时，只要执行这些 insert 语句，即可将对应的数据还原。\n二.备份命令格式方式1\nmysqldump [选项] 数据库名 [表名] &gt; 脚本名\n\n方式2\nmysqldump [选项] --数据库名 [选项 表名] &gt; 脚本名\n\n方式3\nmysqldump [选项] --all-databases [选项]  &gt; 脚本名\n\n方式4 多库导出\nmysqldump [选项] --databases [数据库名1] [数据库名2]  [选项]  &gt; 脚本名\n\n方式5 多库多表导出\nmysqldump [选项] --databases [数据库名1] --tables [表名1] [表名2] --databases [数据库名2] --tables [表名1] [表名2]  [选项]  &gt; 脚本名\n\n方式6 导出结果不导出数据增加 –no-data参数即可\nmysqldump [选项] --no-data  --databases [数据库名1] --tables [表名1] [表名2] --databases [数据库名2] --tables [表名1] [表名2]  [选项]  &gt; 脚本名\n\n方式7 排除某些表\nmysqldump -uroot -p test --ignore-table=test.t1 --ignore-table=test.t2 &gt; 脚本名\n\n\n参数名\t\t\t\t\t\t\t缩写\t含义\n\n–host\t\t\t\t\t\t\t-h\t\t服务器IP地址–port\t\t\t\t\t\t\t-P\t\t服务器端口号–user\t\t\t\t\t\t\t-u\t\tMySQL 用户名–pasword\t\t\t\t\t\t-p\t\tMySQL 密码–databases\t\t\t\t\t\t\t\t指定要备份的数据库–all-databases\t\t\t\t\t\t\t备份mysql服务器上的所有数据库–compact\t\t\t\t\t\t\t\t压缩模式，产生更少的输出–comments\t\t\t\t\t\t\t\t添加注释信息–complete-insert\t\t\t\t\t\t输出完成的插入语句–lock-tables\t\t\t\t\t\t\t备份前，锁定所有数据库表–no-create-db&#x2F;–no-create-info\t\t\t禁止生成创建数据库语句–force\t\t\t\t\t\t\t\t\t当出现错误时仍然继续备份操作–default-character-set\t\t\t\t\t指定默认字符集–add-locks\t\t\t\t\t\t\t\t备份数据库表时锁定数据库表\n–no-create-db，  —取消创建数据库sql(默认存在)–no-create-info，—取消创建表sql(默认存在)–no-data         —不导出数据(默认导出)–add-drop-database —增加删除数据库sql（默认不存在）–skip-add-drop-table  —取消每个数据表创建之前添加drop数据表语句(默认每个表之前存在drop语句)–skip-add-locks       —取消在每个表导出之前增加LOCK TABLES（默认存在锁）–skip-comments        —注释信息(默认存在)\n实例备份所有数据库\nmysqldump -uroot -p --all-databases &gt; /data/mysqldump/all.db\n\n备份指定数据库\nmysqldump -uroot -p test &gt; /data/mysqldump/test.db\n\n备份指定数据库指定表(多个表以空格间隔)\nmysqldump -uroot -p  mysql db event &gt; /data/mysqldump/2table.db\n\n备份指定数据库排除某些表\nmysqldump -uroot -p test --ignore-table=test.t1 --ignore-table=test.t2 &gt; /data/mysqldump/test2.db\n\n\n\n三.还原系统行命令mysqladmin -uroot -p create db_name mysql -uroot -p  db_name &lt; /data/mysqldump/db_name.db\n\n注：在导入备份数据库前，db_name如果没有，是需要创建的； 而且与db_name.db中数据库名是一样的才可以导入。\nsoure 方法mysql &gt; use db_namemysql &gt; source /data/mysqldump/db_name.db\n\n\n\n\n\nEnd\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql之常用引擎对比与参数调优","url":"/20241121/Mysql/12a08aceb506/","content":"MySQL常用存储引擎之MyISAM特性：    1、并发性与锁级别    2、表损坏修复        check table tablename        repair table tablename    3、MyISAM表支持的索引类型        ①、全文索引        ②、前缀索引    4、MyISAM表支持数据压缩        myisampack限制：    版本 &lt; MySQL5.0时默认表大小为4G    如存储达标则要修改MAX_Rows和AVG_ROW_LENGTH    版本 &gt; MySQL5.0时默认支持为256TB适用场景：    1、非事务形应用    2、只读类应用    3、空间类应用\n\nMySQL常用存储引擎之InnodbInnodb存储引擎的特征    1、Innodb是一种事务性存储引擎    2、完全支持事务的ACID特性    3、Redo Log 和 Undo Log    4、Innodb支持行级锁Innodb使用表空间进行 数据存储    为每个表独立创建一个表空间存储        innodb_file_per_table            ON:独立表空间:tablename.ibd            OFF:系统表空间:ibdataX（X是个数字，从1开始的数字）    系统表空间和独立表空间要如何选择    比较：        系统表空间无法捡的收缩文件大小        独立表空格键可以通过optimize table命令收缩系统文件        系统表空间会产生IO瓶颈        独立表空间可以同时向多个文件刷新数据    表转移的步骤    步骤：        1、使用mysqldump到处所有数据库表数据        2、停止MySQL服务，修改参数，并删除Innodb相关文件        3、重启MySQL服务，重建Innodb系统表空间        4、重新导入数据\n\nMySQL常见的存储引擎之CSV文件系统存储特点    1、数据以文本方式存储在文本中    2、.csv文件存储表内容    3、.csm文件存储表的元数据如表状态和数据量    4、.frm文件存储表结构信息    5、以csv格式进行存储    6、所有列必须都是不能为Null的    7、不支持索引    适用场景：        适用作为数据交换的中间表(电子表格-&gt;csv文件-&gt;MySQL数据库目录)\n\nMySQL常用存储引擎之Archive文件系统存储特点    1、以zlib对表数据进行压缩,磁盘I/O更少    2、数据存储在ARZ为后缀的文件中Archive存储引擎的特点    1、只支持insert和select操作    2、只允许在自增的ID列上加索引        适用场景：        日志和数据采集类应用\n\nMySQL常用存储引擎之Memory文件系统存储特点    1、也成HEAP存储引擎，所以数据保存在内存中功能特点：    1、支持HASH索引和Btree索引    2、所有字段都有固定长度varchar(10)=char(10)    3、不支持BLOG和TEXT等大字段    4、Memory存储引擎使用表级锁    5、最大大小由max_heap_table_size参数决定    适用场景：        1、用于查找或者是映射表，例如邮编和地区的对应表        2、用于保存数据分心中产生的中间表        3、用于缓存周期性聚合数据的结果表\n\nMySQL常用存储引擎之Federated特点：    1、提供了访问远程MySQL服务器上表的方法    2、本地不存储数据，数据全部放到远程服务器上    3、本地需要保存表结构和远程服务器的连接信息如何使用    默认静止，启用需要在启动时增加federated参数    mysql://user_name[:password]@host_name[:port]/db_name/table_name    适用场景：        偶尔的统计分析及手工查询\n\n如何选择正确的存储引擎常用引擎MyISAM与InnoDb对比\n\n参考条件    1、是否要支持事务    2、定期备份    3、崩溃恢复    4、存储引擎的特有特性\n\n\n什么影响了性能数据库设计对性能的影响    1、过分的反范式化为表建立太多的列    2、过分的范式化造成太多的表关联（关联的表尽可能的控制在10个之内）    3、在OLTP环境中使用不恰当的分区表    4、使用外键保证数据的完整性\n\n总结性能优化的顺序    1、数据库结构设计和SQL语句    2、数据库存储引擎的选择参数配置    3、系统选择及优化    4、硬件升级\n\nMysql配置参数介绍MySQL获取配置信息路径    1、命令行参数        mysqld_safe --datadir=/data/sql_data    2、配置文件        查看配置文件的命令：        [root@localhost ~]# mysqld --help --verbose | egrep -A 1 &#x27;Default options&#x27;        配置文件的有效路径            /etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnfMySQL配置参数的作用域    1、全局参数        set global 参数名=参数值;        set @@global.参数名:=参数值;    2、会话参数         set [session] 参数名=参数值;        set @@session.参数名:=参数值;内存配置相关参数    1、确定可以使用的内存的上限    2、确定MySQL的每个连接使用的内存        sort_buffer_size        join_buffer_size        read_buffer_size        read_rnd_buffer_size    3、确定需要为操作系统保留多少内存    4、如何为缓存池分配内存        Innodb_buffer_pool_size            注：设置缓存池的大小的考量标准为:总内存-(每个编程所以需要的内存*连接数)-系统保留内存        key_buffer_size        select sum(index_length) from information_schema.tables where engines=&#x27;myisam&#x27;    I/O相关配置参数    Innodo I/O相关配置    Innodb_log_file_size 单个事务日志的大小    Innodb_log_files_in_group 控制文件日子的个数    事务日志总大小 = Innodb_log_files_in_group * Innodb_log_file_size    Innodb_log_buffer_size = (32M or 128M)    Innodb_flush_log_at_trx_commint        0:每秒进行一次log写入cache,并flush log到磁盘        1[默认]:在每次事务提交执行log写入cache，并flush log到磁盘        2[建议]:每次事务提交，执行log数据写入到cache中，每秒执行一次flush log到磁盘    Innodb_flush_method=O_DIRECT    Innodb_file_per_table = 1    Innodb_doublewrite = 1MyISAM I/O相关配置    delay_key_write        OFF:每次写操作后刷新键缓冲中的脏块到磁盘        ON:只对在键表时指定了delay_key_write选项的表使用延迟刷新        ALL:对所有的MyISAM表都使用延迟建写入        安全相关配置参数    expire_logs_days 指定自动清理binlog的天数    max_allowed_packet 控制MySQL可以连接的包大小，建议设置为32M，如果使用了主从复制，参数应该设置成一致的    skip_name_resolve 禁用DNS查找    sysdate_is_now 确保sysdate()返回确保性日期    read_only 禁止非super权限的用户写操作  注：建议在主从复制中的从库开启此功能。以确保不能修改从库中的操作，只能从主库同步过来    skip_slave_start 禁用Salve自动恢复(从库中的设置使用)    sql_mode 设置MySQL所使用的SQL模式 （谨慎操作，可能会造成MySQL无法执行）        ① strict_trans_tables 给定的数据如果不能插入到数据库中，对事务引擎会终端操作，对非事务引擎是没有影响的        ② no_engine_subitiution 在create table中指定engines的时候，如果引擎不可用，不会使用默认引擎建立表        ③ no_zero_date 不能再表中插入0年0月0日的日期        ④ no_zero_in_date 不接受一部分的为0的日期        ⑤ noly_full_group_by     其他常用的配置参数    sync_binlog 控制MySQL如何向磁盘刷新binlog    tmp_table_size 和 max_heap_table_size 控制内存临时表大小(不宜设置的太大，以避免内存的溢出)    max_connections 控制允许的最大连接数(默认为100，有点小，根据自己的业务适当的调整大小)\n\n\nEnd\n来自huiy_小溪\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql在线安装","url":"/20241121/Mysql/c22094c42a0a/","content":"Mysql在线安装# 1.centos的yum 源中默认是没有mysql的，所以我们需要先去官网下载mysql的repo源并安装\n\n# 2.安装 yum repo文件并更新 yum 缓存；rpm -ivh mysql80-community-release-el7-3.noarch.rpm# 3.更新 yum 命令yum clean allyum makecache# 卸载其他版本mysql(无需执行)rpm -qa | grep -i mysqlrpm -e --nodeps   mysql-community-common-8.0.22-1.el7.x86_64\n\n# 4.查看mysql yum仓库中mysql版本yum repolist all | grep mysql[root@k8s-master2 ~]# yum repolist all | grep mysqlmysql-cluster-7.5-community/x86_64  MySQL Cluster 7.5 Community  disabledmysql-cluster-7.5-community-source  MySQL Cluster 7.5 Community  disabledmysql-cluster-7.6-community/x86_64  MySQL Cluster 7.6 Community  disabledmysql-cluster-7.6-community-source  MySQL Cluster 7.6 Community  disabledmysql-cluster-8.0-community/x86_64  MySQL Cluster 8.0 Community  disabledmysql-cluster-8.0-community-source  MySQL Cluster 8.0 Community  disabledmysql-connectors-community/x86_64   MySQL Connectors Community   enabled:    175mysql-connectors-community-source   MySQL Connectors Community - disabledmysql-tools-community/x86_64        MySQL Tools Community        enabled:    120mysql-tools-community-source        MySQL Tools Community - Sour disabledmysql-tools-preview/x86_64          MySQL Tools Preview          disabledmysql-tools-preview-source          MySQL Tools Preview - Source disabledmysql55-community/x86_64            MySQL 5.5 Community Server   disabledmysql55-community-source            MySQL 5.5 Community Server - disabledmysql56-community/x86_64            MySQL 5.6 Community Server   disabledmysql56-community-source            MySQL 5.6 Community Server - disabledmysql57-community/x86_64            MySQL 5.7 Community Server   disabledmysql57-community-source            MySQL 5.7 Community Server - disabledmysql80-community/x86_64            MySQL 8.0 Community Server   enabled:    211mysql80-community-source            MySQL 8.0 Community Server - disabled\n\n# 5.使用 yum-config-manager 命令修改相应的版本为启用状态最新版本为禁用状态yum-config-manager --disable mysql80-communityyum-config-manager --enable mysql57-community# 或者可以编辑 mysql repo文件vi /etc/yum.repos.d/mysql-community.repo # 将相应版本下的enabled改成 1 即可；[root@k8s-master2 ~]# cat /etc/yum.repos.d/mysql-community.repo # Enable to use MySQL 5.5[mysql55-community]name=MySQL 5.5 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.5-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.7[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql80-community]name=MySQL 8.0 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-8.0-community/el/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-connectors-community]name=MySQL Connectors Communitybaseurl=http://repo.mysql.com/yum/mysql-connectors-community/el/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-tools-community]name=MySQL Tools Communitybaseurl=http://repo.mysql.com/yum/mysql-tools-community/el/7/$basearch/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-tools-preview]name=MySQL Tools Previewbaseurl=http://repo.mysql.com/yum/mysql-tools-preview/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-cluster-7.5-community]name=MySQL Cluster 7.5 Communitybaseurl=http://repo.mysql.com/yum/mysql-cluster-7.5-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-cluster-7.6-community]name=MySQL Cluster 7.6 Communitybaseurl=http://repo.mysql.com/yum/mysql-cluster-7.6-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-cluster-8.0-community]name=MySQL Cluster 8.0 Communitybaseurl=http://repo.mysql.com/yum/mysql-cluster-8.0-community/el/7/$basearch/enabled=0gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql\n\n# 6.yum安装yum install mysql-community-server -y\n\n# 7.查看mysql状态systemctl status mysqld.service# 启动mysqlsystemctl start mysqld.service\n\n# 8.获取初始密码登录mysqlcat /var/log/mysqld.log | grep password\n\n# 9.登录mysql# 方式1mysql -u root -p# 修改密码ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;newPassword&#x27;;ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;Ideal@147258&#x27;;use mysql;update user set host = &#x27;%&#x27; where user = &#x27;root&#x27;;FLUSH PRIVILEGES;# 方式2mysql -uroot -puse mysql;# Mysql默认不允许远程登录，所以需要开启远程访问权限select user,authentication_string,host from user;update user set host = &#x27;%&#x27; where user = &#x27;root&#x27;;FLUSH PRIVILEGES;# navicat 连接 mysql 出现`Client does not support authentication protocol requested by server`alter user &#x27;root&#x27;@&#x27;%&#x27; identified with mysql_native_password by &#x27;密码&#x27;;\n\n# 10.mysql服务加入开机启动项，并启动mysql进程systemctl enable mysqld.servicesystemctl restart mysqld.service\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql复制架构","url":"/20241121/Mysql/1c3775abb45e/","content":"Mysql复制架构\n下面简要叙述下MySQL的各种复制模式，为了方便理解，假设有A、B、C三个MySQL实例，它们的复制模式有如下几种。\n\n □ 主从模式　 A→B □ 主主模式 　 A←→B □ 链式复制模式　A→B→C □ 环形复制模式　A→B→C→A\n 以上4种模式为复制的主要模式，生产中一般建议部署为主从模式，这也是最稳健的一种方式。为了方便切换，在一定程度上提高可用性，也可以选择主主模式。需要注意的是，主主模式必须确保任何时刻都只有一个数据库是主动（Active）状态，也就是说同一个时刻只能写入一个主（Master）节点，否则可能导致数据异常。链式或环形复制在生产中很少用到，它们的主要缺点在于，随着节点的增加，整个复制系统的稳健性会下降。后续运维章节（第12章）对复制会有更多的叙述。各种复制模式的基础都是主从模式，可以说，掌握了主从模式也就掌握了其他各种模式。\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql安装初始化","url":"/20241121/Mysql/5c41e25e2db3/","content":"Mysql安装初始化1.安装# 创建用户及用户组groupadd mysql# 用户 (用户名/密码)useradd -g mysql mysql# 授权chown -R mysql.mysql /data/mysql# 解开压缩包，修改目录名称为mysqlmv mysql-5.7.29-linux-glibc2.12-x86_64 mysqlcp /data/mysql/support-files/mysql.server /etc/init.d/mysqldecho &#x27;export PATH=/data/mysql/bin:$PATH&#x27; &gt;&gt;/etc/profilesource /etc/profile\n\n\n2.初始化mysqld --defaults-file=/etc/my.cnf --initializemysqld --initialize-insecure --user=mysql --basedir=/var/lib/mysql --datadir=/var/lib/mysql/datamysqld --user=mysql --basedir=/var/lib/mysql --datadir=/var/lib/mysql/data\n\n3.生成servicecat &gt; /lib/systemd/system/mysqld.service &lt;&lt;EOF[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlExecStart=/var/lib/mysql/bin/mysqldLimitNOFILE = 5000EOF\n\n\n4.配置文件cat &gt;/etc/my.cnf &lt;&lt;EOF[mysqld]user=mysqlbasedir=/var/lib/mysqldatadir=/var/lib/mysql/datasocket=/tmp/mysql.sock[mysql]socket=/tmp/mysql.sockprompt=mysql5091_db01 [\\\\d]&gt;[client]socket=/tmp/mysql.sockEOF\n\n5. 配置权限修改root密码跳过权限验证(可选)\n\n1.修改vim &#x2F;etc&#x2F;my.cnf增加skip-grant-tables（修改后记得注掉，重启mysql）\n2.mysqld_safe –user&#x3D;mysql –skip-grant-tables –skip-networking &amp;\n3.mysqld –user&#x3D;mysql –skip-grant-tables –skip-networking &amp;\n\nmysql -uroot -p-- 方式1update user SET Password=PASSWORD(&#x27;root@123&#x27;) where USER=&#x27;root&#x27;;FLUSH PRIVILEGES;-- 方式2update user SET authentication_string=PASSWORD(&#x27;root@123&#x27;) where USER=&#x27;root&#x27;;FLUSH PRIVILEGES;-- 方式3（8.0版本）alter user &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;Root@123&#x27;;FLUSH PRIVILEGES;alter user &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Root@123&#x27;;FLUSH PRIVILEGES;-- 方式4use mysql;set password=&#x27;Root@123&#x27;;FLUSH PRIVILEGES;\n\n7.创建用户create database &#x27;ideal&#x27; DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;create user &#x27;#userName&#x27;@&#x27;#host&#x27; identified by &#x27;#passWord&#x27;;create user &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;Root@123&#x27;;CREATE USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH &#x27;mysql_native_password&#x27; BY &#x27;Root@123&#x27;;FLUSH PRIVILEGES;\n\n\n8.授权root账户中的host项是localhost表示该账号只能进行本地登录，我们需要修改权限\n-- 方式1GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;Root@123&#x27; WITH GRANT OPTION;FLUSH PRIVILEGES;-- 方式2(8.0版本)GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; WITH GRANT OPTION;FLUSH PRIVILEGES;-- 方式3(8.0版本)GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;localhost&#x27; WITH GRANT OPTION;FLUSH PRIVILEGES;-- 方式4(8.0版本)use mysql;select user,authentication_string,host from user;update user set host = &#x27;%&#x27; where user = &#x27;root&#x27;;FLUSH PRIVILEGES;\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql客户端查询流程","url":"/20241121/Mysql/32159fcfd861/","content":"Mysql客户端查询流程\n客户端（Clients）发布查询的流程如下:\n\n首先连接MySQL（Connection Handling），然后发布查询，如果缓存（Query Cache）中有结果集，则直接返回结果集。如果结果没有被缓存，那么，MySQL解析查询（Parser）将通过优化器（Optimizer）生成执行计划，然后运行执行计划通过API（Pluggable Storage Engine API）从存储引擎获取数据，并返回给客户端\n什么是执行计划（查询计划）呢？执行计划就是一系列的操作步骤。SQL是声明性语言，它只告诉数据库要查询什么，但并不告诉数据库如何去查。数据库所要做的就是基于算法和统计信息计算出一条最佳的访问路径。这个工作是由优化器来完成的。优化器会比较不同的执行计划，然后选择其中最优的一套。\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql强化安全","url":"/20241121/Mysql/8a1aa817964a/","content":"mysql强化安全本节将描述一些常见的需要注意的安全问题，以及一些可以使 MySQL 安装更加安全的、防止黑客和误操作的措施。 强化安全的目的有如下三点。\n\n保护好 MySQL 主机的安全，同时也需要关注其他能访问数据库的主机的安全。\n确保 MySQL 自身的安全，包括生产库和备份，应使用强密码，尽可能分配最小的权限给用户。\n确保网络、物理的安全，同时也需要关注信息内容的保密。\n\n下面是一些安全的指导原则和注意事项。\n\n加强安全意识。比如加密办公电脑、个人笔记本上的重要数据，不要将未加密的数据上传到各种公共云存储中。在不安全的网络环境下，比如一些公共 Wi-Fi 中，涉及账号的操作可能会泄露你的信息。\n一般将所有数据库都部署于内网（仅监听内网 IP ），需要慎重对待跨 IDC 的数据库同步， MySQL 自身并没有很好的方式加密数据传输。\n开放外网访问的 MySQL 服务器，需要有相应的访问控制策略，例如通过部署防火墙来限制来源 IP 。\n如果条件允许，应该增加网络安全团队进行安全检查和审计。\n在不安全的网络环境中访问公司或远程维护机器，建议使用 VPN 。\n不要让任何人（除了 MySQL root 账户）访问 MySQL 数据库中的 mysql 系统库！\n用 GRANT 和 REVOKE 语句来控制对 MySQL 的访问。不要授予超过需求的权限。绝对不能为所有主机授权。\n不要给程序账号授予 SUPER 权限。\n生产库上不要留研发人员的账号。\n隔离生产环境、开发环境和测试环境，不允许研发、测试人员有权限更改生产环境或知道生产环境的账号密码。\n初始安装后应该移除匿名和空密码账号，可以尝试用 “mysql-u root” ，如果你能够成功连接服务器而没有要求 &#x2F; 输入任何密码，则说明有问题。\n不要将纯文本密码保存到数据库中，不要从字典中选择密码，如果你的程序是一个客户端，必须用可读的方式存储密码，那么建议使用可解码的加密办法来存储。一些工具，如 telnet 、 ftp ，使用的是明文传输密码，建议不要使用，使用 ssh 、 sftp是更安全的方式。\n使用更安全的算法加密密码，一些流行算法，如 MD5 已经被证明是弱加密，不适合用于加密密码。曾经比较流行的散列算法SHA-1 也被证明不够安全。推荐的方式是在将密码传入散列函数进行加密之前，将其和一个无意义的字符串拼接在一起，这样即使用户选择了一个在字典中存在的单词作为密码，攻击者也很难使用字典攻击的手段破解密码。\n试试从 Internet 上使用工具扫描端口，或者使用 shell 命令 shell&gt;telnet server_host 3306 ，如果得到连接并得到一些垃圾字符，则端口是打开着的，这种情况应从防火墙或路由器上关闭端口，除非你有足够合理的理由让它开着。\n避免 SQL 注入，不要信任应用程序的用户输入的任何数据。\n有时候人们会认为如果数据库只包含供公共使用的数据，则不需要保护。这是不正确的。即使允许显示数据库中的任何记录，也仍然应该保护和防范、拒绝服务攻击。\n不要向非管理用户授予 FILE 权限。拥有 FILE 权限的任何用户都能在拥有 mysqld 守护进程权限的文件系统里写入一个文件！\nFILE 权限也可以被用来读取任何作为运行服务器的 Unix 用户可读取或访问的文件。使用该权限，可以将任何文件读入数据库表。这可能会被滥用，例如，通过使用 LOAD DATA 装载 “&#x2F;etc&#x2F;passwd” 进入一个数据库表，然后就能用 SELECT 显示它。也可以考虑加密传输 HTTPS 和 SSH tunnnel 等方案，这些措施将会更安全，但成本比较高，实施起来往往会受制于其他因素。相对来说，从应用层做一些安全措施、在硬件防火墙中设置规则及 MySQL 权限控制则是更经济、更标准化的做法，总之，需要在安全和方便上达到一个平衡。研发人员、测试人员也有必要熟悉目前常用的一些攻击手段的原理和预防，如会话（ session ）劫持、中间人攻击、 SQL 注入、跨站脚本攻击等。\n\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql执行SQL文件的3种方式","url":"/20241121/Mysql/d2cb2385b770/","content":"Mysql执行SQL文件的3种方式执行SQL文件的3种方式如下:\nmysql -e &quot;source batch-file&quot;mysql -h host -u root -p &lt; batch-filemysql &gt; source /path/file-name;\n\n\n如果有长的屏幕输出，可以转储到文本或使用more进行查看。\nmysql &lt; batch-file | moremysql &lt; batch-file &gt; mysql.out\n\n导入速度因素:innodb_flush_log_at_trx_commit\nset GLOBAL innodb_flush_log_at_trx_commit = 0;\n1 默认值，最慢，每次事务提交都要写入log并刷新到磁盘上，这是最保险的方式0 最快，每隔1S将log刷新到磁盘，但是不保证。事务提交不会触发log写入。很不安全，mysql挂了，那么上一秒的数据就都丢了。2 折中的一种，事务提交会写入log，但是log刷新还是每秒一次，不保证。这种时候，就算mysql崩了，但是只要操作系统还在运转，数据还是会被写到磁盘上。\nsync_binlog &#x3D; 0\nset GLOBAL sync_binlog = 0;\n0 不刷新 binlog，也就是 mysql 只管写数据，至于数据啥时候刷新，由操作系统负责。1 每1次事务都要强制刷新，写入磁盘，可以看出，这是一种非常保险的方式，但是可想而知，性能会比较差，取决于存储介质的读写速度N，每 N 次事务强制刷新一次磁盘。设的大一些，可以得到一定的性能提升，但是遇到系统崩溃，会丢失 N 个事务的数据。\n另外一些可以设置的地方innodb_autoextend_increment 表空间自增值\ninnodb_log_buffer_size log 缓存区大小\ninnodb_log_file_size binlog 文件大小\nbulk_insert_buffer_size 批量写入的数据大小\n这些变量全都可以在 mysql 官方的文档里面查到，不同的版本有不同的默认值，最新版本的 mysql，这些变量的默认值已经非常大了，都是几十 MB 级别的，一般不会成为性能瓶颈。\n设置2个参数\nmysql&gt; set GLOBAL innodb_flush_log_at_trx_commit = 0;Query OK, 0 rows affected (0.00 sec)\n\nmysql&gt; set GLOBAL sync_binlog = 0;Query OK, 0 rows affected (0.00 sec)\n\n设置完进行导入\n\nThe End\n\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql的基础架构","url":"/20241121/Mysql/8b6ec2c228fc/","content":"mysql的基础架构MySQL是一种关系数据库产品。关系数据库，顾名思义，是建立在关系模型基础上的数据库。现实世界中，实体与实体之间的各种联系一般都可以用关系模型来表示。经过数十年的发展，关系数据库在理论和工业实践中都已经很成熟了。\n\n数据库产品的架构一般可以分为应用层、逻辑层、物理层，对于MySQL，同样可以理解为如下的3个层次。\n\n\n应用层。负责和客户端、用户进行交互，需要和不同的客户端和中间服务器进行交互，建立连接，记住连接的状态，响应它们的请求，返回数据和控制信息（错误信息、状态码等）。\n\n逻辑层。负责具体的查询处理、事务管理、存储管理、恢复管理，以及其他的附加功能。查询处理器负责查询的解析、执行。当接收到客户端的查询时，数据库就会分配一个线程来处理它。先由查询处理器（优化器）生成执行计划，然后交由计划执行器来执行，执行器有时需要访问更底层的事务管理器、存储管理器来操作数据，事务管理器、存储管理器主要负责事务管理、并发控制、存储管理。这其中，将由事务管理器来确保“ACID”特性，通过锁管理器来控制并发，由日志管理器来确保数据持久化，存储管理器一般还包括一个缓冲管理器，由它来确定磁盘和内存缓冲之间的数据传输。\n\n物理层。实际物理磁盘（存储）上的数据库文件，比如，数据文件、日志文件等。\n\n\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Mysql问题汇总","url":"/20241121/Mysql/4ac8a5b1ad38/","content":"mysql问题汇总1.Navicat连接mysql报1129错误错误:\nHost is blocked because of many connection errors; unblock with &#x27;mysqladmin flush-hosts&#x27;\n\n原因:\n同一个ip在短时间内产生太多（超过MySQL数据库max_connection_errors的最大值）中断的数据库连接而导致的阻塞；\n\n解决方案:\n1、提高允许的max_connection_errors数量（治标不治本）：　　① 进入Mysql数据库查看max_connection_errors： show variables like &#x27;%max_connection_errors%&#x27;;　  ② 修改max_connection_errors的数量为1000： set global max_connect_errors = 1000;　　③ 查看是否修改成功：show variables like &#x27;%max_connection_errors%&#x27;;2、使用mysqladmin flush-hosts 命令清理一下hosts文件（不知道mysqladmin在哪个目录下可以使用命令查找：whereis mysqladmin）；　　① 在查找到的目录下使用命令修改：/usr/bin/mysqladmin flush-hosts -h192.168.1.1 -P3308 -uroot -prootpwd;　　备注：1.其中端口号，用户名，密码都可以根据需要来添加和修改；　　　　  2.配置有master/slave主从数据库的要把主库和从库都修改一遍的（我就吃了这个亏明明很容易的几条命令结果折腾了大半天）；　　　　  3.第二步也可以在数据库中进行，命令如下：flush hosts;\n终极解决方案:\n大多数的百度结果是上面这样的,但对于某些特定的机子不行,经多次验证,发现需要加上主机地址，即执行以下命令:\nmysqladmin flush-hosts -h 127.0.0.1 -uroot -p123456\n备注：‘-h’,‘-u’和‘-p’后面跟的是访问的主机ip地址、用户名和密码，中间不需要空格\n2.mysql8.0登录提示caching_sha2_password错误:\nAuthentication plugin &#x27;caching_sha2_password&#x27; cannot be loaded: dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image not foundmysqli_real_connect(): The server requested authentication method unknown to the client [caching_sha2_password]mysqli_real_connect(): (HY000/2054): The server requested authentication method unknown to the client\n\n原因:\n在mysql8之前的版本使用的密码加密规则是mysql_native_password，但是在mysql8则是caching_sha2_password\n\n解决方案1:\n配置 mysql.cnf 配置默认身份验证插件[mysqld]default_authentication_plugin = mysql_native_password\n\n解决方案2:\n查看身份验证类型\nmysql&gt; use mysql;Database changedmysql&gt; SELECT Host, User, plugin from user;+-----------+------------------+-----------------------+| Host      | User             | plugin                |+-----------+------------------+-----------------------+| %         | root             | caching_sha2_password || localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session    | caching_sha2_password || localhost | mysql.sys        | caching_sha2_password || localhost | root             | caching_sha2_password |+-----------+------------------+-----------------------+5 rows in set (0.00 sec)\n\nroot 用户的验证器插件为 caching_sha2_password\n创建一个新用户，并指定加密规则为mysql_native_password\nCREATE USER  &#x27;your username&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;your password;\n\n或者修改身份验证类型(修改密码)\nmysql&gt; ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;Query OK, 0 rows affected (0.00 sec)mysql&gt; ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;Query OK, 0 rows affected (0.01 sec)mysql&gt; FLUSH PRIVILEGES;\n\n验证是否生效\nmysql&gt; SELECT Host, User, plugin from user;+-----------+------------------+-----------------------+| Host      | User             | plugin                |+-----------+------------------+-----------------------+| %         | root             | mysql_native_password || localhost | mysql.infoschema | caching_sha2_password || localhost | mysql.session    | caching_sha2_password || localhost | mysql.sys        | caching_sha2_password || localhost | root             | mysql_native_password |+-----------+------------------+-----------------------+5 rows in set (0.00 sec)\n\n3.mysql提示：Statement violates GTID consistency错误:\n错误代码： 1786Statement violates GTID consistency: CREATE TABLE ... SELECT.\n\n解决方案:\n# vi /etc/my.cnfgtid_mode = ONenforce_gtid_consistency = ON# 改为gtid_mode = OFFenforce_gtid_consistency = OFF# 要两个参数一起改,然后重启mysql\n\n4.mysql提示:Can’t find error-message file’&#x2F;usr&#x2F;share&#x2F;mysql&#x2F;errmsg.sys错误:\n[ERROR] Can&#x27;t find error-message file &#x27;/usr/share/mysql/errmsg.sys&#x27;. Check error-message file location and &#x27;lc-messages-dir&#x27; configuration directive.\n\n解决方案1:\nvi /etc/my.cnf# 添加以下参数basedir=/data/mysql\nfind / -name errmsg.sys# 查找errmsg.sys文件位置# /var/lib/mysql5.7/share/bulgarian/errmsg.sys# /var/lib/mysql5.7/share/czech/errmsg.sys# /var/lib/mysql5.7/share/danish/errmsg.sys# /var/lib/mysql5.7/share/dutch/errmsg.sys# /var/lib/mysql5.7/share/english/errmsg.sys# /var/lib/mysql5.7/share/estonian/errmsg.sys# ******# 将errmsg.sys的english版本文件拷到mysql相关路径下，路径不存在需要创建mkdir -p /data/mysql/share/mysql/cp /var/lib/mysql5.7/share/english/errmsg.sys /data/mysql/share/mysql/errmsg.sys\n\n解决方案2:\nfind / -name errmsg.sys# 查找errmsg.sys文件位置# /var/lib/mysql5.7/share/bulgarian/errmsg.sys# /var/lib/mysql5.7/share/czech/errmsg.sys# /var/lib/mysql5.7/share/danish/errmsg.sys# /var/lib/mysql5.7/share/dutch/errmsg.sys# /var/lib/mysql5.7/share/english/errmsg.sys# /var/lib/mysql5.7/share/estonian/errmsg.sys# ******# 将errmsg.sys的english版本文件拷到mysql相关路径下，路径不存在需要创建mkdir -p /data/mysql/share/mysql/cp /var/lib/mysql5.7/share/english/errmsg.sys /data/mysql/share/mysql/errmsg.sys\n\nvi /etc/my.cnf# 添加以下参数，同时去掉basedir=/data/mysql参数lc-messages-dir=/data/mysql/share/mysql/errmsg.sys\n\n5.mysql8.0+版本提示Account is locked（用户或表格上锁）原因:\n操作Mysql数据库的时候难免会遇到锁住用户登录不进去的情况发生（一般是密码输错很多次，我是学习的时候手动修改了）。实质是account_locked栏位变为Y了，导致后台文件卡控无法登录。\n\n解决方案1:\n# 查询是否上锁，登录不了直接解锁select host,user,account_locked from mysql.user \n\n# 跳过权限管控，启动mysqlmysqld --console --skip-grant-tables --shared-memory# 或者在my.cnf中在[mysqld]下添加skip-grant-tables\n\n再次连接一个窗口进行mysql连接\n# 解除语句ALTER USER ‘root’@‘localhost’ ACCOUNT UNLOCK;# 双重解除update mysql.user set account_locked=‘N’ where user=‘root’;# 提交，很重要的步骤commit;# 最终查询下是否解除成功select user,host,account_locked from mysql.user;\n\n最后重启服务\n# 关掉之前mysqld --console --skip-grant-tables --shared-memory启动的服务，重新正常启动。# 或者去掉my.cnf中在[mysqld]下的skip-grant-tables重启mysqld服务。\n\n6.mysql提示:Error starting thread: Resource temporarily unavailable错误:\n121031 18:53:17  InnoDB: Unable to open the first data fileInnoDB: Error in opening ./ibdata1121031 18:53:17  InnoDB: Operating system error number 11 in a file operation.InnoDB: Error number 11 means &#x27;Resource temporarily unavailable&#x27;.InnoDB: Some operating system error numbers are described atInnoDB: http://dev.mysql.com/doc/refman/5.5/en/operating-system-error-codes.html121031 18:53:17 InnoDB: Could not open or create data files.121031 18:53:17 InnoDB: If you tried to add new data files, and it failed here,121031 18:53:17 InnoDB: you should now edit innodb_data_file_path in my.cnf back121031 18:53:17 InnoDB: to what it was, and remove the new ibdata files InnoDB created 121031 18:53:17 InnoDB: in this failed attempt. InnoDB only wrote those files full of 121031 18:53:17 InnoDB: zeros, but did not yet use them in any way. But be careful: do not121031 18:53:17 InnoDB: remove old data files which contain your precious data!121031 18:53:17 [ERROR] Plugin &#x27;InnoDB&#x27; init function returned error.121031 18:53:17 [ERROR] Plugin &#x27;InnoDB&#x27; registration as a STORAGE ENGINE failed. 121031 18:53:17 [ERROR] Unknown/unsupported storage engine: INNODB121031 18:53:17 [ERROR] Aborting 121031 18:53:17 [Note] /usr/local/mysql/bin/mysqld: Shutdown complete 121031 18:53:17 mysqld_safe mysqld from pid file /data/mysql/mysql_3301/mysql_3301.pid ended121031 18:54:29 mysqld_safe Starting mysqld daemon with databases from /data/mysql/mysql_3301121031 18:54:29 [Warning] The syntax &#x27;--log-slow-queries&#x27; is deprecated and will be removed in a future release. Please use &#x27;--slow-query-log&#x27;/&#x27;--slow-query-log-file&#x27; instead.121031 18:54:29 InnoDB: The InnoDB memory heap is disabled121031 18:54:29 InnoDB: Mutexes and rw_locks use GCC atomic builtins121031 18:54:29 InnoDB: Compressed tables use zlib 1.2.3121031 18:54:30 InnoDB: Initializing buffer pool, size = 2.9G121031 18:54:30 InnoDB: Completed initialization of buffer poolInnoDB: Unable to lock ./ibdata1, error: 11InnoDB: Check that you do not already have another mysqld processInnoDB: using the same InnoDB data or log files.121031 18:54:30  InnoDB: Retrying to lock the first data fileInnoDB: Unable to lock ./ibdata1, error: 11InnoDB: Check that you do not already have another mysqld processInnoDB: using the same InnoDB data or log files.121031 18:54:30  InnoDB: Retrying to lock the first data fileInnoDB: Unable to lock ./ibdata1, error: 11InnoDB: Check that you do not already have another mysqld processInnoDB: using the same InnoDB data or log files.InnoDB: Unable to lock ./ibdata1, error: 11InnoDB: Check that you do not already have another mysqld processInnoDB: using the same InnoDB data or log files.InnoDB: Unable to lock ./ibdata1, error: 11\n\n解决方案\n发现是”max user processes “参数有问题，通过root用户调整大小至 12000,线程数也随着增大。\nvi /etc/security/limits.conf* soft nproc 12000* hard nproc 12000# 重新加载系统参数sysctl -p  \n\nnproc就是”max user processes”，完整描述是: nproc - max number of processes\n参数含义:\n单个用户可以启动的线程数，因为进程也会启动一个线程，所以也间接对进程数有限制。\n注意：\n该参数只对普通用户有用，root用户不在此限制。 所以用root用户可以启动几万个线程，无法重现这个问题.\nEnd\n","categories":["Mysql"],"tags":["Mysql"]},{"title":"Nginx常用功能","url":"/20241121/Nginx/52c6aaae5e9a/","content":"Nginx一.Nginx常用功能1.Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理。这里我给来2张图，对正向代理与反响代理做个诠释，具体细节，大家可以翻阅下资料。\nNginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的走文件服务器，动态页面走web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。\n2、负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。\n\nIp hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。\n\n3、web缓存Nginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。\n4、Nginx相关地址源码：https://trac.nginx.org/nginx/browser\n官网：http://www.nginx.org/\n二.Nginx配置文件结构如果你下载好啦，你的安装文件，不妨打开conf文件夹的nginx.conf文件，Nginx服务器的基础配置，默认的配置也存放在此。\n在 nginx.conf 的注释符号为： #\n默认的 nginx 配置文件 nginx.conf 内容如下：\n#user  nobody;##定义拥有和运行Nginx服务的Linux系统用户worker_processes  1;##定义单进程。通常将其设成CPU的个数或者内核数#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;##定义Nginx在哪里打日志#pid        logs/nginx.pid;##Nginx写入主进程ID（PID）events &#123;    worker_connections  1024;    ##通过worker_connections和worker_processes计算maxclients。    ##max_clients = worker_processes * worker_connections&#125;http &#123;    include       mime.types;    ##在/opt/nginx/conf/mime.types写的配置将在http模块中解析        default_type  application/octet-stream;    #log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;    #                  &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;    #                  &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    #access_log  logs/access.log  main;    sendfile        on;    ##如果是为了获取本地存储的静态化文件，sendfile可以加速服务端，但是如果是反向代理，那么该功能就失效了。    #tcp_nopush     on;    ##在 nginx 中，tcp_nopush 配置和 tcp_nodelay &quot;互斥&quot;。它可以配置一次发送数据的包大小。也就是说，    ##它不是按时间累计  0.2 秒后发送包，而是当包累计到一定大小后就发送。在 nginx 中，tcp_nopush 必须和sendfile 搭配使用。    #keepalive_timeout  0;    keepalive_timeout  65;    ##设置保持客户端连接时间    #gzip  on;    ##告诉服务端用gzip压缩    server &#123;        ##如果你想对虚拟主机进行配置，可以在单独的文件中配置server模块，然后include进来        listen       8080;        ##告诉Nginx TCP端口，监听HTTP连接。listen 80; 和 listen *:80;是一样的        server_name  localhost;        ##定义虚拟主机的名字        #charset koi8-r;        #access_log  logs/host.access.log  main;        location / &#123;        ##location模块可以配置nginx如何反应资源请求            root   html;            index  index.html index.htm;        &#125;        #error_page  404              /404.html;        # redirect server error pages to the static page /50x.html        #        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;        # proxy the PHP scripts to Apache listening on 127.0.0.1:80        #        #location ~ \\.php$ &#123;        #    proxy_pass   http://127.0.0.1;        #&#125;        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000        #        #location ~ \\.php$ &#123;        #    root           html;        #    fastcgi_pass   127.0.0.1:9000;        #    fastcgi_index  index.php;        #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;        #    include        fastcgi_params;        #&#125;        # deny access to .htaccess files, if Apache&#x27;s document root        # concurs with nginx&#x27;s one        #        #location ~ /\\.ht &#123;        #    deny  all;        #&#125;    &#125;    # another virtual host using mix of IP-, name-, and port-based configuration    #    #server &#123;    #    listen       8000;    #    listen       somename:8080;    #    server_name  somename  alias  another.alias;    #    location / &#123;    #        root   html;    #        index  index.html index.htm;    #    &#125;    #&#125;    # HTTPS server    #    #server &#123;    #    listen       443 ssl;    #    server_name  localhost;    #    ssl_certificate      cert.pem;    #    ssl_certificate_key  cert.key;    #    ssl_session_cache    shared:SSL:1m;    #    ssl_session_timeout  5m;    #    ssl_ciphers  HIGH:!aNULL:!MD5;    #    ssl_prefer_server_ciphers  on;    #    location / &#123;    #        root   html;    #        index  index.html index.htm;    #    &#125;    #&#125;    include servers/*;&#125;\n\n\n虽然上面的默认配置很多，但是可以总体归纳为三个模块：\n...              #全局块events &#123;         #events块   ...&#125;http      #http块&#123;    ...   #http全局块    server        #server块    &#123;         ...       #server全局块        location [PATTERN]   #location块        &#123;            ...        &#125;        location [PATTERN]         &#123;            ...        &#125;    &#125;    server    &#123;      ...    &#125;    ...     #http全局块&#125;\n\n\n1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。\n2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。\n3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。\n4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。\n5、location块：配置请求的路由，以及各种页面的处理情况。\n\n下面给大家上2个配置文件，作为理解：\n配置一\n########### 每个指令必须有分号结束。##################user administrator administrators;  #配置用户或者组，默认为nobody nobody。#worker_processes 2;  #允许生成的进程数，默认为1#pid /nginx/pid/nginx.pid;   #指定nginx进程运行文件存放地址error_log log/error.log debug;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emergevents &#123;    accept_mutex on;   #设置网路连接序列化，防止惊群现象发生，默认为on    multi_accept on;  #设置一个进程是否同时接受多个网络连接，默认为off    #use epoll;      #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport    worker_connections  1024;    #最大连接数，默认为512&#125;http &#123;    include       mime.types;   #文件扩展名与文件类型映射表    default_type  application/octet-stream; #默认文件类型，默认为text/plain    #access_log off; #取消服务日志        log_format myFormat &#x27;$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for&#x27;; #自定义格式    access_log log/access.log myFormat;  #combined为日志格式的默认值    sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。    keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。    upstream mysvr &#123;         server 127.0.0.1:7878;      server 192.168.10.121:3333 backup;  #热备    &#125;    error_page 404 https://www.baidu.com; #错误页    server &#123;        keepalive_requests 120; #单连接请求上限次数。        listen       4545;   #监听端口        server_name  127.0.0.1;   #监听地址               location  ~*^.+$ &#123;       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。           #root path;  #根目录           #index vv.txt;  #设置默认页           proxy_pass  http://mysvr;  #请求转向mysvr 定义的服务器列表           deny 127.0.0.1;  #拒绝的ip           allow 172.18.5.54; #允许的ip                   &#125;     &#125;&#125;\n\n配置二(基础转发)\nserver &#123;    listen 80;    server_name **.106.2**.175;    location / &#123;            root   /public/app/dist;            index  index.php index.html index.htm;    &#125;    location /sell &#123;        proxy_set_header   X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header   Host      $http_host;        proxy_set_header X-NginX-Proxy true;        proxy_pass         http://127.0.0.1:8080;        proxy_redirect off;    &#125;&#125;\n\n上面是nginx的基本配置，需要注意的有以下几点：\n1.几个常见配置项：\n\n1.$remote_addr 与 $http_x_forwarded_for 用以记录客户端的ip地址；\n2.$remote_user ：用来记录客户端用户名称；\n3.$time_local ： 用来记录访问时间与时区；\n4.$request ： 用来记录请求的url与http协议；\n5.$status ： 用来记录请求状态；成功是200；\n6.$body_bytes_s ent ：记录发送给客户端文件主体内容大小；\n7.$http_referer ：用来记录从那个页面链接访问过来的；\n8.$http_user_agent ：记录客户端浏览器的相关信息；2.惊群现象：一个网路连接到来，多个睡眠的进程被同时叫醒，但只有一个进程能获得链接，这样会影响系统性能。\n\n3.每个指令必须有分号结束。\n三.Nginx配置Web服务器先介绍对一个web服务进行简单配置，然后对各个重要点简单说明。这个案例中关于反向代理的要点将在下一篇中介绍。\n案列\n########### 每个指令必须有分号结束。##################user administrator administrators;  #配置用户或者组，默认为nobody nobody。#worker_processes 2;  #允许生成的进程数，默认为1#pid /nginx/pid/nginx.pid;   #指定nginx进程运行文件存放地址error_log log/error.log debug;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，#级别以此为：debug|info|notice|warn|error|crit|alert|emergevents &#123;    accept_mutex on;   #设置网路连接序列化，防止惊群现象发生，默认为on    multi_accept on;  #设置一个进程是否同时接受多个网络连接，默认为off    #use epoll;      #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport    worker_connections  1024;    #最大连接数，默认为512&#125;http &#123;    include       mime.types;   #文件扩展名与文件类型映射表    default_type  application/octet-stream; #默认文件类型，默认为text/plain    #access_log off; #取消服务日志        log_format myFormat &#x27;$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for&#x27;; #自定义格式    access_log log/access.log myFormat;  #combined为日志格式的默认值    sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。    keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。    upstream mysvr &#123;         server 127.0.0.1:7878;      server 192.168.10.121:3333 backup;  #热备    &#125;    error_page 404 https://www.baidu.com; #错误页        server &#123;        keepalive_requests 120; #单连接请求上限次数。        listen       4545;   #监听端口        server_name  127.0.0.1;   #监听地址               location  ~*^.+$ &#123;       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。           #root path;  #根目录           #index vv.txt;  #设置默认页           proxy_pass  http://mysvr;  #请求转向mysvr 定义的服务器列表           deny 127.0.0.1;  #拒绝的ip           allow 172.18.5.54; #允许的ip                   &#125;     &#125;&#125; \n\n\n域名与端口配置#上述例子中 listen 4545; #监听端口 表示监听端口是4545。#但是对于一个小白来说有时候看到 listen [::]:80;,listen :80;,listen *:80; #这三种写法还是会很懵逼的，那么他们之间有什么区别啊？listen [::]:80;表示Nginx会同时监听IPv4和IPv6的80端口，listen :80;,listen *:80; 这两种写法是一样的\n\nlocation中URL匹配上述例子中，大家发现location 后面跟着的正则匹配，其实在nginx中，location url 匹配是遵循一定优先级的。\nlocation = / &#123;    # 完全匹配  =    # 大小写敏感 ~    # 忽略大小写 ~*&#125;location ^~ /images/ &#123;    # 前半部分匹配 ^~     # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。&#125;location ~* \\.(gif|jpg|jpeg)$ &#123;    # ~* 表示执行一个正则匹配，不区分大小写    # ~ 表示执行一个正则匹配，区分大小写    # 匹配所有以 gif,jpg或jpeg 结尾的请求&#125;location / &#123;    # 如果以上都未匹配，会进入这里&#125;\n\nlocation中的优先级如下:\n\n(location &#x3D;) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ,* 正则顺序) &gt; (location 部分起始路径) &gt; (&#x2F;)\n\nlocation = / &#123;#仅仅匹配请求[ configuration A ]&#125;location / &#123;#匹配所有以 / 开头的请求。但是如果有更长的同类型的表达式，则选择更长的表达式。#如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration B ]&#125;location /documents/ &#123;# 匹配所有以 /documents/ 开头的请求。但是如果有更长的同类型的表达式，则选择更长的表达式。#如果有正则表达式可以匹配，则优先匹配正则表达式。[ configuration C ]&#125;location ^~ /images/ &#123;# 匹配所有以 /images/ 开头的表达式，如果匹配成功，则停止匹配查找。所以，即便有符合的正则表达式location，也# 不会被使用[ configuration D ]&#125;location ~* \\.(gif|jpg|jpeg)$ &#123;# 匹配所有以 gif jpg jpeg结尾的请求。但是 以 /images/开头的请求，将使用 Configuration D[ configuration E ]&#125;\n\n文件路径定义在location模块中可以定义文件路径，比如:\n根目录设置：\nlocation / &#123;    root /home/barret/test/;&#125;\n主页设置：\nindex /html/index.html /php/index.php;\n\ntry_files 设置try_file主要是功能是去检查文件是否存在，使用第一个被找到文件返回。如果没有一个文件找到, 那么重定向到最后一个参数指定的URI。如：\nlocation /images/ &#123;    try_files $uri /images/default.gif;&#125;location = /images/default.gif &#123;    expires 30s;&#125;\n\nps: $uri 是不带请求参数的当前URI，下面的全局变量中会介绍,最后一个参数也可以是命名的location。如下：\ntry_files $uri $uri.html $uri/index.html @other;location @other &#123;    # 尝试寻找匹配 uri 的文件，失败了就会转到上游处理    proxy_pass  http://localhost:9000;&#125;location / &#123;    # 尝试寻找匹配 uri 的文件，没找到直接返回 502    try_files $uri $uri.html =502;&#125;\n\nRewrite 重定向如果要把一个URL http://www.jianshu.com/users/10001 重写成 http://www.jianshu.com/show?user=10001，可以使用rewrite 规则，参见下面的代码。我在公司站点的改造过程中，遇到了rewrite，重写URL目的是为了更好的SEO。\nlocation /users/ &#123;    rewrite ^/users/(.*)$ /show?user=$1 break;&#125;\n\nrewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite 规则 定向路径 重写类型; \n1、规则：可以是字符串或者正则来表示想匹配的目标url2、定向路径：表示匹配到规则后要定向的路径，如果规则里有正则，则可以使用$index来表示正则里的捕获分组3、重写类型：last ：相当于Apache里德(L)标记，表示完成rewrite，浏览器地址栏URL地址不变break；本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址permanent：返回301永久重定向，浏览器地址栏会显示跳转后的URL地址\nbreak 与 last的区别\nlast一般写在server和if中，而break一般使用在location中\nlast不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配\nbreak和last都能组织继续执行后面的rewrite指令在location里一旦返回break则直接生效并停止后续的匹配location举个例子：server &#123;    location / &#123;        rewrite /last/ /q.html last;        rewrite /break/ /q.html break;    &#125;    location = /q.html &#123;        return 400;    &#125;&#125;\n\n访问&#x2F;last&#x2F;时重写到&#x2F;q.html，然后使用新的uri再匹配，正好匹配到locatoin &#x3D; &#x2F;q.html然后返回了400访问&#x2F;break时重写到&#x2F;q.html，由于返回了break，则直接停止了\nif表达式上面的简单重写很多时候满足不了需求，比如需要判断当文件不存在时、当路径包含xx时等条件，则需要用到ifif的语法如下：\nif (表达式) &#123;&#125;\n\n&#125;\n\n内置的全局变量：\n$args ：这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。\n\n内置的条件判断：\n-f和!-f用来判断是否存在文件-d和!-d用来判断是否存在目录-e和!-e用来判断是否存在文件或目录-x和!-x用来判断文件是否可执行\n有时候在配置文件中看到$http_host。他和$host有什么不同呢？\n $http_host和$host都是原始的’HOST’字段比如请求的时候HOST的值是www.csdn.net 那么反代后还是www.csdn.net如果客户端发过来的请求的header中没有有’HOST’这个字段时，建议使用$host，这时候的$host就等于server_name。\nif 表达式例子：\n# 如果文件不存在则返回400if (!-f $request_filename) &#123;    return 400;&#125;# 如果host不是xuexb.com，则301到xuexb.com中if ( $host != &#x27;xuexb.com&#x27; )&#123;    rewrite ^/(.*)$ https://xuexb.com/$1 permanent;&#125;# 如果请求类型不是POST则返回405if ($request_method = POST) &#123;    return 405;&#125;# 如果参数中有 a=1 则301到指定域名if ($args ~ a=1) &#123;    rewrite ^ http://example.com/ permanent;&#125;\n\nif 通常与location规则搭配使用，如：\n# 访问 /test.html 时location = /test.html &#123;    # 默认值为xiaowu    set $name xiaowu;    # 如果参数中有 name=xx 则使用该值    if ($args ~* name=(\\w+?)(&amp;|$)) &#123;        set $name $1;    &#125;    # 301    rewrite ^ /$name.html permanent;&#125;\n上面表示：\n&#x2F;test.html &#x3D;&gt; &#x2F;xiaowu.html&#x2F;test.html?name&#x3D;ok &#x3D;&gt; &#x2F;ok.html?name&#x3D;ok\n完\n本文来自：菜鸟教程 https://www.runoob.com/w3cnote/nginx-setup-intro.html简书:樂浩beyond https://www.jianshu.com/p/734ef8e5a712\n","categories":["Nginx"],"tags":["Nginx"]},{"title":"Nginx响应头安全策略","url":"/20241121/Nginx/c2240fee007d/","content":"Nginx响应头安全策略1.add_header X-Content-Type-Options 应对漏洞：内容嗅探攻击,屏蔽内容嗅探攻击。2.add_header X-XSS-Protection 应对漏洞：XSS攻击,开启浏览器XSS防护3.add_header X-Frame-Options 应对漏洞：点击劫持;配置的三个参数：deny 标识该页面不允许在frame中展示，即便在相同域名的页面中嵌套也不行。sameorigin 可以在同域名的页面中frame中展示allow-form url 指定的fream中展示。4.add_header Strict-Transport-Security 告诉浏览器只能通过https访问当前资源,在接下来的16070400秒中，浏览器只要向xxx或其子域名发送HTTP请求时，必须采用HTTPS来发起连接。5.add_header Referrer-Policy 用于过滤 Referrer 报头内容，其可选的项有：no-referrer no-referrer-when-downgrade origin origin-when-cross-origin same-origin strict-origin6.add_header X-Permitted-Cross-Domain-Policies7.add_header X-Download-Options用于控制浏览器下载文件是否支持直接打开，如果支持直接打开，可能会有安全隐患。8.add_header Access-Control-Allow-相关配置 对应漏洞：Access-Control-Allow-Origin中不安全的通配符’*’响应【原理扫描】\n\nnginx.conf示例\n\nworker_processes  auto; error_log  /var/log/nginx/error.log notice;pid        /var/run/nginx.pid;  events &#123;    worker_connections  1024;&#125;  http &#123;    include       /etc/nginx/mime.types;    default_type  application/octet-stream;     log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;     access_log  /var/log/nginx/access.log  main;     sendfile        on;    #tcp_nopush     on;     keepalive_timeout  65;     #gzip  on;        server &#123;            listen       80;            server_name  localhost;                    gzip on;                    gzip_buffers 32 4K;                    gzip_comp_level 6;                    gzip_min_length 100;                    gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;                    gzip_disable &quot;MSIE [1-6]\\.&quot;; #配置禁用gzip条件，支持正则。此处表示ie6及以下不启用gzip（因为ie低版本不支持）                    gzip_vary on;            access_log  /var/log/nginx/host.access.log  main;            error_log  /var/log/nginx/error.log  error;          location / &#123;            root    html;            index  index.html index.htm;                         add_header X-Content-Type-Options &quot;nosniff&quot; always;            add_header X-XSS-Protection &quot;1; mode=block&quot; always;            add_header X-Frame-Options SAMEORIGIN;            add_header Strict-Transport-Security &quot;max-age=16070400; includeSubdomains; preload&quot; always;            add_header Referrer-Policy &quot;strict-origin&quot;;            add_header X-Permitted-Cross-Domain-Policies none;            add_header X-Download-Options noopen;                      add_header Access-Control-Allow-Origin *;            add_header Access-Control-Allow-Methods &#x27;GET, POST, OPTIONS&#x27;;            add_header Access-Control-Allow-Headers &#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&#x27;;            if ($request_method = &#x27;OPTIONS&#x27;) &#123;              return 204;            &#125;        &#125;         location /api/ &#123;            # 426 Upgrade Required，使用 proxy_http_version 1.1            proxy_http_version 1.1;            # !!需要修改为后台服务地址            proxy_pass  http://192.168.1.11:5002$request_uri; #API            # proxy_redirect off;            # 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP            proxy_set_header  Host  $host;            proxy_set_header  X-Real-IP  $remote_addr;             proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;            #proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;                         add_header X-Content-Type-Options &quot;nosniff&quot; always;            add_header X-XSS-Protection &quot;1; mode=block&quot; always;            add_header X-Frame-Options SAMEORIGIN;            add_header Strict-Transport-Security &quot;max-age=16070400; includeSubdomains; preload&quot; always;            add_header Referrer-Policy &quot;strict-origin&quot;;            add_header X-Permitted-Cross-Domain-Policies none;            add_header X-Download-Options noopen;            add_header Access-Control-Allow-Origin *;            add_header Access-Control-Allow-Methods &#x27;GET, POST, OPTIONS&#x27;;            add_header Access-Control-Allow-Headers &#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&#x27;;            if ($request_method = &#x27;OPTIONS&#x27;) &#123;              return 204;            &#125;        &#125;                                 location /ws &#123;            proxy_pass http://127.0.0.1:5052$request_uri;            proxy_http_version 1.1;            proxy_connect_timeout 4s;                           proxy_read_timeout 60s;                             proxy_send_timeout 12s;                             proxy_set_header Host $host;            proxy_set_header X-Real-IP $remote_addr;            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;            proxy_set_header Upgrade $http_upgrade;            proxy_set_header Connection &quot;Upgrade&quot;;                         add_header X-Content-Type-Options &quot;nosniff&quot; always;            add_header X-XSS-Protection &quot;1; mode=block&quot; always;            add_header X-Frame-Options SAMEORIGIN;            add_header Strict-Transport-Security &quot;max-age=16070400; includeSubdomains; preload&quot; always;            add_header Referrer-Policy &quot;strict-origin&quot;;            add_header X-Permitted-Cross-Domain-Policies none;            add_header X-Download-Options noopen;                      add_header Access-Control-Allow-Origin *;            add_header Access-Control-Allow-Methods &#x27;GET, POST, OPTIONS&#x27;;            add_header Access-Control-Allow-Headers &#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&#x27;;            if ($request_method = &#x27;OPTIONS&#x27;) &#123;              return 204;            &#125;        &#125;                #error_page  404              /404.html;                         # redirect server error pages to the static page /50x.html            #            error_page   500 502 503 504  /50x.html;            location = /50x.html &#123;                root   /usr/share/nginx/html;                add_header Cache-Control &#x27;no-cache, must-revalidate, proxy-revalidate, max-age=0&#x27;;            &#125;        &#125;&#125;\n\n\n来自带码人来自林老西来自Developer\n","categories":["Nginx"],"tags":["Nginx"]},{"title":"Nginx实现网站https","url":"/20241121/Nginx/d09dafa1d928/","content":"一.生成秘钥\n如果提供了则不需要生成\n\n使用centos系统生成秘钥\n1. 使用openssl生成密钥privkey.pem：openssl genrsa -out privkey.pem 1024/2038\n2. 使用密钥生成证书server.pem：openssl req -new -x509 -key privkey.pem -out server.pem -days 3650\n个人网站为例Common Name (e.g. server FQDN or YOUR name) []: ceshi-test.com也可以通过*.yourdomain.com来匹配你的二级域名\nYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [XX]:CN                         \t              #国家名称State or Province Name (full name) []:ShangHai           \t \t      #省Locality Name (eg, city) [Default City]:ShangHai         \t\t      #市Organization Name (eg, company) [Default Company Ltd]:ACBC                    #公司Organizational Unit Name (eg, section) []:Tech                                #部门Common Name (eg, your name or your server&#x27;s hostname) []:*.mydomain.com       #注意，此处应当填写你要部署的域名，如果是单个则直接添加即可，如果不确定，使用*，表示可以对所有mydomain.com的子域名做认证Email Address []:admin@mydomain.com                                           #以域名结尾即可Please enter the following &#x27;extra&#x27; attributesto be sent with your certificate request A challenge password []:        \t\t\t\t\t\t\t\t\t\t\t   #是否设置密码，可以不写直接回车  An optional company name []:                                                   #其他公司名称 可不写\n\n\n二.安装nginx1.windows版直接下载绿色版https://nginx.org/en/download.html\n\n2.centos版安装(1)在线安装\nyum install -y nginx\n(2)离线下载\nhttps://nginx.org/en/download.html\n\n三.配置nginxvi /etc/nginx/nginx.conf\n\n  编辑server下内容\n# For more information on configuration, see:#   * Official English Documentation: http://nginx.org/en/docs/#   * Official Russian Documentation: http://nginx.org/ru/docs/#user nginx;#worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;events &#123;    worker_connections 1024;&#125;http &#123;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    access_log  /var/log/nginx/access.log  main;    sendfile            on;    tcp_nopush          on;    tcp_nodelay         on;    keepalive_timeout   65;    types_hash_max_size 2048;    include             /etc/nginx/mime.types;    default_type        application/octet-stream;    include /etc/nginx/conf.d/*.conf;    server &#123;        listen       443;        server_name  cesshi-test.com;        ssl_certificate /data/nginx/pki/server.pem;        ssl_certificate_key  /data/nginx/pki/privkey.pem;        ssl_session_timeout     5m; #会话超时时间        ssl_ciphers     ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #加密算法        ssl_protocols   TLSv1 TLSv1.1 TLSv1.2; #SSL协议        #root         /usr/share/nginx/html;        # Load configuration files for the default server block.        include /etc/nginx/default.d/*.conf;       location / &#123;            proxy_pass  http://192.168.11:8080;                   proxy_redirect     off;                  proxy_set_header   Host             $host;                         proxy_set_header   X-Real-IP        $remote_addr;                         proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;                       client_max_body_size       100m;                        index  index.html index.htm;            add_header X-Content-Type-Options &quot;nosniff&quot; always;            add_header X-XSS-Protection &quot;1; mode=block&quot; always;            add_header X-Frame-Options SAMEORIGIN;            add_header Strict-Transport-Security &quot;max-age=16070400; includeSubdomains; preload&quot; always;            add_header Referrer-Policy &quot;strict-origin&quot;;            add_header X-Permitted-Cross-Domain-Policies none;            add_header X-Download-Options noopen;            add_header Access-Control-Allow-Origin *;            add_header Access-Control-Allow-Methods &#x27;GET, POST, OPTIONS&#x27;;            add_header Access-Control-Allow-Headers &#x27;DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization&#x27;;            if ($request_method = &#x27;OPTIONS&#x27;) &#123;              return 204;            &#125;          &#125;        error_page 404 /404.html;            location = /40x.html &#123;        &#125;        error_page 500 502 503 504 /50x.html;            location = /50x.html &#123;        &#125;    &#125;# Settings for a TLS enabled server.    server &#123;        listen       443 ssl http2 default_server;        listen       [::]:443 ssl http2 default_server;        server_name  ceshi2-test.com;   #与申请时的域名保持一致，否则会报错        root         /usr/share/nginx/html;        ssl_certificate  /data/nginx/pki/server.pem;        ssl_certificate_key  /data/nginx/pki/privkey.pem;        ssl_session_timeout     5m; #会话超时时间        ssl_ciphers     ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #加密算法        ssl_protocols   TLSv1 TLSv1.1 TLSv1.2; #SSL协议        #root         /usr/share/nginx/html;        # Load configuration files for the default server block.        include /etc/nginx/default.d/*.conf;       location / &#123;            proxy_pass  http://192.168.12:8080;                   proxy_redirect     off;                  proxy_set_header   Host             $host;                         proxy_set_header   X-Real-IP        $remote_addr;                         proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;                       client_max_body_size       100m;                        index  index.html index.htm;        &#125;        error_page 404 /404.html;            location = /40x.html &#123;        &#125;        error_page 500 502 503 504 /50x.html;            location = /50x.html &#123;        &#125;    &#125;&#125;\n\n\n四.http转发https1.http转发到https，但是http和https不能用同一个配置nginx.conf配置\nserver &#123;\tlisten\t80;\tlisten\twww.xxx.com:80; #此处添加你要该链接访问的域名\tserver_name  www.xxx.com  alias  xxx.com.alias;\trewrite ^(.*) https://$server_name$1 permanent;\t\t#此句最关键&#125;\n\n2.使用同一个端口，http转https原理:http和https是tcp的上层协议，当nginx服务器建立tcp连接后，根据收到的第一份数据来确定客户端是希望建立tls还是http。nginx会判断tcp请求的首写节内容以进行区分，如果是0x80或者0x16就可能是ssl或者tls，然后尝试https握手。如果端口开启了https，但请求过来的并不是，会抛出一个http级别的错误，这个错误的状态码是NGX_HTTP_TO_HTTPS，错误代码497，然后在返回response中会抛出一个400错误(因为497不是标准状态码，丢给浏览器也没有用)，这时浏览器会显示&quot;400 Bad Request,The plain HTTP request was sent to HTTPS port&quot;\nnginx.conf配置\nserver &#123;\tlisten\t80 ssl;\tlisten\twww.xxx.com:80; \t\t\t\t#此处添加你要该链接访问的域名\tserver_name  www.xxx.com  alias  xxx.com.alias;\terror_page 497 https://$host:8080$request_uri;\t\t#此句最关键，重新定义端口\t#error_page 497 https://$http_host$request_uri;\t\t#此句最关键，只是将http改为https，其他不变&#125;\n\nnginx详细功能详见https://liusw.top/categories/Nginx/\n\nThe End\n\n","categories":["Nginx"],"tags":["Nginx"]},{"title":"OnlyOffice容器化安装","url":"/20250409/onlyoffice/3210a0521723/","content":"\nOnlyOffice容器化安装1.创建工作目录sudo mkdir -p &quot;/app/onlyoffice/mysql/conf.d&quot;;sudo mkdir -p &quot;/app/onlyoffice/mysql/data&quot;;sudo mkdir -p &quot;/app/onlyoffice/mysql/initdb&quot;;sudo mkdir -p &quot;/app/onlyoffice/CommunityServer/data&quot;;sudo mkdir -p &quot;/app/onlyoffice/CommunityServer/logs&quot;;sudo mkdir -p &quot;/app/onlyoffice/CommunityServer/letsencrypt&quot;;sudo mkdir -p &quot;/app/onlyoffice/DocumentServer/data&quot;;sudo mkdir -p &quot;/app/onlyoffice/DocumentServer/logs&quot;;sudo mkdir -p &quot;/app/onlyoffice/MailServer/data/certs&quot;;sudo mkdir -p &quot;/app/onlyoffice/MailServer/logs&quot;;sudo mkdir -p &quot;/app/onlyoffice/ControlPanel/data&quot;;sudo mkdir -p &quot;/app/onlyoffice/ControlPanel/logs&quot;;\n\n2.docker创建网络sudo docker network create --driver bridge onlyoffice\n\n3.安装mysql创建相关文件\necho &quot;[mysqld]sql_mode = &#x27;NO_ENGINE_SUBSTITUTION&#x27;max_connections = 1000max_allowed_packet = 1048576000group_concat_max_len = 2048&quot; &gt; /app/onlyoffice/mysql/conf.d/onlyoffice.cnfecho &quot;ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;my-secret-pw&#x27;;CREATE USER IF NOT EXISTS &#x27;onlyoffice_user&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;onlyoffice_pass&#x27;;CREATE USER IF NOT EXISTS &#x27;mail_admin&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;Isadmin123&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;onlyoffice_user&#x27;@&#x27;%&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;mail_admin&#x27;@&#x27;%&#x27;;FLUSH PRIVILEGES;&quot; &gt; /app/onlyoffice/mysql/initdb/setup.sql\n安装mysql容器\nsudo docker run --net onlyoffice -i -t -d --restart=always --name onlyoffice-mysql-server \\ -v /app/onlyoffice/mysql/conf.d:/etc/mysql/conf.d \\ -v /app/onlyoffice/mysql/data:/var/lib/mysql \\ -v /app/onlyoffice/mysql/initdb:/docker-entrypoint-initdb.d \\ -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ -e MYSQL_DATABASE=onlyoffice \\ mysql:8.0.29\n\n等一下后，进入容器内部执行setup.sql(或者连接mysql手动执行sql文件内语句)\ndocker exec -it onlyoffice-mysql-server /bin/bashcd /var/lib/mysqlmysql -uroot -p&quot;my-secret-pw&quot; mysql&gt; source setup.sql;\n\n3.创建documentserver容器sudo docker run --net onlyoffice -i -t -d --restart=always --name &quot;onlyoffice-document-server&quot; \\ -e JWT_ENABLED=false \\ -e JWT_SECRET=$&#123;JWT_SECRET&#125; \\ -e JWT_HEADER=AuthorizationJwt \\ -v /app/onlyoffice/DocumentServer/logs:/var/log/onlyoffice  \\ -v /app/onlyoffice/DocumentServer/data:/var/www/onlyoffice/Data  \\ -v /app/onlyoffice/DocumentServer/fonts:/usr/share/fonts/truetype/custom \\ -v /app/onlyoffice/DocumentServer/forgotten:/var/lib/onlyoffice/documentserver/App_Data/cache/files/forgotten \\ onlyoffice/documentserver\n\n3.创建documentserver容器sudo docker run --init --net onlyoffice --privileged -i -t -d --restart=always --name onlyoffice-mail-server -p 25:25 -p 143:143 -p 587:587 \\ -e MYSQL_SERVER=onlyoffice-mysql-server \\ -e MYSQL_SERVER_PORT=3306 \\ -e MYSQL_ROOT_USER=root \\ -e MYSQL_ROOT_PASSWD=my-secret-pw \\ -e MYSQL_SERVER_DB_NAME=onlyoffice_mailserver \\ -v /app/onlyoffice/MailServer/data:/var/vmail \\ -v /app/onlyoffice/MailServer/data/certs:/etc/pki/tls/mailserver \\ -v /app/onlyoffice/MailServer/logs:/var/log \\ -h yourdomain.com \\ onlyoffice/mailserver\n\n设置MAIL_SERVER_IP地址\nMAIL_SERVER_IP=$(docker inspect -f &#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; onlyoffice-mail-server)\n\n4.创建controlpanel容器docker run --net onlyoffice -i -t -d --restart=always --name onlyoffice-control-panel \\-v /var/run/docker.sock:/var/run/docker.sock \\-v /app/onlyoffice/CommunityServer/data:/app/onlyoffice/CommunityServer/data \\-v /app/onlyoffice/ControlPanel/data:/var/www/onlyoffice/Data \\-v /app/onlyoffice/ControlPanel/logs:/var/log/onlyoffice onlyoffice/controlpanel\n\n5.创建communityserver容器sudo docker run --net onlyoffice -i -t -d --privileged --restart=always --name onlyoffice-community-server -p 8061:80 -p 8443:443 -p 5222:5222 --cgroupns=host \\ -e MYSQL_SERVER_ROOT_PASSWORD=my-secret-pw \\ -e MYSQL_SERVER_DB_NAME=onlyoffice \\ -e MYSQL_SERVER_HOST=onlyoffice-mysql-server \\ -e MYSQL_SERVER_USER=onlyoffice_user \\ -e MYSQL_SERVER_PASS=onlyoffice_pass \\ -e DOCUMENT_SERVER_PORT_80_TCP_ADDR=onlyoffice-document-server \\ -e DOCUMENT_SERVER_JWT_ENABLED=false \\ -e DOCUMENT_SERVER_JWT_SECRET=$&#123;JWT_SECRET&#125; \\ -e DOCUMENT_SERVER_JWT_HEADER=AuthorizationJwt \\ -e MAIL_SERVER_API_HOST=$&#123;MAIL_SERVER_IP&#125; \\ -e MAIL_SERVER_DB_HOST=onlyoffice-mysql-server \\ -e MAIL_SERVER_DB_NAME=onlyoffice_mailserver \\ -e MAIL_SERVER_DB_PORT=3306 \\ -e MAIL_SERVER_DB_USER=root \\ -e MAIL_SERVER_DB_PASS=my-secret-pw \\ -e CONTROL_PANEL_PORT_80_TCP=80 \\ -e CONTROL_PANEL_PORT_80_TCP_ADDR=onlyoffice-control-panel \\ -v /app/onlyoffice/CommunityServer/data:/var/www/onlyoffice/Data \\ -v /app/onlyoffice/CommunityServer/logs:/var/log/onlyoffice \\ -v /app/onlyoffice/CommunityServer/letsencrypt:/etc/letsencrypt \\ -v /sys/fs/cgroup:/sys/fs/cgroup:rw \\ onlyoffice/communityserver\n\n等待初始化完成后访问web\nhttp://本地IP地址:8061/\n\n\n完\n\n问题Error: DNS lookup xxx.xxx.xxx.xx(family:undefined, host:undefined) is not allowed. Because, It is private IP address.\n解决方式：需要编辑配置文件允许私有ip通过\n编辑documentserver容器中/etc/onlyoffice/documentserver/default.json​下的内容：搜索并修改以下字段为true：&quot;request-filtering-agent&quot; : &#123;&quot;allowPrivateIPAddress&quot;: true,&quot;allowMetaIPAddress&quot;: true&#125;,\n\n也可以将文件复制出来再进行编辑\ndocker cp onlyoffice-document-server:/etc/onlyoffice/documentserver/default.json ./\n\n修改文件内容\nvim default.json\n\n将文件复制回容器\ndocker cp ./default.json onlyoffice-document-server:/etc/onlyoffice/documentserver/default.json\n\n更改后，需要重启容器后恢复正常。\n此文引用自GitHub&#x2F;ONLYOFFICE\n","categories":["onlyoffice"],"tags":["onlyoffice"]},{"title":"PostgreSQL修炼之道","url":"/20241121/PostgreSQL/01821b0963bc/","content":"PostgreSQL修炼之道\n\n\n\nThe End\n\n","categories":["PostgreSQL"],"tags":["PostgreSQL"]},{"title":"Prometheus普罗米修斯之安装配置使用","url":"/20241121/Prometheus/527f1fcaefa2/","content":"Prometheus普罗米修斯之安装配置使用一、基础环境\n\n\n环境&#x2F;组件\n版本\n下载地址\n\n\n\n操作系统\nCentOS7.6\n\n\n\ngo\n1.15.7\nhttps://golang.org/dl/\n\n\nPrometheus\n2.6.0\nhttps://prometheus.io/download/#prometheus\n\n\nGrafana\n5.4.2\nhttps://dl.grafana.com/oss/release/grafana-5.4.2-1.x86_64.rpm\n\n\nexporter\n\nhttps://prometheus.io/docs/instrumenting/exporters/\n\n\n二、安装go 1.解压安装 \ntar -C /usr/local/ -xvf go1.15.7.linux-amd64.tar.gz\n\n 2.配置环境变量 \necho &quot;export PATH=$PATH:/usr/local/go/bin&quot; &gt;&gt; /etc/profilesource /etc/profile\n\n3.验证 \ngo version\n\n\n\n三、安装Prometheus 1.安装 \ntar -C /usr/local/ -xvf prometheus-2.28.1.linux-amd64.tarln -sv /usr/local/prometheus-2.28.1.linux-amd64/ /usr/local/Prometheus\n\n 2.注册服务\ntee /etc/systemd/system/prometheus.service &lt;&lt;EOF[Unit]Description=PrometheusDocumentation=https://prometheus.io/After=network.target[Service]Type=simpleExecStart=/usr/local/Prometheus \\  --config.file=/usr/local/Prometheus/prometheus.yml \\  --web.listen-address=:9090Restart=on-failure[Install]WantedBy=multi-user.targetEOF\n\n 3.启动\nsystemctl status prometheussystemctl start prometheussystemctl enable prometheus\n\n  4.登录验证\n浏览器打开IP:9090端口即可打开普罗米修斯自带的监控页面\n\n\n\n四、安装Grafana普罗米修斯默认的页面可能没有那么直观，我们可以安装grafana使监控看起来更直观\n 1.安装 \nyum install -y grafana-5.4.2-1.x86_64.rpm\n\nyum源无法使用时强制rpm安装\nrpm -ivh --nodeps grafana-5.4.2-1.x86_64.rpm\n\n 2.启动 \nsystemctl daemon-reloadsystemctl enable grafana-server.servicesystemctl start grafana-server.service\n\n 3.访问grafana \n 浏览器访问IP:3000端口，即可打开grafana页面，默认用户名密码都是admin\n初次登录会要求修改默认的登录密码\n五、一些常用监控举例1.监控linux机器（node-exporter） 被监控的机器安装node-exporter \ntar  -zxvf  node_exporter-0.17.0.linux-amd64.tar.gz -C /usr/local/\n\n注册node-exporter 服务\nvim /lib/systemd/system/node_exporter.service\n\n[Unit]Description=This is prometheus node exporter[Service]Type=simpleExecStart=/usr/local/node_exporter/node_exporter --web.listen-address=:9101ExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.target\n\n启动服务\nsystemctl status node_exportersystemctl start node_exportersystemctl enable node_exporter\n\n 普罗米修斯配置文件添加监控项 \nvi /usr/local/Prometheus/prometheus.yml\n\n 默认node-exporter端口为9100 添加如下配置:\n- job_name: &#x27;node_exporter&#x27;  static_configs:  - targets: [&#x27;192.168.0.102:9100&#x27;]    labels:      instance: Prometheus\n\n 重启普罗米修斯\nsystemctl restart prometheus\n\n grafana导入画好的dashboard模板\n   dashboard json   \n&#123;\n  \"__inputs\": [\n    &#123;\n      \"name\": \"DS_PROMETHEUS_111\",\n      \"label\": \"prometheus\",\n      \"description\": \"\",\n      \"type\": \"datasource\",\n      \"pluginId\": \"prometheus\",\n      \"pluginName\": \"Prometheus\"\n    &#125;\n  ],\n  \"__requires\": [\n    &#123;\n      \"type\": \"grafana\",\n      \"id\": \"grafana\",\n      \"name\": \"Grafana\",\n      \"version\": \"5.3.2\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"grafana-piechart-panel\",\n      \"name\": \"Pie Chart\",\n      \"version\": \"1.3.3\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"graph\",\n      \"name\": \"Graph\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"datasource\",\n      \"id\": \"prometheus\",\n      \"name\": \"Prometheus\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"singlestat\",\n      \"name\": \"Singlestat\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"table\",\n      \"name\": \"Table\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"text\",\n      \"name\": \"Text\",\n      \"version\": \"5.0.0\"\n    &#125;\n  ],\n  \"annotations\": &#123;\n    \"list\": [\n      &#123;\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations & Alerts\",\n        \"type\": \"dashboard\"\n      &#125;\n    ]\n  &#125;,\n  \"description\": \"使用 Node Exporter v0.16，精简优化重要指标展示。\\r\\n包含：CPU 内存 磁盘 IO 网络 温度等监控指标。\\r\\nhttps://github.com/starsliao/Prometheus\",\n  \"editable\": true,\n  \"gnetId\": 8919,\n  \"graphTooltip\": 0,\n  \"id\": null,\n  \"iteration\": 1542357033300,\n  \"links\": [],\n  \"panels\": [\n    &#123;\n      \"content\": \"\",\n      \"editable\": true,\n      \"error\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      &#125;,\n      \"id\": 11,\n      \"links\": [],\n      \"minSpan\": 4,\n      \"mode\": \"html\",\n      \"repeat\": \"node\",\n      \"repeatDirection\": \"h\",\n      \"style\": &#123;&#125;,\n      \"title\": \"$node\",\n      \"type\": \"text\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 1,\n      \"description\": \"\",\n      \"format\": \"s\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 2,\n        \"x\": 0,\n        \"y\": 1\n      &#125;,\n      \"hideTimeOverride\": true,\n      \"id\": 15,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"time() - node_boot_time_seconds&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"instant\": true,\n          \"intervalFactor\": 2,\n          \"refId\": \"A\",\n          \"step\": 40\n        &#125;\n      ],\n      \"thresholds\": \"\",\n      \"title\": \"系统运行时间\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"100%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"description\": \"\",\n      \"format\": \"short\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 2,\n        \"w\": 2,\n        \"x\": 2,\n        \"y\": 1\n      &#125;,\n      \"id\": 14,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"count(count(node_cpu_seconds_total&#123;instance=~\\\"$node\\\", mode='system'&#125;) by (cpu))\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"\",\n      \"title\": \"CPU 核数\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"100%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 2,\n      \"description\": \"\",\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 3,\n        \"x\": 4,\n        \"y\": 1\n      &#125;,\n      \"id\": 167,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 2,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"100 - (avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"idle\\\"&#125;[5m])) * 100)\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"50,80\",\n      \"title\": \"CPU使用率（5m）\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 2,\n      \"description\": \"\",\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 3,\n        \"x\": 7,\n        \"y\": 1\n      &#125;,\n      \"id\": 20,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 2,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"iowait\\\"&#125;[5m])) * 100\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"20,50\",\n      \"title\": \"CPU iowait（5m）\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 0,\n      \"description\": \"\",\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 3,\n        \"x\": 10,\n        \"y\": 1\n      &#125;,\n      \"hideTimeOverride\": false,\n      \"id\": 172,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"((node_memory_MemTotal_bytes&#123;instance=~\\\"$node\\\"&#125; - node_memory_MemFree_bytes&#123;instance=~\\\"$node\\\"&#125; - node_memory_Buffers_bytes&#123;instance=~\\\"$node\\\"&#125; - node_memory_Cached_bytes&#123;instance=~\\\"$node\\\"&#125;) / (node_memory_MemTotal_bytes&#123;instance=~\\\"$node\\\"&#125; )) * 100\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"80,90\",\n      \"title\": \"内存使用率\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorPostfix\": false,\n      \"colorPrefix\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 2,\n      \"description\": \"\",\n      \"format\": \"short\",\n      \"gauge\": &#123;\n        \"maxValue\": 10000,\n        \"minValue\": null,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 3,\n        \"x\": 13,\n        \"y\": 1\n      &#125;,\n      \"hideTimeOverride\": false,\n      \"id\": 16,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_filefd_allocated&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"refId\": \"B\"\n        &#125;\n      ],\n      \"thresholds\": \"7000,9000\",\n      \"title\": \"当前打开的文件描述符\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"70%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": null,\n      \"description\": \"\",\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 4,\n        \"x\": 16,\n        \"y\": 1\n      &#125;,\n      \"id\": 166,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"repeatDirection\": \"h\",\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"100 - ((node_filesystem_avail_bytes&#123;instance=~\\\"$node\\\",mountpoint=\\\"/\\\",fstype=~\\\"ext4|xfs\\\"&#125; * 100) / node_filesystem_size_bytes &#123;instance=~\\\"$node\\\",mountpoint=\\\"/\\\",fstype=~\\\"ext4|xfs\\\"&#125;)\",\n          \"format\": \"time_series\",\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"70,90\",\n      \"title\": \"根分区使用率\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": null,\n      \"description\": \"通过变量maxmount获取最大的分区。\",\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": true,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 5,\n        \"w\": 4,\n        \"x\": 20,\n        \"y\": 1\n      &#125;,\n      \"id\": 154,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"repeat\": null,\n      \"repeatDirection\": \"h\",\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"100 - ((node_filesystem_avail_bytes&#123;instance=~\\\"$node\\\",mountpoint=\\\"$maxmount\\\",fstype=~\\\"ext4|xfs\\\"&#125; * 100) / node_filesystem_size_bytes &#123;instance=~\\\"$node\\\",mountpoint=\\\"$maxmount\\\",fstype=~\\\"ext4|xfs\\\"&#125;)\",\n          \"format\": \"time_series\",\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"70,90\",\n      \"title\": \"最大分区($maxmount)使用率\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": null,\n      \"description\": \"\",\n      \"format\": \"bytes\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 3,\n        \"w\": 2,\n        \"x\": 2,\n        \"y\": 3\n      &#125;,\n      \"id\": 75,\n      \"interval\": null,\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"70%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"50%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_memory_MemTotal_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;instance&#125;&#125;\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"\",\n      \"title\": \"内存总量\",\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [\n        &#123;\n          \"op\": \"=\",\n          \"text\": \"N/A\",\n          \"value\": \"null\"\n        &#125;\n      ],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"15分钟\": \"#6ED0E0\",\n        \"1分钟\": \"#BF1B00\",\n        \"5分钟\": \"#CCA300\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 1,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 6,\n        \"w\": 11,\n        \"x\": 0,\n        \"y\": 6\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 13,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null as zero\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"repeat\": null,\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_load1&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"1m\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"expr\": \"node_load5&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"5m\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"node_load15&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"instant\": false,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"15m\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"系统平均负载\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"cumulative\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"/\": \"#eab839\",\n        \"/boot\": \"#bf1b00\",\n        \"/data\": \"#1f78c1\"\n      &#125;,\n      \"breakPoint\": \"100%\",\n      \"cacheTimeout\": null,\n      \"combine\": &#123;\n        \"label\": \"Others\",\n        \"threshold\": \"\"\n      &#125;,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 1,\n      \"fontSize\": \"50%\",\n      \"format\": \"bytes\",\n      \"gridPos\": &#123;\n        \"h\": 6,\n        \"w\": 5,\n        \"x\": 11,\n        \"y\": 6\n      &#125;,\n      \"hideTimeOverride\": false,\n      \"id\": 171,\n      \"interval\": null,\n      \"legend\": &#123;\n        \"header\": \"\",\n        \"percentage\": false,\n        \"percentageDecimals\": 0,\n        \"show\": true,\n        \"sideWidth\": 142,\n        \"values\": true\n      &#125;,\n      \"legendType\": \"Right side\",\n      \"links\": [],\n      \"maxDataPoints\": 3,\n      \"nullPointMode\": \"connected\",\n      \"pieType\": \"pie\",\n      \"strokeWidth\": \"2\",\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_filesystem_size_bytes &#123;instance=~\\\"$node\\\",fstype=~\\\"ext4|xfs\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"instant\": true,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;mountpoint&#125;&#125;\",\n          \"refId\": \"A\"\n        &#125;\n      ],\n      \"title\": \"磁盘总空间\",\n      \"type\": \"grafana-piechart-panel\",\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"columns\": [],\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"fontSize\": \"120%\",\n      \"gridPos\": &#123;\n        \"h\": 6,\n        \"w\": 8,\n        \"x\": 16,\n        \"y\": 6\n      &#125;,\n      \"id\": 164,\n      \"links\": [],\n      \"pageSize\": null,\n      \"scroll\": true,\n      \"showHeader\": true,\n      \"sort\": &#123;\n        \"col\": 11,\n        \"desc\": true\n      &#125;,\n      \"styles\": [\n        &#123;\n          \"alias\": \"Time\",\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"pattern\": \"Time\",\n          \"type\": \"hidden\"\n        &#125;,\n        &#123;\n          \"alias\": \"分区\",\n          \"colorMode\": null,\n          \"colors\": [\n            \"rgba(50, 172, 45, 0.97)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(245, 54, 54, 0.9)\"\n          ],\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"decimals\": 2,\n          \"mappingType\": 1,\n          \"pattern\": \"mountpoint\",\n          \"thresholds\": [\n            \"\"\n          ],\n          \"type\": \"string\",\n          \"unit\": \"bytes\"\n        &#125;,\n        &#123;\n          \"alias\": \"可用空间\",\n          \"colorMode\": \"value\",\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"decimals\": 2,\n          \"mappingType\": 1,\n          \"pattern\": \"Value #A\",\n          \"thresholds\": [\n            \"10000000000\",\n            \"20000000000\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"bytes\"\n        &#125;,\n        &#123;\n          \"alias\": \"使用率\",\n          \"colorMode\": \"cell\",\n          \"colors\": [\n            \"rgba(50, 172, 45, 0.97)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(245, 54, 54, 0.9)\"\n          ],\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"decimals\": 2,\n          \"mappingType\": 1,\n          \"pattern\": \"Value #B\",\n          \"thresholds\": [\n            \"70\",\n            \"90\"\n          ],\n          \"type\": \"number\",\n          \"unit\": \"percentunit\"\n        &#125;,\n        &#123;\n          \"alias\": \"总空间\",\n          \"colorMode\": null,\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"decimals\": 1,\n          \"link\": false,\n          \"mappingType\": 1,\n          \"pattern\": \"Value #C\",\n          \"thresholds\": [],\n          \"type\": \"number\",\n          \"unit\": \"bytes\"\n        &#125;,\n        &#123;\n          \"alias\": \"文件系统\",\n          \"colorMode\": null,\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"dateFormat\": \"YYYY-MM-DD HH:mm:ss\",\n          \"decimals\": 2,\n          \"link\": false,\n          \"mappingType\": 1,\n          \"pattern\": \"fstype\",\n          \"thresholds\": [],\n          \"type\": \"number\",\n          \"unit\": \"short\"\n        &#125;,\n        &#123;\n          \"alias\": \"\",\n          \"colorMode\": null,\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"decimals\": 2,\n          \"pattern\": \"/.*/\",\n          \"preserveFormat\": true,\n          \"sanitize\": false,\n          \"thresholds\": [],\n          \"type\": \"hidden\",\n          \"unit\": \"short\"\n        &#125;\n      ],\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_filesystem_size_bytes&#123;instance=~'$node',fstype=~\\\"ext4|xfs\\\"&#125;\",\n          \"format\": \"table\",\n          \"hide\": true,\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"C\"\n        &#125;,\n        &#123;\n          \"expr\": \"node_filesystem_avail_bytes &#123;instance=~'$node',fstype=~\\\"ext4|xfs\\\"&#125;\",\n          \"format\": \"table\",\n          \"hide\": false,\n          \"instant\": true,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"A\"\n        &#125;,\n        &#123;\n          \"expr\": \"1-(node_filesystem_free_bytes&#123;instance=~'$node',fstype=~\\\"ext4|xfs\\\"&#125; / node_filesystem_size_bytes&#123;instance=~'$node',fstype=~\\\"ext4|xfs\\\"&#125;)\",\n          \"format\": \"table\",\n          \"hide\": false,\n          \"instant\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"B\"\n        &#125;,\n        &#123;\n          \"expr\": \"\",\n          \"format\": \"table\",\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"refId\": \"D\"\n        &#125;\n      ],\n      \"title\": \"各分区可用空间\",\n      \"transform\": \"table\",\n      \"transparent\": false,\n      \"type\": \"table\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"Idle - Waiting for something to happen\": \"#052B51\",\n        \"guest\": \"#9AC48A\",\n        \"idle\": \"#052B51\",\n        \"iowait\": \"#EAB839\",\n        \"irq\": \"#BF1B00\",\n        \"nice\": \"#C15C17\",\n        \"sdb_每秒I/O操作%\": \"#d683ce\",\n        \"softirq\": \"#E24D42\",\n        \"steal\": \"#FCE2DE\",\n        \"system\": \"#508642\",\n        \"user\": \"#5195CE\",\n        \"磁盘花费在I/O操作占比\": \"#ba43a9\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 2,\n      \"description\": \"node_disk_io_time_seconds_total：\\n磁盘花费在输入/输出操作上的毫秒数。该值为累加值。（Milliseconds Spent Doing I/Os）\\n\\nirate(node_disk_io_time_seconds_total[1m])：\\n计算每秒的速率：(last值-last前一个值)/时间戳差值，即：1秒钟内磁盘花费在I/O操作的时间占比。\",\n      \"fill\": 1,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 16,\n        \"x\": 0,\n        \"y\": 12\n      &#125;,\n      \"id\": 7,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sideWidth\": null,\n        \"sort\": null,\n        \"sortDesc\": null,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"minSpan\": 4,\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"repeat\": null,\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"system\\\"&#125;[1m]))\",\n          \"format\": \"time_series\",\n          \"interval\": \"\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"System\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"user\\\"&#125;[1m]))\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"User\",\n          \"refId\": \"B\",\n          \"step\": 240\n        &#125;,\n        &#123;\n          \"expr\": \"avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"idle\\\"&#125;[1m]))\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"Idle\",\n          \"refId\": \"F\",\n          \"step\": 240\n        &#125;,\n        &#123;\n          \"expr\": \"avg(irate(node_cpu_seconds_total&#123;instance=~\\\"$node\\\",mode=\\\"iowait\\\"&#125;[1m]))\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"Iowait\",\n          \"refId\": \"D\",\n          \"step\": 240\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_io_time_seconds_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_每秒I/O操作%\",\n          \"refId\": \"C\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"CPU使用率、磁盘每秒的I/O操作耗费时间（%）\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"decimals\": null,\n          \"format\": \"percentunit\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": \"1\",\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"内存_Avaliable\": \"#6ED0E0\",\n        \"内存_Cached\": \"#EF843C\",\n        \"内存_Free\": \"#629E51\",\n        \"内存_Total\": \"#6d1f62\",\n        \"内存_Used\": \"#eab839\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"decimals\": 2,\n      \"fill\": 1,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 16,\n        \"y\": 12\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 156,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": true,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_memory_MemTotal_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"总内存\",\n          \"refId\": \"A\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"node_memory_MemTotal_bytes&#123;instance=~\\\"$node\\\"&#125; - (node_memory_Cached_bytes&#123;instance=~\\\"$node\\\"&#125; + node_memory_Buffers_bytes&#123;instance=~\\\"$node\\\"&#125; + node_memory_MemFree_bytes&#123;instance=~\\\"$node\\\"&#125;)\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"已用\",\n          \"refId\": \"B\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"node_memory_MemFree_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"内存_Free\",\n          \"refId\": \"C\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"node_memory_Buffers_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"内存_Buffers\",\n          \"refId\": \"D\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"node_memory_Cached_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"内存_Cached\",\n          \"refId\": \"E\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"node_memory_MemAvailable_bytes&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"可用\",\n          \"refId\": \"F\",\n          \"step\": 4\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"内存信息\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"decbytes\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": \"0\",\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"vda_write\": \"#6ED0E0\"\n      &#125;,\n      \"bars\": true,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"description\": \"Reads completed: 每个磁盘分区每秒读完成次数\\n\\nWrites completed: 每个磁盘分区每秒写完成次数\\n\\nIO now 每个磁盘分区每秒正在处理的输入/输出请求数\",\n      \"fill\": 2,\n      \"gridPos\": &#123;\n        \"h\": 8,\n        \"w\": 8,\n        \"x\": 0,\n        \"y\": 19\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 161,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"/.*_读取$/\",\n          \"transform\": \"negative-Y\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"irate(node_disk_reads_completed_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_读取\",\n          \"refId\": \"A\",\n          \"step\": 10\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_writes_completed_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_写入\",\n          \"refId\": \"B\",\n          \"step\": 10\n        &#125;,\n        &#123;\n          \"expr\": \"node_disk_io_now&#123;instance=~\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;\",\n          \"refId\": \"C\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"磁盘读写速率（IOPS）\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"decimals\": null,\n          \"format\": \"iops\",\n          \"label\": \"读取（-）/写入（+）I/O ops/sec\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"vda_write\": \"#6ED0E0\"\n      &#125;,\n      \"bars\": true,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"description\": \"Read bytes 每个磁盘分区每秒读取的比特数\\nWritten bytes 每个磁盘分区每秒写入的比特数\",\n      \"fill\": 2,\n      \"gridPos\": &#123;\n        \"h\": 8,\n        \"w\": 8,\n        \"x\": 8,\n        \"y\": 19\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 168,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"/.*_读取$/\",\n          \"transform\": \"negative-Y\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"irate(node_disk_read_bytes_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_读取\",\n          \"refId\": \"A\",\n          \"step\": 10\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_written_bytes_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_写入\",\n          \"refId\": \"B\",\n          \"step\": 10\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"磁盘读写容量大小\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"decimals\": null,\n          \"format\": \"Bps\",\n          \"label\": \"读取（-）/写入（+）\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"vda\": \"#6ED0E0\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"description\": \"Read time ms 每个磁盘分区读操作花费的秒数\\n\\nWrite time ms 每个磁盘分区写操作花费的秒数\\n\\nIO time ms 每个磁盘分区输入/输出操作花费的秒数\\n\\nIO time weighted 每个磁盘分区输入/输出操作花费的加权秒数\",\n      \"fill\": 3,\n      \"gridPos\": &#123;\n        \"h\": 8,\n        \"w\": 8,\n        \"x\": 16,\n        \"y\": 19\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 160,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"/,*_读取$/\",\n          \"transform\": \"negative-Y\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"irate(node_disk_io_time_seconds_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"interval\": \"\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;\",\n          \"refId\": \"A\",\n          \"step\": 10\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_io_time_weighted_seconds_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_加权\",\n          \"refId\": \"D\"\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_read_time_seconds_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_读取\",\n          \"refId\": \"B\"\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_disk_write_time_seconds_total&#123;instance=~\\\"$node\\\"&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_写入\",\n          \"refId\": \"C\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"磁盘IO读写时间\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"s\",\n          \"label\": \"读取（-）/写入（+）\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"fill\": 1,\n      \"gridPos\": &#123;\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 27\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 157,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": false,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": false,\n        \"min\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"/.*_out上传$/\",\n          \"transform\": \"negative-Y\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"irate(node_network_receive_bytes_total&#123;instance=~'$node',device!~'tap.*'&#125;[5m])*8\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_in下载\",\n          \"refId\": \"A\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_network_transmit_bytes_total&#123;instance=~'$node',device!~'tap.*'&#125;[5m])*8\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"&#123;&#123;device&#125;&#125;_out上传\",\n          \"refId\": \"B\",\n          \"step\": 4\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"网络流量\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"bps\",\n          \"label\": \"上传（-）/下载（+）\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": false\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"TCP\": \"#6ED0E0\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"description\": \"CurrEstab - 当前状态为 ESTABLISHED 或 CLOSE-WAIT 的 TCP 连接数\\n\\nActiveOpens - 已从 CLOSED 状态直接转换到 SYN-SENT 状态的 TCP 平均连接数(1分钟内)\\n\\nPassiveOpens - 已从 LISTEN 状态直接转换到 SYN-RCVD 状态的 TCP 平均连接数(1分钟内)\\n\\nTCP_alloc - 已分配（已建立、已申请到sk_buff）的TCP套接字数量\\n\\nTCP_inuse - 正在使用（正在侦听）的TCP套接字数量\\n\\nTCP_tw - 等待关闭的TCP连接数\",\n      \"fill\": 0,\n      \"gridPos\": &#123;\n        \"h\": 8,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 27\n      &#125;,\n      \"height\": \"300\",\n      \"id\": 158,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": false,\n        \"current\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_netstat_Tcp_CurrEstab&#123;instance=~'$node'&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"10s\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"ESTABLISHED\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"node_sockstat_TCP_tw&#123;instance=~'$node'&#125;\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"TCP_tw\",\n          \"refId\": \"D\"\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_netstat_Tcp_ActiveOpens&#123;instance=~'$node'&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"ActiveOpens\",\n          \"refId\": \"B\"\n        &#125;,\n        &#123;\n          \"expr\": \"irate(node_netstat_Tcp_PassiveOpens&#123;instance=~'$node'&#125;[1m])\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"PassiveOpens\",\n          \"refId\": \"C\"\n        &#125;,\n        &#123;\n          \"expr\": \"node_sockstat_TCP_alloc&#123;instance=~'$node'&#125;\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"TCP_alloc\",\n          \"refId\": \"E\"\n        &#125;,\n        &#123;\n          \"expr\": \"node_sockstat_TCP_inuse&#123;instance=~'$node'&#125;\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"TCP_inuse\",\n          \"refId\": \"F\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"TCP 连接情况\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n      \"fill\": 0,\n      \"gridPos\": &#123;\n        \"h\": 10,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 35\n      &#125;,\n      \"id\": 169,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": true,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 1,\n      \"links\": [],\n      \"nullPointMode\": \"null as zero\",\n      \"percentage\": false,\n      \"pointradius\": 0.5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_hwmon_temp_celsius&#123;instance=\\\"$node\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123;chip&#125;&#125; &#123;&#123;sensor&#125;&#125;\",\n          \"refId\": \"A\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"硬件温度\",\n      \"tooltip\": &#123;\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"celsius\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;\n  ],\n  \"refresh\": false,\n  \"schemaVersion\": 16,\n  \"style\": \"dark\",\n  \"tags\": [\n    \"StarsL\",\n    \"Prometheus\"\n  ],\n  \"templating\": &#123;\n    \"list\": [\n      &#123;\n        \"auto\": true,\n        \"auto_count\": 30,\n        \"auto_min\": \"10s\",\n        \"current\": &#123;\n          \"text\": \"1m\",\n          \"value\": \"1m\"\n        &#125;,\n        \"hide\": 0,\n        \"label\": \"interval\",\n        \"name\": \"interval\",\n        \"options\": [\n          &#123;\n            \"selected\": false,\n            \"text\": \"auto\",\n            \"value\": \"$__auto_interval_interval\"\n          &#125;,\n          &#123;\n            \"selected\": true,\n            \"text\": \"1m\",\n            \"value\": \"1m\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"10m\",\n            \"value\": \"10m\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"30m\",\n            \"value\": \"30m\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1h\",\n            \"value\": \"1h\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"6h\",\n            \"value\": \"6h\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"12h\",\n            \"value\": \"12h\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1d\",\n            \"value\": \"1d\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"7d\",\n            \"value\": \"7d\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"14d\",\n            \"value\": \"14d\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"30d\",\n            \"value\": \"30d\"\n          &#125;\n        ],\n        \"query\": \"1m,10m,30m,1h,6h,12h,1d,7d,14d,30d\",\n        \"refresh\": 2,\n        \"skipUrlSync\": false,\n        \"type\": \"interval\"\n      &#125;,\n      &#123;\n        \"allFormat\": \"glob\",\n        \"allValue\": null,\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"环境\",\n        \"multi\": false,\n        \"multiFormat\": \"regex values\",\n        \"name\": \"env\",\n        \"options\": [],\n        \"query\": \"label_values(node_exporter_build_info,env)\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      &#125;,\n      &#123;\n        \"allFormat\": \"glob\",\n        \"allValue\": null,\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"主机名\",\n        \"multi\": false,\n        \"multiFormat\": \"regex values\",\n        \"name\": \"name\",\n        \"options\": [],\n        \"query\": \"label_values(node_exporter_build_info&#123;env='$env'&#125;,name)\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      &#125;,\n      &#123;\n        \"allFormat\": \"glob\",\n        \"allValue\": null,\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"节点\",\n        \"multi\": false,\n        \"multiFormat\": \"regex values\",\n        \"name\": \"node\",\n        \"options\": [],\n        \"query\": \"label_values(node_exporter_build_info&#123;name='$name'&#125;,instance)\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 1,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      &#125;,\n      &#123;\n        \"allValue\": null,\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROMETHEUS_111&#125;\",\n        \"hide\": 2,\n        \"includeAll\": false,\n        \"label\": \"\",\n        \"multi\": false,\n        \"name\": \"maxmount\",\n        \"options\": [],\n        \"query\": \"query_result(topk(1,sort_desc (max(node_filesystem_size_bytes&#123;instance=~'$node',fstype=~\\\"ext4|xfs\\\"&#125;) by (mountpoint))))\",\n        \"refresh\": 1,\n        \"regex\": \"/.*\\\\\\\"(.*)\\\\\\\".*/\",\n        \"skipUrlSync\": false,\n        \"sort\": 0,\n        \"tagValuesQuery\": \"\",\n        \"tags\": [],\n        \"tagsQuery\": \"\",\n        \"type\": \"query\",\n        \"useTags\": false\n      &#125;\n    ]\n  &#125;,\n  \"time\": &#123;\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  &#125;,\n  \"timepicker\": &#123;\n    \"now\": true,\n    \"refresh_intervals\": [\n      \"5s\",\n      \"10s\",\n      \"30s\",\n      \"1m\",\n      \"5m\",\n      \"15m\",\n      \"30m\",\n      \"1h\",\n      \"2h\",\n      \"1d\"\n    ],\n    \"time_options\": [\n      \"5m\",\n      \"15m\",\n      \"1h\",\n      \"6h\",\n      \"12h\",\n      \"24h\",\n      \"2d\",\n      \"7d\",\n      \"30d\"\n    ]\n  &#125;,\n  \"timezone\": \"browser\",\n  \"title\": \"1 Node Exporter 0.16+ 监控展示看板 for Prometheus\",\n  \"uid\": \"9CWBz0bik\",\n  \"version\": 28\n&#125;\t\n   \n\n如果没有任何显示，是grafana缺少相关显示需要用到的插件piechart，grafana的默认插件目录是/var/lib/grafana/plugins，可以将下载好的插件解压到这个目录，重启grafana即可 \nservice grafana-server restart# 查看已安装插件/usr/sbin/grafana-cli plugins ls\n\n 再刷新grafana页面，即可看到我们刚才设置好的node监控 \n2.监控MySQL（mysqld-exporter） 被监控mysql机器安装mysqld-exporter \ntar -C /usr/local/ -xvf mysqld_exporter-0.11.0.linux-amd64.tar\n\n 设置配置文件，user为数据库登录用户，password为这个用户的密码 \nvi .my.cnf[client]user=rootpassword=123456\n\n 启动mysqld-exporter 也可注册服务与node-exporter同理\n/usr/local/mysqld_exporter-0.11.0.linux-amd64/mysqld_exporter --config.my-cnf=&quot;/usr/local/mysqld_exporter-0.11.0.linux-amd64/.my.cnf&quot; &amp;\n\n prometheus配置文件中加入mysql监控并重启 \nvim /usr/local/Prometheus/prometheus.yml# 默认mysqld-exporter端口为9104- job_name: &#x27;MySQL&#x27;    static_configs:    - targets: [&#x27;192.168.0.103:9104&#x27;]\n\n 设置数据源 \n grafana界面添加mysql数据源 \n 添加需要被监控的数据库及相关信息 \n 导入已经画好的dashboard，数据源选择刚刚创建好的mysql数据源即可 \n   dashboard-mysql.json   \n&#123;\n  \"__inputs\": [\n    &#123;\n      \"name\": \"DS_PROMETHEUS\",\n      \"label\": \"Prometheus\",\n      \"description\": \"\",\n      \"type\": \"datasource\",\n      \"pluginId\": \"prometheus\",\n      \"pluginName\": \"Prometheus\"\n    &#125;\n  ],\n  \"__requires\": [\n    &#123;\n      \"type\": \"grafana\",\n      \"id\": \"grafana\",\n      \"name\": \"Grafana\",\n      \"version\": \"5.0.4\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"graph\",\n      \"name\": \"Graph\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"datasource\",\n      \"id\": \"prometheus\",\n      \"name\": \"Prometheus\",\n      \"version\": \"5.0.0\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"singlestat\",\n      \"name\": \"Singlestat\",\n      \"version\": \"5.0.0\"\n    &#125;\n  ],\n  \"annotations\": &#123;\n    \"list\": [\n      &#123;\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": false,\n        \"iconColor\": \"#e0752d\",\n        \"limit\": 100,\n        \"name\": \"PMM Annotations\",\n        \"showIn\": 0,\n        \"tags\": [\n          \"pmm_annotation\"\n        ],\n        \"type\": \"tags\"\n      &#125;\n    ]\n  &#125;,\n  \"editable\": true,\n  \"gnetId\": null,\n  \"graphTooltip\": 1,\n  \"id\": null,\n  \"iteration\": 1536577388808,\n  \"links\": [\n    &#123;\n      \"icon\": \"dashboard\",\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"QAN\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"Query Analytics\",\n      \"type\": \"link\",\n      \"url\": \"/graph/dashboard/db/_pmm-query-analytics\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"OS\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"OS\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"MySQL\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"MySQL\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"MongoDB\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"MongoDB\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"HA\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"HA\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"Cloud\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"Cloud\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"Insight\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"Insight\",\n      \"type\": \"dashboards\"\n    &#125;,\n    &#123;\n      \"asDropdown\": true,\n      \"includeVars\": true,\n      \"keepTime\": true,\n      \"tags\": [\n        \"PMM\"\n      ],\n      \"targetBlank\": false,\n      \"title\": \"PMM\",\n      \"type\": \"dashboards\"\n    &#125;\n  ],\n  \"panels\": [\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      &#125;,\n      \"id\": 382,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 1,\n      \"description\": \"**MySQL Uptime**\\n\\nThe amount of time since the last restart of the MySQL server process.\",\n      \"editable\": true,\n      \"error\": false,\n      \"format\": \"s\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 4,\n        \"w\": 6,\n        \"x\": 0,\n        \"y\": 1\n      &#125;,\n      \"height\": \"125px\",\n      \"id\": 12,\n      \"interval\": \"$interval\",\n      \"links\": [],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"postfix\": \"s\",\n      \"postfixFontSize\": \"80%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"80%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"10m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_uptime&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"5m\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 300\n        &#125;\n      ],\n      \"thresholds\": \"300,3600\",\n      \"title\": \"MySQL Uptime\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**Current QPS**\\n\\nBased on the queries reported by MySQL's ``SHOW STATUS`` command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count \\n``COM_PING`` or ``COM_STATISTICS`` commands.\",\n      \"editable\": true,\n      \"error\": false,\n      \"format\": \"short\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 4,\n        \"w\": 6,\n        \"x\": 6,\n        \"y\": 1\n      &#125;,\n      \"height\": \"125px\",\n      \"id\": 13,\n      \"interval\": \"$interval\",\n      \"links\": [\n        &#123;\n          \"targetBlank\": true,\n          \"title\": \"MySQL Server Status Variables\",\n          \"type\": \"absolute\",\n          \"url\": \"https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#statvar_Queries\"\n        &#125;\n      ],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"80%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": true\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"10m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_queries&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_queries&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": \"35,75\",\n      \"title\": \"Current QPS\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": false,\n      \"colors\": [\n        \"rgba(50, 172, 45, 0.97)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(245, 54, 54, 0.9)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 0,\n      \"description\": \"**InnoDB Buffer Pool Size**\\n\\nInnoDB maintains a storage area called the buffer pool for caching data and indexes in memory.  Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.\",\n      \"editable\": true,\n      \"error\": false,\n      \"format\": \"bytes\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 4,\n        \"w\": 6,\n        \"x\": 12,\n        \"y\": 1\n      &#125;,\n      \"height\": \"125px\",\n      \"id\": 51,\n      \"interval\": \"$interval\",\n      \"links\": [\n        &#123;\n          \"targetBlank\": true,\n          \"title\": \"Tuning the InnoDB Buffer Pool Size\",\n          \"type\": \"absolute\",\n          \"url\": \"https://www.percona.com/blog/2015/06/02/80-ram-tune-innodb_buffer_pool_size/\"\n        &#125;\n      ],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"80%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"10m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_innodb_buffer_pool_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"5m\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 300\n        &#125;\n      ],\n      \"thresholds\": \"90,95\",\n      \"title\": \"InnoDB Buffer Pool Size\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"cacheTimeout\": null,\n      \"colorBackground\": false,\n      \"colorValue\": true,\n      \"colors\": [\n        \"rgba(245, 54, 54, 0.9)\",\n        \"rgba(237, 129, 40, 0.89)\",\n        \"rgba(50, 172, 45, 0.97)\"\n      ],\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 0,\n      \"description\": \"**InnoDB Buffer Pool Size % of Total RAM**\\n\\nInnoDB maintains a storage area called the buffer pool for caching data and indexes in memory.  Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.\",\n      \"editable\": true,\n      \"error\": false,\n      \"format\": \"percent\",\n      \"gauge\": &#123;\n        \"maxValue\": 100,\n        \"minValue\": 0,\n        \"show\": false,\n        \"thresholdLabels\": false,\n        \"thresholdMarkers\": true\n      &#125;,\n      \"gridPos\": &#123;\n        \"h\": 4,\n        \"w\": 6,\n        \"x\": 18,\n        \"y\": 1\n      &#125;,\n      \"height\": \"125px\",\n      \"id\": 52,\n      \"interval\": \"$interval\",\n      \"links\": [\n        &#123;\n          \"targetBlank\": true,\n          \"title\": \"Tuning the InnoDB Buffer Pool Size\",\n          \"type\": \"absolute\",\n          \"url\": \"https://www.percona.com/blog/2015/06/02/80-ram-tune-innodb_buffer_pool_size/\"\n        &#125;\n      ],\n      \"mappingType\": 1,\n      \"mappingTypes\": [\n        &#123;\n          \"name\": \"value to text\",\n          \"value\": 1\n        &#125;,\n        &#123;\n          \"name\": \"range to text\",\n          \"value\": 2\n        &#125;\n      ],\n      \"maxDataPoints\": 100,\n      \"nullPointMode\": \"connected\",\n      \"nullText\": null,\n      \"postfix\": \"\",\n      \"postfixFontSize\": \"50%\",\n      \"prefix\": \"\",\n      \"prefixFontSize\": \"80%\",\n      \"rangeMaps\": [\n        &#123;\n          \"from\": \"null\",\n          \"text\": \"N/A\",\n          \"to\": \"null\"\n        &#125;\n      ],\n      \"repeat\": null,\n      \"sparkline\": &#123;\n        \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n        \"full\": false,\n        \"lineColor\": \"rgb(31, 120, 193)\",\n        \"show\": false\n      &#125;,\n      \"tableColumn\": \"\",\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"10m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"(mysql_global_variables_innodb_buffer_pool_size&#123;instance=\\\"$host\\\"&#125; * 100) / on (instance) node_memory_MemTotal&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"5m\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 300\n        &#125;\n      ],\n      \"thresholds\": \"40,80\",\n      \"title\": \"Buffer Pool Size of Total RAM\",\n      \"transparent\": false,\n      \"type\": \"singlestat\",\n      \"valueFontSize\": \"80%\",\n      \"valueMaps\": [],\n      \"valueName\": \"current\"\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 5\n      &#125;,\n      \"id\": 383,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Connections\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 0,\n      \"description\": \"**Max Connections** \\n\\nMax Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections.\\n\\nmysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root.\\n\\nMax Used Connections is the maximum number of connections that have been in use simultaneously since the server started.\\n\\nConnections is the number of connection attempts (successful or not) to the MySQL server.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 6\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 92,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"targetBlank\": true,\n          \"title\": \"MySQL Server System Variables\",\n          \"type\": \"absolute\",\n          \"url\": \"https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_connections\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Max Connections\",\n          \"fill\": 0\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"max(max_over_time(mysql_global_status_threads_connected&#123;instance=\\\"$host\\\"&#125;[$interval])  or mysql_global_status_threads_connected&#123;instance=\\\"$host\\\"&#125; )\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Connections\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_max_used_connections&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Max Used Connections\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_max_connections&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Max Connections\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Connections\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"cumulative\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Active Threads**\\n\\nThreads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 6\n      &#125;,\n      \"id\": 10,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Peak Threads Running\",\n          \"color\": \"#E24D42\",\n          \"lines\": false,\n          \"pointradius\": 1,\n          \"points\": true\n        &#125;,\n        &#123;\n          \"alias\": \"Peak Threads Connected\",\n          \"color\": \"#1F78C1\"\n        &#125;,\n        &#123;\n          \"alias\": \"Avg Threads Running\",\n          \"color\": \"#EAB839\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"max_over_time(mysql_global_status_threads_connected&#123;instance=\\\"$host\\\"&#125;[$interval]) or\\nmax_over_time(mysql_global_status_threads_connected&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Peak Threads Connected\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"max_over_time(mysql_global_status_threads_running&#123;instance=\\\"$host\\\"&#125;[$interval]) or\\nmax_over_time(mysql_global_status_threads_running&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Peak Threads Running\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"avg_over_time(mysql_global_status_threads_running&#123;instance=\\\"$host\\\"&#125;[$interval]) or \\navg_over_time(mysql_global_status_threads_running&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Avg Threads Running\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Client Thread Activity\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": [\n          \"total\"\n        ]\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"Threads\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": false\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 13\n      &#125;,\n      \"id\": 384,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Table Locks\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"description\": \"**MySQL Questions**\\n\\nThe number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. \\n\\nThis variable does not count the following commands:\\n* ``COM_PING``\\n* ``COM_STATISTICS``\\n* ``COM_STMT_PREPARE``\\n* ``COM_STMT_CLOSE``\\n* ``COM_STMT_RESET``\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 14\n      &#125;,\n      \"id\": 53,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"targetBlank\": true,\n          \"title\": \"MySQL Queries and Questions\",\n          \"type\": \"absolute\",\n          \"url\": \"https://www.percona.com/blog/2014/05/29/how-mysql-queries-and-questions-are-measured/\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_questions&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_questions&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Questions\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Questions\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Thread Cache**\\n\\nThe thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client's threads are put in the cache if the cache is not full. It is autosized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created.\\n\\n* *Threads_created*: The number of threads created to handle connections.\\n* *Threads_cached*: The number of threads in the thread cache.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 14\n      &#125;,\n      \"id\": 11,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Tuning information\",\n          \"type\": \"absolute\",\n          \"url\": \"https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_thread_cache_size\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Threads Created\",\n          \"fill\": 0\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_thread_cache_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Thread Cache Size\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_threads_cached&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Threads Cached\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_threads_created&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_threads_created&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Threads Created\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Thread Cache\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 21\n      &#125;,\n      \"id\": 385,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Temporary Objects\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 22\n      &#125;,\n      \"id\": 22,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_created_tmp_tables&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_created_tmp_tables&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Created Tmp Tables\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_created_tmp_disk_tables&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_created_tmp_disk_tables&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Created Tmp Disk Tables\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_created_tmp_files&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_created_tmp_files&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Created Tmp Files\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Temporary Objects\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Select Types**\\n\\nAs with most relational databases, selecting based on indexes is more efficient than scanning an entire table's data. Here we see the counters for selects not done with indexes.\\n\\n* ***Select Scan*** is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned.\\n* ***Select Range*** is how many queries used a range scan, which means MySQL scanned all rows in a given range.\\n* ***Select Full Join*** is the number of joins that are not joined on an index, this is usually a huge performance hit.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 22\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 311,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_select_full_join&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_select_full_join&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Select Full Join\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_select_full_range_join&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_select_full_range_join&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Select Full Range Join\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_select_range&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_select_range&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Select Range\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_select_range_check&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_select_range_check&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Select Range Check\",\n          \"metric\": \"\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_select_scan&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_select_scan&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Select Scan\",\n          \"metric\": \"\",\n          \"refId\": \"E\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Select Types\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 29\n      &#125;,\n      \"id\": 386,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Sorts\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Sorts**\\n\\nDue to a query's structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1.\\n\\nThis graph also shows when sorts had to scan a whole table or a given range of a table in order to return the results and which could not have been sorted via an index.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 30\n      &#125;,\n      \"id\": 30,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_sort_rows&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_sort_rows&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Sort Rows\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_sort_range&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_sort_range&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Sort Range\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_sort_merge_passes&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_sort_merge_passes&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Sort Merge Passes\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_sort_scan&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_sort_scan&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Sort Scan\",\n          \"metric\": \"\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Sorts\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Slow Queries**\\n\\nSlow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 30\n      &#125;,\n      \"id\": 48,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_slow_queries&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_slow_queries&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Slow Queries\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Slow Queries\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"cumulative\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 37\n      &#125;,\n      \"id\": 387,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Aborted\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**Aborted Connections**\\n\\nWhen a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema).\\n\\nIf the amount of failed requests without a successful connection reaches the value of max_connect_errors, mysqld assumes that something is wrong and blocks the host from further connection.\\n\\nTo allow connections from that host again, you need to issue the ``FLUSH HOSTS`` statement.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 38\n      &#125;,\n      \"id\": 47,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_aborted_connects&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_aborted_connects&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Aborted Connects (attempts)\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_aborted_clients&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_aborted_clients&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Aborted Clients (timeout)\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Aborted Connections\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"cumulative\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**Table Locks**\\n\\nMySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases.\\n\\nIt is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 38\n      &#125;,\n      \"id\": 32,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_table_locks_immediate&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_locks_immediate&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Table Locks Immediate\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_table_locks_waited&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_locks_waited&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Table Locks Waited\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Table Locks\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 45\n      &#125;,\n      \"id\": 388,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Network\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Network Traffic**\\n\\nHere we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 46\n      &#125;,\n      \"id\": 9,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_bytes_received&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_bytes_received&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Inbound\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_bytes_sent&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_bytes_sent&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Outbound\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Network Traffic\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"Bps\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"none\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": true,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Network Usage Hourly**\\n\\nHere we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 46\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 381,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"increase(mysql_global_status_bytes_received&#123;instance=\\\"$host\\\"&#125;[1h])\",\n          \"format\": \"time_series\",\n          \"interval\": \"1h\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Received\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 3600\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"increase(mysql_global_status_bytes_sent&#123;instance=\\\"$host\\\"&#125;[1h])\",\n          \"format\": \"time_series\",\n          \"interval\": \"1h\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Sent\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 3600\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": \"24h\",\n      \"timeShift\": null,\n      \"title\": \"MySQL Network Usage Hourly\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"none\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 53\n      &#125;,\n      \"id\": 389,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Memory\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 0,\n      \"description\": \"***System Memory***: Total Memory for the system.\\\\\\n***InnoDB Buffer Pool Data***: InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory.\\\\\\n***TokuDB Cache Size***: Similar in function to the InnoDB Buffer Pool,  TokuDB will allocate 50% of the installed RAM for its own cache.\\\\\\n***Key Buffer Size***: Index blocks for MYISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks.\\\\\\n***Adaptive Hash Index Size***: When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes.\\\\\\n ***Query Cache Size***: The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time.\\\\\\n***InnoDB Dictionary Size***: The data dictionary is InnoDB ‘s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running.\\\\\\n***InnoDB Log Buffer Size***: The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 54\n      &#125;,\n      \"id\": 50,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Detailed descriptions about metrics\",\n          \"type\": \"absolute\",\n          \"url\": \"https://www.percona.com/doc/percona-monitoring-and-management/dashboard.mysql-overview.html#mysql-internal-memory-overview\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"System Memory\",\n          \"fill\": 0,\n          \"stack\": false\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"expr\": \"node_memory_MemTotal&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"System Memory\",\n          \"refId\": \"G\",\n          \"step\": 4\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_status_innodb_page_size&#123;instance=\\\"$host\\\"&#125; * on (instance) mysql_global_status_buffer_pool_pages&#123;instance=\\\"$host\\\",state=\\\"data\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"InnoDB Buffer Pool Data\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_variables_innodb_log_buffer_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"InnoDB Log Buffer Size\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_variables_innodb_additional_mem_pool_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"InnoDB Additional Memory Pool Size\",\n          \"refId\": \"H\",\n          \"step\": 40\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_status_innodb_mem_dictionary&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"InnoDB Dictionary Size\",\n          \"refId\": \"F\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_variables_key_buffer_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Key Buffer Size\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_variables_query_cache_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Query Cache Size\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_status_innodb_mem_adaptive_hash&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Adaptive Hash Index Size\",\n          \"refId\": \"E\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_variables_tokudb_cache_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"TokuDB Cache Size\",\n          \"refId\": \"I\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Internal Memory Overview\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"bytes\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"label\": null,\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 61\n      &#125;,\n      \"id\": 390,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Command, Handlers, Processes\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**Top Command Counters**\\n\\nThe Com_&#123;&#123;xxx&#125;&#125; statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count [``DELETE``](https://dev.mysql.com/doc/refman/5.7/en/delete.html) and [``UPDATE``](https://dev.mysql.com/doc/refman/5.7/en/update.html) statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to [``DELETE``](https://dev.mysql.com/doc/refman/5.7/en/delete.html) and [``UPDATE``](https://dev.mysql.com/doc/refman/5.7/en/update.html) statements that use multiple-table syntax.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 62\n      &#125;,\n      \"id\": 14,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"hideZero\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Server Status Variables (Com_xxx)\",\n          \"type\": \"absolute\",\n          \"url\": \"https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#statvar_Com_xxx\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"topk(5, rate(mysql_global_status_commands_total&#123;instance=\\\"$host\\\"&#125;[$interval])>0) or topk(5, irate(mysql_global_status_commands_total&#123;instance=\\\"$host\\\"&#125;[5m])>0)\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Com_&#123;&#123; command &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Top Command Counters\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": true,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**Top Command Counters Hourly**\\n\\nThe Com_&#123;&#123;xxx&#125;&#125; statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count [``DELETE``](https://dev.mysql.com/doc/refman/5.7/en/delete.html) and [``UPDATE``](https://dev.mysql.com/doc/refman/5.7/en/update.html) statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to [``DELETE``](https://dev.mysql.com/doc/refman/5.7/en/delete.html) and [``UPDATE``](https://dev.mysql.com/doc/refman/5.7/en/update.html) statements that use multiple-table syntax.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 69\n      &#125;,\n      \"id\": 39,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"dashboard\": \"https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#statvar_Com_xxx\",\n          \"title\": \"Server Status Variables (Com_xxx)\",\n          \"type\": \"absolute\",\n          \"url\": \"https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#statvar_Com_xxx\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"topk(5, increase(mysql_global_status_commands_total&#123;instance=\\\"$host\\\"&#125;[1h])>0)\",\n          \"format\": \"time_series\",\n          \"interval\": \"1h\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Com_&#123;&#123; command &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 3600\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": \"24h\",\n      \"timeShift\": null,\n      \"title\": \"Top Command Counters Hourly\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Handlers**\\n\\nHandler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes.\\n\\nThis is in fact the layer between the Storage Engine and MySQL.\\n\\n* `read_rnd_next` is incremented when the server performs a full table scan and this is a counter you don't really want to see with a high value.\\n* `read_key` is incremented when a read is done with an index.\\n* `read_next` is incremented when the storage engine is asked to 'read the next index entry'. A high value means a lot of index scans are being done.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 76\n      &#125;,\n      \"id\": 8,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_handlers_total&#123;instance=\\\"$host\\\", handler!~\\\"commit|rollback|savepoint.*|prepare\\\"&#125;[$interval]) or irate(mysql_global_status_handlers_total&#123;instance=\\\"$host\\\", handler!~\\\"commit|rollback|savepoint.*|prepare\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123; handler &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"J\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Handlers\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 83\n      &#125;,\n      \"id\": 28,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_handlers_total&#123;instance=\\\"$host\\\", handler=~\\\"commit|rollback|savepoint.*|prepare\\\"&#125;[$interval]) or irate(mysql_global_status_handlers_total&#123;instance=\\\"$host\\\", handler=~\\\"commit|rollback|savepoint.*|prepare\\\"&#125;[5m])\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123; handler &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Transaction Handlers\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 0,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 90\n      &#125;,\n      \"id\": 40,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_info_schema_threads&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123; state &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Process States\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": true,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 97\n      &#125;,\n      \"id\": 49,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"hideZero\": true,\n        \"max\": true,\n        \"min\": false,\n        \"rightSide\": true,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"topk(5, avg_over_time(mysql_info_schema_threads&#123;instance=\\\"$host\\\"&#125;[1h]))\",\n          \"interval\": \"1h\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123; state &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 3600\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": \"24h\",\n      \"timeShift\": null,\n      \"title\": \"Top Process States Hourly\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 104\n      &#125;,\n      \"id\": 391,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Query Cache\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Query Cache Memory**\\n\\nThe query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE.\\n\\nThis also means that the larger the `query_cache_size` is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature.\\n\\nThe recommended settings for most environments is to set:\\n  ``query_cache_type=0``\\n  ``query_cache_size=0``\\n\\nNote that while you can dynamically change these values, to completely remove the contention point you have to restart the database.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 105\n      &#125;,\n      \"id\": 46,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_qcache_free_memory&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Free Memory\",\n          \"metric\": \"\",\n          \"refId\": \"F\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_query_cache_size&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Query Cache Size\",\n          \"metric\": \"\",\n          \"refId\": \"E\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Query Cache Memory\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Query Cache Activity**\\n\\nThe query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE.\\n\\nThis also means that the larger the `query_cache_size` is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature.\\n\\nThe recommended settings for most environments is to set:\\n``query_cache_type=0``\\n``query_cache_size=0``\\n\\nNote that while you can dynamically change these values, to completely remove the contention point you have to restart the database.\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 105\n      &#125;,\n      \"height\": \"\",\n      \"id\": 45,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_qcache_hits&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_qcache_hits&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Hits\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_qcache_inserts&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_qcache_inserts&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Inserts\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_qcache_not_cached&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_qcache_not_cached&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Not Cached\",\n          \"metric\": \"\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_qcache_lowmem_prunes&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_qcache_lowmem_prunes&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Prunes\",\n          \"metric\": \"\",\n          \"refId\": \"F\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_qcache_queries_in_cache&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Queries in Cache\",\n          \"metric\": \"\",\n          \"refId\": \"E\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Query Cache Activity\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 112\n      &#125;,\n      \"id\": 392,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Files and Tables\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 113\n      &#125;,\n      \"id\": 43,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_opened_files&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_opened_files&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Openings\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL File Openings\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 113\n      &#125;,\n      \"id\": 41,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_open_files&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Open Files\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_open_files_limit&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Open Files Limit\",\n          \"metric\": \"\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"mysql_global_status_innodb_num_open_files&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"InnoDB Open Files\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Open Files\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 120\n      &#125;,\n      \"id\": 393,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"Table Openings\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Table Open Cache Status**\\n\\nThe recommendation is to set the `table_open_cache_instances` to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached.\\n\\nThe `table_definition_cache` and `table_open_cache` can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 0,\n        \"y\": 121\n      &#125;,\n      \"id\": 44,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Server Status Variables (table_open_cache)\",\n          \"type\": \"absolute\",\n          \"url\": \"http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_open_cache\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Table Open Cache Hit Ratio\",\n          \"yaxis\": 2\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(mysql_global_status_opened_tables&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_opened_tables&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Openings\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"rate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Hits\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"rate(mysql_global_status_table_open_cache_misses&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_misses&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Misses\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"rate(mysql_global_status_table_open_cache_overflows&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_overflows&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Misses due to Overflows\",\n          \"refId\": \"D\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"(rate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[5m]))/((rate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_hits&#123;instance=\\\"$host\\\"&#125;[5m]))+(rate(mysql_global_status_table_open_cache_misses&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_table_open_cache_misses&#123;instance=\\\"$host\\\"&#125;[5m])))\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Table Open Cache Hit Ratio\",\n          \"refId\": \"E\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Table Open Cache Status\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"percentunit\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Open Tables**\\n\\nThe recommendation is to set the `table_open_cache_instances` to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached.\\n\\nThe `table_definition_cache` and `table_open_cache` can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 12,\n        \"x\": 12,\n        \"y\": 121\n      &#125;,\n      \"id\": 42,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Server Status Variables (table_open_cache)\",\n          \"type\": \"absolute\",\n          \"url\": \"http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_open_cache\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_open_tables&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Open Tables\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_table_open_cache&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Table Open Cache\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Open Tables\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 128\n      &#125;,\n      \"id\": 394,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"MySQL Table Definition Cache\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"description\": \"**MySQL Table Definition Cache**\\n\\nThe recommendation is to set the `table_open_cache_instances` to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached.\\n\\nThe `table_definition_cache` and `table_open_cache` can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).\",\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 129\n      &#125;,\n      \"id\": 54,\n      \"legend\": &#123;\n        \"alignAsTable\": true,\n        \"avg\": true,\n        \"current\": false,\n        \"max\": true,\n        \"min\": true,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [\n        &#123;\n          \"title\": \"Server Status Variables (table_open_cache)\",\n          \"type\": \"absolute\",\n          \"url\": \"http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_open_cache\"\n        &#125;\n      ],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Opened Table Definitions\",\n          \"yaxis\": 2\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_status_open_table_definitions&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Open Table Definitions\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"mysql_global_variables_table_definition_cache&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Table Definitions Cache Size\",\n          \"metric\": \"\",\n          \"refId\": \"C\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"rate(mysql_global_status_opened_table_definitions&#123;instance=\\\"$host\\\"&#125;[$interval]) or irate(mysql_global_status_opened_table_definitions&#123;instance=\\\"$host\\\"&#125;[5m])\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Opened Table Definitions\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"MySQL Table Definition Cache\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"short\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"collapsed\": false,\n      \"gridPos\": &#123;\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 136\n      &#125;,\n      \"id\": 395,\n      \"panels\": [],\n      \"repeat\": null,\n      \"title\": \"System Charts\",\n      \"type\": \"row\"\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 0,\n        \"y\": 137\n      &#125;,\n      \"id\": 31,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(node_vmstat_pgpgin&#123;instance=\\\"$host\\\"&#125;[$interval]) * 1024 or irate(node_vmstat_pgpgin&#123;instance=\\\"$host\\\"&#125;[5m]) * 1024\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Page In\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(node_vmstat_pgpgout&#123;instance=\\\"$host\\\"&#125;[$interval]) * 1024 or irate(node_vmstat_pgpgout&#123;instance=\\\"$host\\\"&#125;[5m]) * 1024\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Page Out\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"I/O Activity\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"Bps\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 8,\n        \"y\": 137\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 37,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"node_memory_MemTotal&#123;instance=\\\"$host\\\"&#125; - (node_memory_MemFree&#123;instance=\\\"$host\\\"&#125; + node_memory_Buffers&#123;instance=\\\"$host\\\"&#125; + node_memory_Cached&#123;instance=\\\"$host\\\"&#125;)\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Used\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"node_memory_MemFree&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Free\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"node_memory_Buffers&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Buffers\",\n          \"metric\": \"\",\n          \"refId\": \"D\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"node_memory_Cached&#123;instance=\\\"$host\\\"&#125;\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Cached\",\n          \"metric\": \"\",\n          \"refId\": \"E\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Memory Distribution\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"bytes\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;\n        \"Load 1m\": \"#58140C\",\n        \"Max Core Utilization\": \"#bf1b00\",\n        \"iowait\": \"#e24d42\",\n        \"nice\": \"#1f78c1\",\n        \"softirq\": \"#806eb7\",\n        \"system\": \"#eab839\",\n        \"user\": \"#508642\"\n      &#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 6,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 16,\n        \"y\": 137\n      &#125;,\n      \"height\": \"\",\n      \"id\": 2,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Max Core Utilization\",\n          \"lines\": false,\n          \"pointradius\": 1,\n          \"points\": true,\n          \"stack\": false\n        &#125;,\n        &#123;\n          \"alias\": \"Load 1m\",\n          \"color\": \"#58140C\",\n          \"fill\": 2,\n          \"stack\": false,\n          \"yaxis\": 2\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": true,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"clamp_max(((avg by (mode) ( (clamp_max(rate(node_cpu&#123;instance=\\\"$host\\\",mode!=\\\"idle\\\"&#125;[$interval]),1)) or (clamp_max(irate(node_cpu&#123;instance=\\\"$host\\\",mode!=\\\"idle\\\"&#125;[5m]),1)) ))*100 or (avg_over_time(node_cpu_average&#123;instance=~\\\"$host\\\", mode!=\\\"total\\\", mode!=\\\"idle\\\"&#125;[$interval]) or avg_over_time(node_cpu_average&#123;instance=~\\\"$host\\\", mode!=\\\"total\\\", mode!=\\\"idle\\\"&#125;[5m]))),100)\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"&#123;&#123; mode &#125;&#125;\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"clamp_max(max by () (sum  by (cpu) ( (clamp_max(rate(node_cpu&#123;instance=\\\"$host\\\",mode!=\\\"idle\\\",mode!=\\\"iowait\\\"&#125;[$interval]),1)) or (clamp_max(irate(node_cpu&#123;instance=\\\"$host\\\",mode!=\\\"idle\\\",mode!=\\\"iowait\\\"&#125;[5m]),1)) ))*100,100)\",\n          \"format\": \"time_series\",\n          \"hide\": true,\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Max Core Utilization\",\n          \"refId\": \"B\",\n          \"step\": 20\n        &#125;,\n        &#123;\n          \"expr\": \"node_load1&#123;instance=\\\"$host\\\"&#125;\",\n          \"format\": \"time_series\",\n          \"hide\": false,\n          \"intervalFactor\": 2,\n          \"legendFormat\": \"Load 1m\",\n          \"refId\": \"C\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"CPU Usage / Load\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"decimals\": 1,\n          \"format\": \"percent\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": 100,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"none\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": 2,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 0,\n        \"y\": 144\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 36,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": true,\n        \"hideZero\": true,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": false,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 1,\n      \"points\": true,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"sum((rate(node_disk_read_time_ms&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[$interval]) / rate(node_disk_reads_completed&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[$interval])) or (irate(node_disk_read_time_ms&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[5m]) / irate(node_disk_reads_completed&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[5m]))\\nor avg_over_time(aws_rds_read_latency_average&#123;instance=\\\"$host\\\"&#125;[$interval]) or avg_over_time(aws_rds_read_latency_average&#123;instance=\\\"$host\\\"&#125;[5m]))\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Read\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2m\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"sum((rate(node_disk_write_time_ms&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[$interval]) / rate(node_disk_writes_completed&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[$interval])) or (irate(node_disk_write_time_ms&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[5m]) / irate(node_disk_writes_completed&#123;device!~\\\"dm-.+\\\", instance=\\\"$host\\\"&#125;[5m])) or \\navg_over_time(aws_rds_write_latency_average&#123;instance=\\\"$host\\\"&#125;[$interval]) or avg_over_time(aws_rds_write_latency_average&#123;instance=\\\"$host\\\"&#125;[5m]))\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Write\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Disk Latency\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"ms\",\n          \"label\": \"\",\n          \"logBase\": 2,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"ms\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 8,\n        \"y\": 144\n      &#125;,\n      \"height\": \"250px\",\n      \"id\": 21,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [\n        &#123;\n          \"alias\": \"Outbound\",\n          \"transform\": \"negative-Y\"\n        &#125;\n      ],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"sum(rate(node_network_receive_bytes&#123;instance=\\\"$host\\\", device!=\\\"lo\\\"&#125;[$interval])) or sum(irate(node_network_receive_bytes&#123;instance=\\\"$host\\\", device!=\\\"lo\\\"&#125;[5m])) or sum(max_over_time(rdsosmetrics_network_rx&#123;instance=\\\"$host\\\"&#125;[$interval])) or sum(max_over_time(rdsosmetrics_network_rx&#123;instance=\\\"$host\\\"&#125;[5m])) \",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Inbound\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"sum(rate(node_network_transmit_bytes&#123;instance=\\\"$host\\\", device!=\\\"lo\\\"&#125;[$interval])) or sum(irate(node_network_transmit_bytes&#123;instance=\\\"$host\\\", device!=\\\"lo\\\"&#125;[5m])) or\\nsum(max_over_time(rdsosmetrics_network_tx&#123;instance=\\\"$host\\\"&#125;[$interval])) or sum(max_over_time(rdsosmetrics_network_tx&#123;instance=\\\"$host\\\"&#125;[5m]))\",\n          \"format\": \"time_series\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Outbound\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Network Traffic\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"Bps\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": null,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;,\n    &#123;\n      \"aliasColors\": &#123;&#125;,\n      \"bars\": false,\n      \"dashLength\": 10,\n      \"dashes\": false,\n      \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n      \"decimals\": null,\n      \"editable\": true,\n      \"error\": false,\n      \"fill\": 2,\n      \"grid\": &#123;&#125;,\n      \"gridPos\": &#123;\n        \"h\": 7,\n        \"w\": 8,\n        \"x\": 16,\n        \"y\": 144\n      &#125;,\n      \"id\": 38,\n      \"legend\": &#123;\n        \"alignAsTable\": false,\n        \"avg\": true,\n        \"current\": false,\n        \"hideEmpty\": false,\n        \"max\": false,\n        \"min\": false,\n        \"rightSide\": false,\n        \"show\": true,\n        \"sort\": \"avg\",\n        \"sortDesc\": true,\n        \"total\": false,\n        \"values\": true\n      &#125;,\n      \"lines\": true,\n      \"linewidth\": 2,\n      \"links\": [],\n      \"nullPointMode\": \"null\",\n      \"percentage\": false,\n      \"pointradius\": 5,\n      \"points\": false,\n      \"renderer\": \"flot\",\n      \"seriesOverrides\": [],\n      \"spaceLength\": 10,\n      \"stack\": false,\n      \"steppedLine\": false,\n      \"targets\": [\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(node_vmstat_pswpin&#123;instance=\\\"$host\\\"&#125;[$interval]) * 4096 or irate(node_vmstat_pswpin&#123;instance=\\\"$host\\\"&#125;[5m]) * 4096\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Swap In (Reads)\",\n          \"metric\": \"\",\n          \"refId\": \"A\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;,\n        &#123;\n          \"calculatedInterval\": \"2s\",\n          \"datasourceErrors\": &#123;&#125;,\n          \"errors\": &#123;&#125;,\n          \"expr\": \"rate(node_vmstat_pswpout&#123;instance=\\\"$host\\\"&#125;[$interval]) * 4096 or irate(node_vmstat_pswpout&#123;instance=\\\"$host\\\"&#125;[5m]) * 4096\",\n          \"interval\": \"$interval\",\n          \"intervalFactor\": 1,\n          \"legendFormat\": \"Swap Out (Writes)\",\n          \"metric\": \"\",\n          \"refId\": \"B\",\n          \"step\": 20,\n          \"target\": \"\"\n        &#125;\n      ],\n      \"thresholds\": [],\n      \"timeFrom\": null,\n      \"timeShift\": null,\n      \"title\": \"Swap Activity\",\n      \"tooltip\": &#123;\n        \"msResolution\": false,\n        \"shared\": true,\n        \"sort\": 0,\n        \"value_type\": \"individual\"\n      &#125;,\n      \"transparent\": false,\n      \"type\": \"graph\",\n      \"xaxis\": &#123;\n        \"buckets\": null,\n        \"mode\": \"time\",\n        \"name\": null,\n        \"show\": true,\n        \"values\": []\n      &#125;,\n      \"yaxes\": [\n        &#123;\n          \"format\": \"Bps\",\n          \"label\": \"\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;,\n        &#123;\n          \"format\": \"bytes\",\n          \"logBase\": 1,\n          \"max\": null,\n          \"min\": 0,\n          \"show\": true\n        &#125;\n      ],\n      \"yaxis\": &#123;\n        \"align\": false,\n        \"alignLevel\": null\n      &#125;\n    &#125;\n  ],\n  \"refresh\": \"1m\",\n  \"schemaVersion\": 16,\n  \"style\": \"dark\",\n  \"tags\": [\n    \"Percona\",\n    \"MySQL\"\n  ],\n  \"templating\": &#123;\n    \"list\": [\n      &#123;\n        \"allFormat\": \"glob\",\n        \"auto\": true,\n        \"auto_count\": 200,\n        \"auto_min\": \"1s\",\n        \"current\": &#123;\n          \"text\": \"auto\",\n          \"value\": \"$__auto_interval_interval\"\n        &#125;,\n        \"datasource\": \"Prometheus\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"Interval\",\n        \"multi\": false,\n        \"multiFormat\": \"glob\",\n        \"name\": \"interval\",\n        \"options\": [\n          &#123;\n            \"selected\": true,\n            \"text\": \"auto\",\n            \"value\": \"$__auto_interval_interval\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1s\",\n            \"value\": \"1s\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"5s\",\n            \"value\": \"5s\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1m\",\n            \"value\": \"1m\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"5m\",\n            \"value\": \"5m\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1h\",\n            \"value\": \"1h\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"6h\",\n            \"value\": \"6h\"\n          &#125;,\n          &#123;\n            \"selected\": false,\n            \"text\": \"1d\",\n            \"value\": \"1d\"\n          &#125;\n        ],\n        \"query\": \"1s,5s,1m,5m,1h,6h,1d\",\n        \"refresh\": 2,\n        \"type\": \"interval\"\n      &#125;,\n      &#123;\n        \"allFormat\": \"glob\",\n        \"allValue\": null,\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROMETHEUS&#125;\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"label\": \"Host\",\n        \"multi\": false,\n        \"multiFormat\": \"regex values\",\n        \"name\": \"host\",\n        \"options\": [],\n        \"query\": \"label_values(mysql_up, instance)\",\n        \"refresh\": 1,\n        \"refresh_on_load\": false,\n        \"regex\": \"\",\n        \"sort\": 1,\n        \"tagValuesQuery\": null,\n        \"tags\": [],\n        \"tagsQuery\": null,\n        \"type\": \"query\",\n        \"useTags\": false\n      &#125;\n    ]\n  &#125;,\n  \"time\": &#123;\n    \"from\": \"now/d\",\n    \"to\": \"now/d\"\n  &#125;,\n  \"timepicker\": &#123;\n    \"collapse\": false,\n    \"enable\": true,\n    \"hidden\": false,\n    \"notice\": false,\n    \"now\": true,\n    \"refresh_intervals\": [\n      \"5s\",\n      \"10s\",\n      \"30s\",\n      \"1m\",\n      \"5m\",\n      \"15m\",\n      \"30m\",\n      \"1h\",\n      \"2h\",\n      \"1d\"\n    ],\n    \"status\": \"Stable\",\n    \"time_options\": [\n      \"5m\",\n      \"15m\",\n      \"1h\",\n      \"6h\",\n      \"12h\",\n      \"24h\",\n      \"2d\",\n      \"7d\",\n      \"30d\"\n    ],\n    \"type\": \"timepicker\"\n  &#125;,\n  \"timezone\": \"browser\",\n  \"title\": \"MySQL Overview\",\n  \"uid\": \"MQWgroiiz\",\n  \"version\": 1\n&#125;\n   \n\n\n\n3.监控Redis（redis_exporter） 安装redis_exporter \ntar -C /usr/local/ -xvf redis_exporter-v0.15.0.linux-amd64.tar\n\n 启动redis_exporter \n默认redis_exporter端口为9121\n/usr/local/redis_exporter redis//192.168.0.103:6379 &amp; -web.listenaddress 192.168.0.103:9121\n\n prometheus配置文件中加入redis监控并重启 \nvim /usr/local/Prometheus/prometheus.yml# 默认redis-exporter端口为9121- job_name: &#x27;Redis&#x27;    static_configs:    - targets: [&#x27;192.168.0.103:9121&#x27;]\n\n grafana导入画好的dashboard \n   dashboard-redis.json   \n&#123;\n  \"__inputs\": [\n    &#123;\n      \"name\": \"DS_PROM\",\n      \"label\": \"prom\",\n      \"description\": \"Prometheus Data Source\",\n      \"type\": \"datasource\",\n      \"pluginId\": \"prometheus\",\n      \"pluginName\": \"Prometheus\"\n    &#125;\n  ],\n  \"__requires\": [\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"singlestat\",\n      \"name\": \"Singlestat\",\n      \"version\": \"\"\n    &#125;,\n    &#123;\n      \"type\": \"panel\",\n      \"id\": \"graph\",\n      \"name\": \"Graph\",\n      \"version\": \"\"\n    &#125;,\n    &#123;\n      \"type\": \"grafana\",\n      \"id\": \"grafana\",\n      \"name\": \"Grafana\",\n      \"version\": \"3.1.1\"\n    &#125;,\n    &#123;\n      \"type\": \"datasource\",\n      \"id\": \"prometheus\",\n      \"name\": \"Prometheus\",\n      \"version\": \"1.0.0\"\n    &#125;\n  ],\n  \"id\": null,\n  \"title\": \"Prometheus Redis\",\n  \"description\": \"Prometheus dashboard for Redis servers\",\n  \"tags\": [\n    \"prometheus\",\n    \"redis\"\n  ],\n  \"style\": \"dark\",\n  \"timezone\": \"browser\",\n  \"editable\": true,\n  \"hideControls\": false,\n  \"sharedCrosshair\": false,\n  \"rows\": [\n    &#123;\n      \"collapse\": false,\n      \"editable\": true,\n      \"height\": \"250px\",\n      \"panels\": [\n        &#123;\n          \"cacheTimeout\": null,\n          \"colorBackground\": false,\n          \"colorValue\": false,\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"decimals\": 0,\n          \"editable\": true,\n          \"error\": false,\n          \"format\": \"s\",\n          \"gauge\": &#123;\n            \"maxValue\": 100,\n            \"minValue\": 0,\n            \"show\": false,\n            \"thresholdLabels\": false,\n            \"thresholdMarkers\": true\n          &#125;,\n          \"id\": 9,\n          \"interval\": null,\n          \"isNew\": true,\n          \"links\": [],\n          \"mappingType\": 1,\n          \"mappingTypes\": [\n            &#123;\n              \"name\": \"value to text\",\n              \"value\": 1\n            &#125;,\n            &#123;\n              \"name\": \"range to text\",\n              \"value\": 2\n            &#125;\n          ],\n          \"maxDataPoints\": 100,\n          \"nullPointMode\": \"connected\",\n          \"nullText\": null,\n          \"postfix\": \"\",\n          \"postfixFontSize\": \"50%\",\n          \"prefix\": \"\",\n          \"prefixFontSize\": \"50%\",\n          \"rangeMaps\": [\n            &#123;\n              \"from\": \"null\",\n              \"text\": \"N/A\",\n              \"to\": \"null\"\n            &#125;\n          ],\n          \"span\": 1,\n          \"sparkline\": &#123;\n            \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n            \"full\": false,\n            \"lineColor\": \"rgb(31, 120, 193)\",\n            \"show\": false\n          &#125;,\n          \"targets\": [\n            &#123;\n              \"expr\": \"redis_uptime_in_seconds&#123;addr=\\\"$addr\\\"&#125;\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 1800\n            &#125;\n          ],\n          \"thresholds\": \"\",\n          \"title\": \"Uptime\",\n          \"type\": \"singlestat\",\n          \"valueFontSize\": \"70%\",\n          \"valueMaps\": [\n            &#123;\n              \"op\": \"=\",\n              \"text\": \"N/A\",\n              \"value\": \"null\"\n            &#125;\n          ],\n          \"valueName\": \"avg\"\n        &#125;,\n        &#123;\n          \"cacheTimeout\": null,\n          \"colorBackground\": false,\n          \"colorValue\": false,\n          \"colors\": [\n            \"rgba(245, 54, 54, 0.9)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(50, 172, 45, 0.97)\"\n          ],\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"decimals\": 0,\n          \"editable\": true,\n          \"error\": false,\n          \"format\": \"none\",\n          \"gauge\": &#123;\n            \"maxValue\": 100,\n            \"minValue\": 0,\n            \"show\": false,\n            \"thresholdLabels\": false,\n            \"thresholdMarkers\": true\n          &#125;,\n          \"hideTimeOverride\": true,\n          \"id\": 12,\n          \"interval\": null,\n          \"isNew\": true,\n          \"links\": [],\n          \"mappingType\": 1,\n          \"mappingTypes\": [\n            &#123;\n              \"name\": \"value to text\",\n              \"value\": 1\n            &#125;,\n            &#123;\n              \"name\": \"range to text\",\n              \"value\": 2\n            &#125;\n          ],\n          \"maxDataPoints\": 100,\n          \"nullPointMode\": \"connected\",\n          \"nullText\": null,\n          \"postfix\": \"\",\n          \"postfixFontSize\": \"50%\",\n          \"prefix\": \"\",\n          \"prefixFontSize\": \"50%\",\n          \"rangeMaps\": [\n            &#123;\n              \"from\": \"null\",\n              \"text\": \"N/A\",\n              \"to\": \"null\"\n            &#125;\n          ],\n          \"span\": 1,\n          \"sparkline\": &#123;\n            \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n            \"full\": false,\n            \"lineColor\": \"rgb(31, 120, 193)\",\n            \"show\": true\n          &#125;,\n          \"targets\": [\n            &#123;\n              \"expr\": \"redis_connected_clients&#123;addr=\\\"$addr\\\"&#125;\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 2\n            &#125;\n          ],\n          \"thresholds\": \"\",\n          \"timeFrom\": \"1m\",\n          \"timeShift\": null,\n          \"title\": \"Clients\",\n          \"type\": \"singlestat\",\n          \"valueFontSize\": \"80%\",\n          \"valueMaps\": [\n            &#123;\n              \"op\": \"=\",\n              \"text\": \"N/A\",\n              \"value\": \"null\"\n            &#125;\n          ],\n          \"valueName\": \"current\"\n        &#125;,\n        &#123;\n          \"cacheTimeout\": null,\n          \"colorBackground\": false,\n          \"colorValue\": false,\n          \"colors\": [\n            \"rgba(50, 172, 45, 0.97)\",\n            \"rgba(237, 129, 40, 0.89)\",\n            \"rgba(245, 54, 54, 0.9)\"\n          ],\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"decimals\": 0,\n          \"editable\": true,\n          \"error\": false,\n          \"format\": \"percent\",\n          \"gauge\": &#123;\n            \"maxValue\": 100,\n            \"minValue\": 0,\n            \"show\": true,\n            \"thresholdLabels\": false,\n            \"thresholdMarkers\": true\n          &#125;,\n          \"hideTimeOverride\": true,\n          \"id\": 11,\n          \"interval\": null,\n          \"isNew\": true,\n          \"links\": [],\n          \"mappingType\": 1,\n          \"mappingTypes\": [\n            &#123;\n              \"name\": \"value to text\",\n              \"value\": 1\n            &#125;,\n            &#123;\n              \"name\": \"range to text\",\n              \"value\": 2\n            &#125;\n          ],\n          \"maxDataPoints\": 100,\n          \"nullPointMode\": \"connected\",\n          \"nullText\": null,\n          \"postfix\": \"\",\n          \"postfixFontSize\": \"50%\",\n          \"prefix\": \"\",\n          \"prefixFontSize\": \"50%\",\n          \"rangeMaps\": [\n            &#123;\n              \"from\": \"null\",\n              \"text\": \"N/A\",\n              \"to\": \"null\"\n            &#125;\n          ],\n          \"span\": 2,\n          \"sparkline\": &#123;\n            \"fillColor\": \"rgba(31, 118, 189, 0.18)\",\n            \"full\": false,\n            \"lineColor\": \"rgb(31, 120, 193)\",\n            \"show\": true\n          &#125;,\n          \"targets\": [\n            &#123;\n              \"expr\": \"100 * (redis_memory_used_bytes&#123;addr=~\\\"$addr\\\"&#125;  / redis_config_maxmemory&#123;addr=~\\\"$addr\\\"&#125; )\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 2\n            &#125;\n          ],\n          \"thresholds\": \"80,95\",\n          \"timeFrom\": \"1m\",\n          \"timeShift\": null,\n          \"title\": \"Memory Usage\",\n          \"type\": \"singlestat\",\n          \"valueFontSize\": \"80%\",\n          \"valueMaps\": [\n            &#123;\n              \"op\": \"=\",\n              \"text\": \"N/A\",\n              \"value\": \"null\"\n            &#125;\n          ],\n          \"valueName\": \"current\"\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 1,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 2,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": false,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 4,\n          \"stack\": false,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"rate(redis_commands_processed_total&#123;addr=~\\\"$addr\\\"&#125;[5m])\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"\",\n              \"metric\": \"A\",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Commands Executed / sec\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"cumulative\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"decimals\": 2,\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 1,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 1,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": false,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": true,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 4,\n          \"stack\": false,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"irate(redis_keyspace_hits_total&#123;addr=\\\"$addr\\\"&#125;[5m])\",\n              \"hide\": false,\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"hits\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;,\n            &#123;\n              \"expr\": \"irate(redis_keyspace_misses_total&#123;addr=\\\"$addr\\\"&#125;[5m])\",\n              \"hide\": false,\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"misses\",\n              \"metric\": \"\",\n              \"refId\": \"B\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Hits / Misses per Sec\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"individual\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"short\",\n              \"label\": \"\",\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": 0,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;\n      ],\n      \"title\": \"Row\"\n    &#125;,\n    &#123;\n      \"collapse\": false,\n      \"editable\": true,\n      \"height\": \"250px\",\n      \"panels\": [\n        &#123;\n          \"aliasColors\": &#123;\n            \"max\": \"#BF1B00\"\n          &#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 1,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 7,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"hideEmpty\": false,\n            \"hideZero\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": true,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"null as zero\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 6,\n          \"stack\": false,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"redis_memory_used_bytes&#123;addr=~\\\"$addr\\\"&#125; \",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"used\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;,\n            &#123;\n              \"expr\": \"redis_config_maxmemory&#123;addr=~\\\"$addr\\\"&#125; \",\n              \"hide\": false,\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"max\",\n              \"refId\": \"B\",\n              \"step\": 240\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Total Memory Usage\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"cumulative\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"bytes\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": 0,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 1,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 10,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": true,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 6,\n          \"stack\": false,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"rate(redis_net_input_bytes_total&#123;addr=\\\"$addr\\\"&#125;[5m])\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"&#123;&#123; input &#125;&#125;\",\n              \"refId\": \"A\",\n              \"step\": 240\n            &#125;,\n            &#123;\n              \"expr\": \"rate(redis_net_output_bytes_total&#123;addr=\\\"$addr\\\"&#125;[5m])\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"&#123;&#123; output &#125;&#125;\",\n              \"refId\": \"B\",\n              \"step\": 240\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Network I/O\",\n          \"tooltip\": &#123;\n            \"msResolution\": true,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"cumulative\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"bytes\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;\n      ],\n      \"title\": \"New row\"\n    &#125;,\n    &#123;\n      \"collapse\": false,\n      \"editable\": true,\n      \"height\": \"250px\",\n      \"panels\": [\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 7,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 5,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"alignAsTable\": true,\n            \"avg\": false,\n            \"current\": true,\n            \"max\": false,\n            \"min\": false,\n            \"rightSide\": true,\n            \"show\": true,\n            \"total\": false,\n            \"values\": true\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 6,\n          \"stack\": true,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"sum (redis_db_keys&#123;addr=~\\\"$addr\\\"&#125;) by (db)\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"&#123;&#123; db &#125;&#125; \",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Total Items per DB\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"individual\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"none\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 7,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 13,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": true,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 6,\n          \"stack\": true,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"sum (redis_db_keys&#123;addr=~\\\"$addr\\\"&#125;) - sum (redis_db_keys_expiring&#123;addr=~\\\"$addr\\\"&#125;) \",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"not expiring\",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;,\n            &#123;\n              \"expr\": \"sum (redis_db_keys_expiring&#123;addr=~\\\"$addr\\\"&#125;) \",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"expiring\",\n              \"metric\": \"\",\n              \"refId\": \"B\",\n              \"step\": 240\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Expiring vs Not-Expiring Keys\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"individual\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;\n            \"evicts\": \"#890F02\",\n            \"memcached_items_evicted_total&#123;instance=\\\"172.17.0.1:9150\\\",job=\\\"prometheus\\\"&#125;\": \"#890F02\",\n            \"reclaims\": \"#3F6833\"\n          &#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 1,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 8,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": true,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 2,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [\n            &#123;\n              \"alias\": \"reclaims\",\n              \"yaxis\": 2\n            &#125;\n          ],\n          \"span\": 6,\n          \"stack\": false,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"sum(rate(redis_expired_keys_total&#123;addr=~\\\"$addr\\\"&#125;[5m])) by (addr)\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"expired\",\n              \"metric\": \"\",\n              \"refId\": \"A\",\n              \"step\": 240,\n              \"target\": \"\"\n            &#125;,\n            &#123;\n              \"expr\": \"sum(rate(redis_evicted_keys_total&#123;addr=~\\\"$addr\\\"&#125;[5m])) by (addr)\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"evicted\",\n              \"refId\": \"B\",\n              \"step\": 240\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Expired / Evicted\",\n          \"tooltip\": &#123;\n            \"msResolution\": false,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"cumulative\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;,\n        &#123;\n          \"aliasColors\": &#123;&#125;,\n          \"bars\": false,\n          \"datasource\": \"$&#123;DS_PROM&#125;\",\n          \"editable\": true,\n          \"error\": false,\n          \"fill\": 8,\n          \"grid\": &#123;\n            \"threshold1\": null,\n            \"threshold1Color\": \"rgba(216, 200, 27, 0.27)\",\n            \"threshold2\": null,\n            \"threshold2Color\": \"rgba(234, 112, 112, 0.22)\"\n          &#125;,\n          \"id\": 14,\n          \"isNew\": true,\n          \"legend\": &#123;\n            \"avg\": false,\n            \"current\": false,\n            \"max\": false,\n            \"min\": false,\n            \"show\": true,\n            \"total\": false,\n            \"values\": false\n          &#125;,\n          \"lines\": true,\n          \"linewidth\": 1,\n          \"links\": [],\n          \"nullPointMode\": \"connected\",\n          \"percentage\": false,\n          \"pointradius\": 5,\n          \"points\": false,\n          \"renderer\": \"flot\",\n          \"seriesOverrides\": [],\n          \"span\": 6,\n          \"stack\": true,\n          \"steppedLine\": false,\n          \"targets\": [\n            &#123;\n              \"expr\": \"topk(5, irate(redis_command_call_duration_seconds_count&#123;addr=~\\\"$addr\\\"&#125; [1m]))\",\n              \"interval\": \"\",\n              \"intervalFactor\": 2,\n              \"legendFormat\": \"&#123;&#123; cmd &#125;&#125;\",\n              \"metric\": \"redis_command_calls_total\",\n              \"refId\": \"A\",\n              \"step\": 240\n            &#125;\n          ],\n          \"timeFrom\": null,\n          \"timeShift\": null,\n          \"title\": \"Command Calls / sec\",\n          \"tooltip\": &#123;\n            \"msResolution\": true,\n            \"shared\": true,\n            \"sort\": 0,\n            \"value_type\": \"cumulative\"\n          &#125;,\n          \"type\": \"graph\",\n          \"xaxis\": &#123;\n            \"show\": true\n          &#125;,\n          \"yaxes\": [\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;,\n            &#123;\n              \"format\": \"short\",\n              \"label\": null,\n              \"logBase\": 1,\n              \"max\": null,\n              \"min\": null,\n              \"show\": true\n            &#125;\n          ]\n        &#125;\n      ],\n      \"title\": \"New row\"\n    &#125;\n  ],\n  \"time\": &#123;\n    \"from\": \"now-24h\",\n    \"to\": \"now\"\n  &#125;,\n  \"timepicker\": &#123;\n    \"refresh_intervals\": [\n      \"5s\",\n      \"10s\",\n      \"30s\",\n      \"1m\",\n      \"5m\",\n      \"15m\",\n      \"30m\",\n      \"1h\",\n      \"2h\",\n      \"1d\"\n    ],\n    \"time_options\": [\n      \"5m\",\n      \"15m\",\n      \"1h\",\n      \"6h\",\n      \"12h\",\n      \"24h\",\n      \"2d\",\n      \"7d\",\n      \"30d\"\n    ]\n  &#125;,\n  \"templating\": &#123;\n    \"list\": [\n      &#123;\n        \"current\": &#123;&#125;,\n        \"datasource\": \"$&#123;DS_PROM&#125;\",\n        \"hide\": 0,\n        \"includeAll\": false,\n        \"multi\": false,\n        \"name\": \"addr\",\n        \"options\": [],\n        \"query\": \"label_values(redis_connected_clients, addr)\",\n        \"refresh\": 1,\n        \"regex\": \"\",\n        \"type\": \"query\"\n      &#125;\n    ]\n  &#125;,\n  \"annotations\": &#123;\n    \"list\": []\n  &#125;,\n  \"refresh\": \"30s\",\n  \"schemaVersion\": 12,\n  \"version\": 52,\n  \"links\": [],\n  \"gnetId\": 763\n&#125;\n   \n\n\n\n\n\n原文来自 咖啡那么浓\n","categories":["Prometheus"],"tags":["Prometheus"]},{"title":"Python3安装报错","url":"/20241121/Python/9a41e0bcb93f/","content":"Python3安装报错报错1(1).报错内容[root@localhost ~]# python3Could not find platform independent libraries &lt;prefix&gt;Could not find platform dependent libraries &lt;exec prefix&gt;Consider setting SPYTHONHOME to &lt;prefix&gt;[:&lt;exec prefix&gt;]Fatal Python error: Py_Initialize: Unable to get the locale encodinModuleNotFoundiA-ror: No module named &#x27;encodings&#x27;Current thread 0x80007/218eec9740 (most recent call first):Aborted\n\n\n(2).报错原因同一服务器安装了多版本的python版本\n[root@localhost ~]# ll /usr/bin/python*lrwxrwxrwx. 1 root root        7 11月 27 16:41 /usr/bin/python -&gt; python2lrwxrwxrwx. 1 root root        9 11月 27 16:41 /usr/bin/python2 -&gt; python2.7-rwxr-xr-x. 1 root root     7216 10月 31 2018 /usr/bin/python2.7lrwxrwxrwx. 1 root root        9 1月  24 17:19 /usr/bin/python3 -&gt; python3.6-rwxr-xr-x. 1 root root 12711504 1月  24 09:38 /usr/bin/python3.6lrwxrwxrwx. 1 root root       17 1月  24 17:19 /usr/bin/python3.6-config -&gt; python3.6m-config-rwxr-xr-x. 1 root root 12711504 1月  24 09:38 /usr/bin/python3.6m-rwxr-xr-x. 1 root root     3105 1月  24 09:38 /usr/bin/python3.6m-configlrwxrwxrwx. 1 root root       16 1月  24 17:19 /usr/bin/python3-config -&gt; python3.6-config\n\n(3).解决方案添加环境变量：\nvi /etc/profile# 最后一行添加(注意安装路径,可能在其他路径,不知道可以用find / -name python3.6查找)export PYTHONPATH=/usr/lib/python3.6export PYTHONHOME=/usr/bin/python3.6export PYTHONHOME=$PYTHONHOME:/usr/lib/python3.6/site-packages\n\n加载环境变量\nsource etc/profile\n\n执行python3命令\n[root@localhost ~]# python3Python 3.6.8 (default, Jan 21 2022, 15:07:01) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; \n\n报错2(1).报错内容在CentOS7上安装完Python3后，运行python3时提示libpython3.7m.so.1.0不存在。我们可以看到以下错误信息[root@carbon ~]# python3python3: error while loading shared libraries: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory\n\n(2).报错原因由于安装完Python3后，没有将libpython3.7m.so.1.0放入/usr/lib64中，导致初始化时无法加载该文件。解决方法也非常简单，我们只需要把文件复制到/usr/lib64目录下。需要注意的是本文的系统是64位的操作系统，并且安装的python也是64位的。\n\n(3).解决方案复制libpython3.7m.so.1.0文件到&#x2F;usr&#x2F;lib64下\ncp /usr/local/src/Python-3.7.4/libpython3.7m.so.1.0 /usr/lib64\n\n\n","categories":["Python"],"tags":["Python"]},{"title":"Shell命令行","url":"/20241121/CentOS/a7892ea08df4/","content":"Shell命令行\n\nThe End\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"Spark集群搭建","url":"/20241121/Hadoop/2ead831a117f/","content":"Spark集群搭建在安装spark之前，需要安装hadoop集群环境\n集群列表\n\n\n服务器\n地址\n角色\n备注\n\n\n\nhadoop1\n192.168.11.81\nmaster\n32G 12C 800G\n\n\nhadoop2\n192.168.11.82\nslaves\n32G 12C 800G\n\n\nhadoop3\n192.168.11.83\nslaves\n32G 12C 800G\n\n\n一.基础环境设置关闭防火墙systemctl stop firewalldsystemctl disable firewalld\n\n关闭selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/configsetenforce 0\n\n添加hostsvi /etc/hosts192.168.11.81 hadoop1192.168.11.82 hadoop2192.168.11.83 hadoop3\n\n二.安装spark集群下载spark包https://archive.apache.org/dist/spark# 安装包地址https://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.6.tgz \n\n上传解压spark安装包tar -xzvf spark-2.4.2-bin-hadoop2.6.tgz -C /data\n\n配置环境变量cat &gt;&gt;/etc/profile &lt;&lt;EOF# sparkexport SPARK_HOME=/data/spark-2.4.2export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinEOFsource /etc/profile\n\n配置spark参数配置文件放在$SPARK_HOME&#x2F;conf下\ncd $SPARK_HOME/confcp spark-env.sh.template spark-env.shvi spark-env.sh# 添加# sparkexport JAVA_HOME=/jdk1.8.0_271export SCALA_HOME=/scala-2.12.4/export HADOOP_HOME=/data/hadoop-2.6.4export HADOOP_CONF_DIR=/data/hadoop-2.6.4/etc/hadoopSPARK_MASTER_IP=hadoop1SPARK_LOCAL_DIRS=/data/spark-2.4.2SPARK_DRIVER_MEMORY=1GPYSPARK_PYTHON=/usr/bin/python3.6\n\n配置工作节点\ncp slaves.template slavesvi slaves# 添加hadoop2hadoop3\n\n注:以上配置需要复制到其它节点中\n启动服务启动hadoop后 在启动spark集群\ncd $SPARK_HOME/sbin/./start-all.sh./start-history-server.sh\n\n关闭服务cd $SPARK_HOME/sbin/./stop-all.sh./stop-history-server.sh\n\n四.web可视化# 资源管理http://192.168.11.81:8080/\n\n五.安装scalaspark既可以使用Scala作为开发语言，也可以使用python作为开发语言。\nspark中已经默认带有scala，如果没有或者要安装其他版本可以下载安装包安装。\n下载scala安装包https://www.scala-lang.org/download/\n\n上传解压scala安装包tar -zxvf scala-2.12.4.tgz -C /\n\n添加环境变量cat &gt;&gt;/etc/profile &lt;&lt;EOF# scalaexport SCALA_HOME=/scala-2.12.4/export PATH=$PATH:$SCALA_HOME/binEOFsource /etc/profile\n\n注:其它节点中也要配置安装scala\n测试是否安装成功[root@hadoop1 pkg]# scala -versionScala code runner version 2.12.4 -- Copyright 2002-2017, LAMP/EPFL and Lightbend, Inc.\n\n启动Spark shell界面spark-shell --master spark://hadoop1:7077# 服务器默认安装python2，若需要python3 自行安装pyspark --master spark://hadoop1:7077\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"Web版SSH GateOne","url":"/20241121/CentOS/ab09246c57ee/","content":"Web版SSH  GateOneGateOne 是一款使用 HTML5 技术编写的网页版 SSH 终端模拟器。基于现代的 HTML5 技术，无需任何浏览器插件、支持多个 SSH 进程、可以嵌入到其他任意应用程序中、支持使用 JavaScript，Python 甚至纯 CSS 编写的插件、支持 SSH 进程副本，打开多个进程而无需重复输入密码、 支持各种服务器端的日志功能，支持 Keberos-based 单点登录甚至活动目录、 支持操作日志记录，具有操作记录回放功能。wssh 是基于 paramiko 模块的，但是通过 paramiko 模块访问 vi ， vim ， nano 等软件会存在问题。而 gateone 基本上已经满足了需要，效果是非常不错的。\nGateOne 的部署首先下载 GateOne 的源码：在 https://github.com/liftoff/GateOne/downloads 中 下载 gateone-1.1.tar.gz 。\n直接下载地址为：https://github.com/downloads/liftoff/GateOne/gateone-1.1.tar.gz\n\n1.解压安装包\n\ncd GateOne\n\n\n\n2.依赖环境的搭建\n\n在线安装及安装依赖\nyum -y install epel-release\n\n安装PIP\nyum install python-pip\nyum install python-imaging\n\n\n升级PIP\npip install --upgrade pip\npip install pyopenssl\npip install ordereddict\n# 目前 GateOne 貌似对 tornado 的版本敏感，所以选择此版本\npip install tornado==2.4.1\n\n\n离线安装，安装时保存离线包安装PIP，离线安装无需此步骤\nyum -y install epel-release python-pip  python-imaging --downloadonly --downloaddir=./\n\n\n下载模块及依赖，离线安装无需此步骤\npip download pyopenssl ordereddict tornado==2.4.1\n\n\n离线安装将离线包及依赖放在服务器内执行\nrpm -ivh *.rpm\npip install --no-index --find-links=./ pyopenssl ordereddict tornado==2.4.1\n\n\n确保在 GateOne 目录内，安装 GateOne\npython setup.py install\n\n\n\n3.修改配置文件\n\n\n先执行一下 gateone.py，会生成 server.conf 文件\ncd gateone\n./gateone.py\n\n\n此时应该已经生成 server.conf 文件，修改下列重要项vi server.conf\n# -*- coding: utf-8 -*-\nlocale = \"en_US\"\npam_service = \"login\"\nsyslog_facility = \"daemon\"\nsyslog_host = None\nenable_unix_socket = False\nport = 9000             # 修改端口\nuid = \"0\"\nurl_prefix = \"/gate\"                # 要和Nginx设置一致\nuser_dir = \"/opt/gateone/users\"\ndtach = True\ncertificate = \"certificate.pem\"\nlog_to_stderr = False\nsession_logs_max_age = \"30d\"\ngid = \"0\"\npid_file = \"/var/run/gateone.pid\"\nsso_realm = None\ncookie_secret = \"YTNkZWFhODQyYmY5NDFiODk5MmUwMjQ0NzIxMjliMjIyN\"\npam_realm = \"ubuntu-host\"\nsso_service = \"HTTP\"\nhttps_redirect = False\nsyslog_session_logging = False\ndisable_ssl = True              # 修改为true, 关闭https\ndebug = False\nsession_dir = \"/tmp/gateone\"\nauth = \"none\"\naddress = \"\"\napi_timestamp_window = \"30s\"\nlog_file_num_backups = 10\nlogging = \"info\"\nembedded = False\n# 对应的 origin，这里用 ; 分隔，注意其中的地址，关系到后面的访问\norigins = \"http://192.168.137.11:9000;http://192.168.3.35:80/gate;http://192.168.137.11:9000/gate\"\nsession_logging = True\nunix_socket_path = \"/var/run/gateone.sock\"\n...\n\n\n配置文件说明\n\norigins 登录的地址\naddress改为”127.0.0.1”，这样外网不能直接访问GateOne，只能通过Nginx转发\ndisable_ssl设为True，表示不用GateOne自带的证书\nport改为一个未占用的端口，要和Nginx设置一致\nurl_prefix改为”&#x2F;gateone&#x2F;“，要和Nginx设置一致\n\n后台启动\nnohup python ./gateone.py >gateone.log &\n\n\n打开浏览器，访问https://yourip:端口\n会提示输入对应的用户名和密码。\n\n5.加入到 init.d，启动 GateOne\n\n\n/etc/init.d/gateone restart\n# * Stopping Gate One daemon gateone.py                                             [ OK ]\n# * Starting Gate One daemon gateone.py                                             [ OK ]\n\n\n# 配置nginx\n# 配置文件内添加，注意添加位置\n# 其中的location和proxy_pass中的端口，要和GateOne中的设置一致\n# gateone\nhttp｛\n    upstream gateone &#123;\n        server 192.168.137.11:80 weight=4;\n        server 192.168.137.13:9000 weight=2;\n    &#125;\n\n    server &#123;\n        listen       9000;  # 端口\n        server_name  192.168.3.35; # 地址或者域名\n    \n        #charset koi8-r;\n        charset utf-8;\n    \n        location /gate/ &#123;\n            proxy_pass http://gateone;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection upgrade;\n        &#125;\n    &#125;\n｝\n\n\n设置好后，重启Nginx和GateOne，然后在浏览器输入http://IP/gate/ 就能使用了\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"WordPress网站添加ICP和公安备案信息","url":"/20241121/WordPress/e3868e03af53/","content":"WordPress网站添加ICP和公安备案信息在主题编辑footer.php添加以下代码\n&lt;!-- 公安备案开始--&gt;\t&lt;div style=&quot;width:300px;margin:0 auto; padding:20px 0;&quot;&gt; &lt;!-- 公安备案网址--&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://beian.miit.gov.cn/&quot; style=&quot;display:inline-block;text-decoration:none;height:20px;line-height:20px;&quot;&gt;&lt;img src=&quot;http://liusw.top/wp-content/备案图标.png&quot; style=&quot;float:left;&quot;/&gt;&lt;p style=&quot;float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;&quot;&gt;吉公安网 ICP备 2020008057号&lt;/p&gt;&lt;/a&gt;&lt;/div&gt;\t&lt;!-- 公安备案结束--&gt;\n\n添加位置\n&lt;!-- 公安备案开始--&gt;&lt;div style=&quot;width:300px;margin:0 auto; padding:20px 0;&quot;&gt; &lt;!-- 公安备案网址--&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://beian.miit.gov.cn/&quot; style=&quot;display:inline-block;text-decoration:none;height:20px;line-height:20px;&quot;&gt;&lt;img src=&quot;http://liusw.top/wp-content/备案图标.png&quot; style=&quot;float:left;&quot;/&gt;&lt;p style=&quot;float:left;height:20px;line-height:20px;margin: 0px 0px 0px 5px; color:#939393;&quot;&gt;吉公安网 ICP备 2020008057号&lt;/p&gt;&lt;/a&gt;&lt;/div&gt;&lt;!-- 公安备案结束--&gt;&lt;?php wp_footer(); ?&gt;&lt;/body&gt;&lt;/html&gt;\n\n这个方法直接参考复制了 辰鲲闲谈天地的文章\n","categories":["WordPress"],"tags":["WordPress"]},{"title":"Zabbix-agent安装","url":"/20241121/Zabbix/6a522cc32ed5/","content":"Zabbix-agent安装# yum安装Zabbix-agent配置# 1、安装yum数据库# 官网下载连接：https://www.zabbix.com/cn/download# 需要选择对应服务器的版本，安装对应yum数据库，这里以安装Centos7，Agent4.4版本为例：rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpmyum clean all# 2、yum安装Zabbix-Agentyum install -y zabbix-agent# 3、修改Zabbix-Agent配置文件zabbix_agentd.conf# vi /etc/zabbix/zabbix_agentd.conf# 添加三项参数：如下图：Server=[Zabbix Server的IP地址或主机名]ServerActive=[Zabbix Server的IP地址或主机名]Hostname=[agent的IP地址或主机名]# 注释原有的参数# Server=127.0.0.1# ServerActive=127.0.0.1# Hostname=Zabbix server# 重启并配置Zabbix-Agent开机自启systemctl restart zabbix-agentsystemctl enable zabbix-agent\n\n\n","categories":["Zabbix"],"tags":["Zabbix"]},{"title":"apt-get获取安装包及依赖","url":"/20241121/apt/e25b4c8c4f35/","content":"apt-get获取安装包及依赖适用于debian&#x2F;ubuntu等版本。\n#!/bin/bash# $1     pkg  # 参数1为要安装的包名称,如果不知道包名称,# 根据apt list|grep 包名称  进行查找get_all_depends()&#123;  apt-cache depends --no-pre-depends --no-suggests --no-recommends \\    --no-conflicts --no-breaks --no-enhances\\    --no-replaces --recurse $1 | awk &#x27;&#123;print $2&#125;&#x27;| tr -d &#x27;&lt;&gt;&#x27; | sort --unique&#125;## 遍历命令行参数，参数应为包名。for pkg in $*do  all_depends=$(get_all_depends $pkg)  echo &quot;所有依赖共计&quot;$(echo $all_depends | wc -w)&quot;个&quot;  echo $all_depends  i=0  for depend in $all_depends  do    i=$((i+1))    echo &quot;\\033[1;32m正在下载第$i个依赖：&quot;$depend &quot;\\033[0m&quot;    apt-get download $depend  done  apt-get download $pkgdone\n\n\nThe End\n\n","categories":["apt"],"tags":["apt"]},{"title":"bind安装配置","url":"/20241121/CentOS/5a85e7696711/","content":"bind安装配置1.安装yum安装\nyum install bind -y\n\n2.修改配置文件修改etc下的配置文件\nvim /etc/named.conf\nlisten-on port 53 &#123; 192.168.149.20; &#125;; //改成相应的IPallow-query     &#123; any; &#125;;              //改成允许如何主机访问forwarders       &#123; 192.168.149.1; &#125;;   //网关recursion yes;                         //默认为yesdnssec-enable no;                      //改成nodnssec-validation no;                 //改成no\n\n3.验证配置文件验证配置文件/etc/named.conf是否书写正确\nnamed-checkconf\n\n4.修改区域配置文件增加/etc/named.rfc1912.zones\nvim /etc/named.rfc1912.zoneszone &quot;host.com&quot; IN &#123;        type master;        file &quot;host.com.zone&quot;;        allow-update &#123; 192.168.149.20; &#125;;&#125;;zone &quot;old.com&quot; IN &#123;        type master;        file &quot;old.com.zone&quot;;        allow-update &#123; 192.168.149.20; &#125;;&#125;;\n\n5.配置区域数据文件修改/var/named/host.com.zone\nvim /var/named/host.com.zone$ORIGIN host.com.$TTL 600       ; 10 minutes@       IN SOA dns.host.com. dnsadmin.host.com.(\t\t\t\t\t\t\t   2020102601 ; serial\t\t\t\t\t\t\t   10800      ; refresh (3 hours)\t\t\t\t\t\t\t   900        ; retry (15 minutes)\t\t\t\t\t\t\t   604800     ; expire (1 week)\t\t\t\t\t\t\t   86400      ; minimum (1 day)\t\t\t\t\t\t\t   )\t\t\t\t\t    NS   dns.host.com.$TTL 60 ; 1minutedns               A     192.168.149.11Docker-01         A     192.168.149.11Docker-02         A     192.168.149.12CentOS7-01        A     192.168.149.10CentOS7-02        A     192.168.149.11\n\n修改/var/named/old.com.zone\ncat &gt;/var/named/old.com.zone$ORIGIN old.com.$TTL 600       ; 10 minutes@       IN SOA dns.old.com. dnsadmin.old.com.(\t\t\t\t\t\t\t   2020102601 ; serial\t\t\t\t\t\t\t   10800      ; refresh (3 hours)\t\t\t\t\t\t\t   900        ; retry (15 minutes)\t\t\t\t\t\t\t   604800     ; expire (1 week)\t\t\t\t\t\t\t   86400      ; minimum (1 day)\t\t\t\t\t\t\t   )\t\t\t\t\t    NS   dns.old.com.$TTL 60 ; 1minutedns               A     192.168.149.11\n\n6.验证配置文件验证配置文件/etc/named.conf是否书写正确\nnamed-checkconf\n\n7.增加搜索主机域修改配置文件/etc/resolv.conf\nvim /etc/resolv.confsearch host.com\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"docker上传images到harbor","url":"/20241121/Harbor/1e3b12680404/","content":"docker上传images到harbor#!/usr/bin/env bash############################################################### 脚本名称: push_to_harbor.sh# 脚本功能: 上传images到harbor###############################################################harborIp=192.168.137.11:81project=systemecho &#x27;############################################################################&#x27;echo &#x27;# docker版本 &#x27;echo &#x27;############################################################################&#x27;docker --versionif [ $? -ne 0 ];then  echo &quot;请先安装docker!&quot;  exitfiecho &#x27;############################################################################&#x27;echo &#x27;# images列表及版本号: &#x27;echo &#x27;############################################################################&#x27;docker images |awk &#x27;NR &gt;1 &#123;printf(&quot;%-70s%-15s%s\\n&quot;,$1,$2,$3)&#125;&#x27;echo &#x27;############################################################################&#x27;echo &quot;# 登录$harborIp &quot;echo &#x27;############################################################################&#x27;docker login $harborIpif [ $? -ne 0 ];then  echo &quot;登录失败，请检查后重试!&quot;  exitfidocImages=($(docker images |awk &#x27;NR &gt;1 &#123;printf($1&quot;:&quot;$2&quot; &quot;)&#125;&#x27;))imageName=($(docker images |awk &#x27;NR &gt;1 &#123;print($1)&#125;&#x27;|awk -F &quot;/&quot; &#x27;&#123;print $NF&#125;&#x27;))imageVersion=($(docker images |awk &#x27;NR &gt;1 &#123;print($2)&#125;&#x27;))Number=()t=1if [ $&#123;#docImages[@]&#125; -ne $&#123;#imageName[@]&#125; -o $&#123;#imageVersion[@]&#125; -ne $&#123;#imageName[@]&#125; ];then  echo &quot;获取images镜像出错，请检查!&quot;  exit 1fiecho &#x27;############################################################################&#x27;echo &#x27;# 上传harbor的images镜像列表 &#x27;echo &#x27;############################################################################&#x27;for ((i=0;i&lt;$&#123;#docImages[@]&#125;;i++));do  echo &quot;$(expr $&#123;i&#125; + 1) $&#123;docImages[i]&#125;&quot;doneecho &quot;$(expr $&#123;#docImages[@]&#125; + 1) 退出&quot;while ((t&lt;5));do  read -p &quot;请选择要tag的images(填写序号):&quot; -t 500 -a Number  if [ -z $&#123;#Number[@]&#125; ];then    read -p &quot;请选择要tag的images(填写序号):&quot; -t 500 -a Number  else    for ((k=0;k&lt;$&#123;#Number[@]&#125;;k++));do\t  if [ $&#123;Number[$k]&#125; -eq $(expr $&#123;#docImages[@]&#125; + 1) ];then        echo &quot;退出&quot;\t\tbreak\t    exit 1\t  fi\tdone\tbreak  fidoneecho &#x27;############################################################################&#x27;echo &#x27;# 上传images镜像&#x27;echo &#x27;############################################################################&#x27;for ((a=0;a&lt;$&#123;#Number[@]&#125;;a++));do  docker tag $&#123;docImages[$a]&#125; $&#123;harborIp&#125;/$&#123;project&#125;/$&#123;imageName[$a]&#125;:$&#123;imageVersion[$a]&#125;  docker push $&#123;harborIp&#125;/$&#123;project&#125;/$&#123;imageName[$a]&#125;:$&#123;imageVersion[$a]&#125;  docker rmi -f $&#123;harborIp&#125;/$&#123;project&#125;/$&#123;imageName[$a]&#125;:$&#123;imageVersion[$a]&#125; &gt;/dev/nulldoneecho &#x27;############################################################################&#x27;echo &#x27;# The End.&#x27;echo &#x27;###########################################################################&#x27;\n","categories":["Docker","Harbor"],"tags":["Docker","Harbor"]},{"title":"docker下载k8s相关镜像1.17.3","url":"/20241121/Kubernetes/e133c536cdeb/","content":"docker下载k8s相关镜像1.17.3#!/usr/bin/env bashecho &#x27;############################################################################&#x27;echo &#x27;# 此脚本为下载k8s相关镜像&#x27;echo &#x27;############################################################################&#x27;url=registry.cn-hangzhou.aliyuncs.com/google_containersversion=v1.17.3echo &#x27;############################################################################&#x27;echo &quot;# 下载kube-scheduler kube-proxy kube-controller-manager kube-apiserver 版本:$&#123;version&#125;&quot;echo &#x27;############################################################################&#x27;kube_images=(kube-scheduler:v1.17.3 kube-proxy:v1.17.3 kube-controller-manager:v1.17.3 kube-apiserver:v1.17.3)for kubename in $&#123;kube_images[@]&#125; ; do  docker pull $url/$kubename  docker tag $url/$kubename k8s.gcr.io/$kubename  docker rmi -f $url/$kubenamedoneimages=($(kubeadm config images list --kubernetes-version=$version | awk -F &#x27;/&#x27; &#x27;&#123;print $2&#125;&#x27;| grep -v &quot;kube-&quot;))echo &#x27;############################################################################&#x27;echo &quot;# 下载$&#123;images[@]&#125;&quot;echo &#x27;############################################################################&#x27;for imagename in $&#123;images[@]&#125; ; do  docker pull $url/$imagename  docker tag $url/$imagename k8s.gcr.io/$imagename  docker rmi -f $url/$imagenamedoneecho &#x27;############################################################################&#x27;echo &quot;# 下载完成&quot;echo &#x27;############################################################################&#x27;\n","categories":["Docker","Kubernetes"],"tags":["Docker","Kubernetes"]},{"title":"docker之容器内中文乱码问题","url":"/20241121/Docker/21ac443776de/","content":"docker之容器内中文乱码问题Dockerfile示例\n\n示例容器为debian11，ubuntu和centos同理,换一下各自的源即可。\n\nFROM 192.168.11.6:8083/dotnet/aspnet:6.0-ex#RUN uname -a &amp;&amp; \\#    rm -f /etc/apt/sources.list#COPY sources.list /etc/apt/sources.listRUN echo &quot;deb http://ftp.de.debian.org/debian bullseye main &quot; &gt;&gt;/etc/apt/sources.list# 更新源，安装字体RUN apt-get update &amp;&amp; \\    apt-get install -y locales xfonts-intl-chinese ttf-wqy-microhei# 中文设置RUN localedef -f UTF-8 -i zh_CN zh_CN.UTF-8# 编码环境设置ENV LANG zh_CN.UTF-8ENV LC_ALL zh_CN.UTF-8ENV LANGUAGE zh_CN.UTF-8\n\ndocker 打包编译构建镜像, 构建是遇到无法apt-get安装,构建时需添加参数–network host\ndocker build --network host  -t 192.168.11.6:8083/dotnet/aspnet:6.0-ex-language-pack-zh ./\n","categories":["Docker"],"tags":["Docker"]},{"title":"docker之日志清理","url":"/20241121/Docker/c353c8619853/","content":"docker之日志清理查看服务器内docker容器的日志占用情况for name in $(docker ps -a  | awk &#x27;&#123;print $1&#125;&#x27; | grep -v CONTAINER); do docker inspect $name | grep LogPath | awk &#x27;&#123;print $NF&#125;&#x27; | tr -d &#x27;&quot;,&#x27; |xargs du -sh;done\n\n配置 Docker日志轮转，数据目录最好不要存放在系统盘vim /etc/docker/daemon.json&#123;  &quot;registry-mirrors&quot;: [&quot;https://4xr1qpsp.mirror.aliyuncs.com&quot;],  &quot;graph&quot;: &quot;/data/docker&quot;,  &quot;log-driver&quot;:&quot;json-file&quot;, # 可不写  &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;100m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;&#125;\n\n\nregistry-mirrors 镜像加速配置\ngraph 定义数据存储目录\nlog-driver 默认 logging driver是 json-file\nmax-size&#x3D;100m  意味着一个容器日志大小上限是100M\nmax-file&#x3D;3，意味着一个容器有三个日志，分别是id+.json、id+1.json、id+2.json\n\n","categories":["Docker"],"tags":["Docker"]},{"title":"docker基础","url":"/20241121/Docker/a22074b3fadf/","content":"docker基础一、Docker介绍-Cenos71.下载Dcoker依的赖环境想安装Docker，需要先将依赖的环境全部下载下来，就像Maven依赖JDK一样\nyum -y install yum-utils device-mapper-persistent-data lvm2\n\n2.指定Docker镜像源默认下载Docker会去国外服务器下载，速度较慢，可以设置为阿里云镜像源，速度更快\nyum-config-manager \\\t--add-repo \\\thttp://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n更新软件包得索引\n\nCenOS7yum makecache fast\nCenOS8yum makecache\n设置加速器\n\n新版的 Docker 使用 /etc/docker/daemon.json\ncat &gt;/etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;registry-mirrors&quot;:[&quot;https://registry.docker-cn.com&quot;],   &quot;graph&quot;: &quot;/opt/docker/data&quot;,  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;insecure-registries&quot;:[&quot;192.168.150.21:81&quot;],   &quot;log-driver&quot;:&quot;json-file&quot;,  &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;100m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;&#125;EOF\n参数设置:registry-mirrors: 设置镜像加速graph: docker目录exec-opts: cgroupfs驱动设置,默认为cgroupfsinsecure-registries: 本地镜像仓库地址(本地仓库需要添加此项,默认80端口可不写)log-driver: 日志格式log-opts: 日志大小和数量设置\n3.安装Docker安装 docker-ce 社区版 docker-ee 企业版\nyum -y install docker-ceyum -y install docker-ce docker-ce-cli containerd.io\n指定版本安装\nyum -y install docker-ce-版本 docker-ce-cli-版本 containerd.io\n启动 Docker 服务\nsystemctl daemon-reloadsystemctl restart docker\n查看 Docker 服务状态\nsystemctl  status docker\n\n简单安装Docker方法\nyum install wgetwget -qO- https://get.docker.com | sh\n\n卸载Docker\n\n1.卸载依赖\nyum remove docker-ce docker-ce-cli containerd.io\n\n2.删除资源（默认路径）\nrm -rf /var/lib/docker\n\n4.启动Docker并测试安装成功后，需要手动启动，设置为开机启动，并测试一下 Docker\n#启动docker服务systemctl start docker#设置开机自动启动systemctl enable docker#测试docker run hello-world\n\n5.常用命令# 查看docker版本信息docker version # 详细信息docker info# 帮助命令docker --help# 帮助文档地址https://docs.docker.com/engine/reference\n\n\n\n二、Docker的中央仓库\n1.Docker官方的中央仓库：这个仓库是镜像最全的，但是下载速度较慢。https://hub.docker.com/\n\n2.国内的镜像网站：网易蜂巢，daoCloud等，下载速度快，但是镜像相对不全。\n  https://c.163yun.com/hub#/home\n  http://hub.daocloud.io/ （推荐使用）\n\n3.在公司内部会采用私服的方式拉取镜像（添加配置）\n\n\n#需要创建 &#x2F;etc&#x2F;docker&#x2F;daemon.json，并添加如下内容\ncat &gt;/etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;graph&quot;: &quot;/opt/docker/data&quot;,  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;insecure-registries&quot;:[&quot;192.168.150.21:81&quot;],   &quot;log-driver&quot;:&quot;json-file&quot;,  &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;100m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;&#125;EOF\n参数设置:graph: docker目录exec-opts: cgroupfs驱动设置,默认为cgroupfsinsecure-registries: 本地镜像仓库地址log-driver: 日志格式log-opts: 日志大小和数量设置\n重启两个服务\nsystemctl daemon-reloadsystemctl restart docker\n\n三、镜像的操作1.拉取镜像从中央仓库拉取镜像到本地\ndocker pull 镜像名称[:tag]\n\n举个栗子:\ndocker pull tomcatdocker pull daocloud.io/library/tomcat:8.5.15-jre8docker pull daocloud.io/library/tomcat:latest\n\n2.查看本地全部镜像 查看本地已经安装过的镜像信息，包含标识，名称，版本，更新时间，大小\ndocker images# REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE# centos                       latest              865d0f6b5b72        7 days ago          392MB\n\n解释\nREPOSITORY\t\t镜像的仓库源\nAG\t\t\t\t镜像版本标签信息\nIMAGE ID\t\t\t镜像的ID\nCREATED\t\t\t镜像创建时间\nSIZE\t\t\t\t镜像的大小\n\n参数\ndocker images --help\n\n\nOptions:\n-a, –all  列出所有镜像\n-q, –quiet 只显示镜像ID\n\n搜索镜像\ndocker search mysql\n\n3.删除本地镜像镜像会占用磁盘空间，可以直接手动删除，标识通过查看获取\ndocker rmi + 镜像的标识\n\n4.镜像的导入导出如果因为网络原因可以通过硬盘的方式传输镜像，虽然不规范，但是有效，但是这种方式导出的镜像名称和版本都是null，需要手动修改\n将本地的镜像导出\ndocker save -o + 导出的路径 + 镜像id# 加载本地的镜像文件docker load -i + 镜像文件# 修改镜像文件docker tag + 镜像id + 新镜像名称：版本\n\n四、容器的操作1.运行容器运行容器需要定制具体镜像，如果镜像不存在，会直接下载\n# 简单操作docker run 镜像的标识|镜像的名称[:tag]# 常用的参数docker run -d -p 宿主机端口:容器端口 --name 容器名称 镜像的标识|镜像名称[:tag]# -d 代表后台运行容器docker run -d centos -c &quot;echo 1&quot;# --name 容器名称:指定容器的名称# -it 交互方式运行,进入容器查看内容# -P 随机指定端口# -p 宿主机端口:容器端口：为了映射当前Linux的端口和容器的端口# \t\t-p ip:主机端口:容器端口# \t\t-p 主机端口:容器端口#\t\t-p 容器端口#\t\t容器端口\n\n2.查看正在运行的容器# 查看全部正在运行的容器信息docker ps [-qa]# -a 查看全部的容器，包括没有运行# -q 只查看容器的标识# -n 查看最近创建的容器docker ps -a -n=2\n\n3.查看容器日志查看容器日志，以查看容器运行的信息\ndocker logs -f 容器id# -t 显示全部日志# -f 显示时间戳# --tail num 查看后num行\n\n4.进入正在运行容器的内部可以进入容器的内部进行操作\ndocker exec -it 容器id /bin/bashdocker attach 容器id /bin/bash\n区别\n\ndocker exec     进入容器后开启一个新的终端,可以在里面操作(常用)\ndocker attach 　进入正在运行的终端,不会启动新的进程\n\n5.复制内容到容器将宿主机的文件复制到容器内部的指定目录\n或将容器内部的文件复制到宿主机的指定目录\ndocker cp 文件名称 容器id:容器内部路径docker cp 容器id:容器内部路径/文件名称 宿主机指定的目录\n\n6.重启 启动 停止 删除容器容器的启动，停止，删除等操作，后续会经常使用到\n# 重新启动容器docker restart 容器id# 启动停止运行的容器docker start 容器id\n # 停止指定的容器(删除容器前，需要先停止容器)docker stop 容器id# 停止全部容器docker stop $(docker ps -qa)\n# 删除指定容器docker rm 容器id# 删除全部容器docker rm -f $(docker ps -qa)\n\n7.查看容器进程查看容器内部进程信息\ndocker top 容器id\n\n8.查看容器元数据查看元数据\ndocker inspect 容器id\n\n\n\n五、Docker应用1.docker安装tomcat运行Tomcat容器，为部署ssm工程做准备\ndocker run -d -p 8080:8080 --name tomcat  daocloud.io/library/tomcat:8.5.15-jre8# 或者已经下载了tomcat镜像docker run -d -p 8080:8080 --name tomcat 镜像的标识\n\n2.运行MySQL容器docker run -d -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=root daocloud.io/library/mysql:5.7.4\n\n3.部署ssm工程ssm.war下载\nhttps://download.lixian.fun/web_project/Jpress-0.4.0.tar.gz\n将上面连接的tar下载下来，解压，在wars目录里面将jpress-web-newest.war重命名为ssm.war就行了\n说明：视频里的ssm项目我并没有，照葫芦画瓢只要是个war包就可以。这是Jpress软件包，同样是Java开发的。将war包的名字改一下就行了。\n# 修改SSM工程环境，设置为Linux中Docker容器的信息\n# 通过Maven的package重新打成war包\n# 将Windows下的war包复制到Linux中\n# 通过docker命令将宿主机的war包复制到容器内部\n docker cp 文件名称 容器id:容器内部路径\n测试访问SSM工程\n4.docker可视化面板# portainerdocker run -d -p 8088:9000 \\--restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainer\n\n\n\n六、数据卷# 为了部署SSM的工程，需要使用到cp的命令将宿主机内的ssm.war文件复制到容器内部。# 数据卷：将宿主机的一个目录映射到容器的一个目录中。# 可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变。\n\n1.创建数据卷创建数据卷后，默认会存放在一个目录下/var/lib/docker/volumes/数据卷名称/_data\ndocker volume create 数据卷名称\n\n2.查看全部数据卷查看全部数据卷信息\ndocker volume ls\n\n3.查看数据卷详情查看数据卷的详细信息，可以查询到存放的路径，创建时间等等\ndocker volume inspect 数据卷名称\n\n4.删除数据卷删除指定的数据卷\ndocker volume rm 数据卷名称\n\n5.容器映射数据卷通过数据卷名称映射，如果数据卷不存在。Docker会帮你自动创建，会将容器内部自带的文件，存储在默认的存放路径中。\ndocker run -d -p 8080:8080 --name tomcat -v 数据卷名称:容器内部的路径 镜像id\n 通过路径映射数据卷，直接指定一个路径作为数据卷的存放位置。但是这个路径下是空的\ndocker run -d -p 8080:8080 --name tomcat -v 路径(/root/自己创建的文件夹):容器内部的路径 镜像id\n\n七、Dockerfile自定义镜像1.Dockerfile创建自定义镜像就需要创建一个Dockerfiler,如下为Dockerfile的语言 from：指定当前自定义镜像依赖的环境copy：将相对路径下的内容复制到自定义镜像中workdir：声明镜像的默认工作目录run：执行的命令，可以编写多个cmd：需要执行的命令（在workdir下执行的，cmd可以写多个，只以最后一个为准） # 示例：from daocloud.io/library/tomcat:8.5.15-jre8copy ssm.war /usr/local/tomcat/webapps\n2.通过Dockerfile制作镜像编写完Dockerfile后需要通过命令将其制作为镜像，并且要在Dockerfile当前目录下，之后可在镜像中查看到指定镜像信息，注意最后的 .\ndocker build -t 镜像名称[:tag] ./\n\n八、Docker-Compose1.下载并安装Docker-Compose1.1下载Docker-Compose\n方式1去github官网搜索docker-compose，下载1.24.1版本的Docker-Compose\n\n下载路径：https://github.com/docker/compose/releases/download/1.24.1/docker-compose-Linux-x86_64\n\n方式2安装依赖yum -y install epel-release\n安装PIPyum -y install python-pip\n# 升级PIPpip install --upgrade pip# 查看版本pip --version\n安装docker-composepip install -U docker-compose==1.23.2\n\n1.2设置权限需要将DockerCompose文件的名称修改一下，给予DockerCompose文件一个可执行的权限\nmv docker-compose-Linux-x86_64 docker-composechmod 777 docker-compose\n\n1.3配置环境变量方便后期操作，配置一个环境变量\n将docker-compose文件移动到了/usr/local/bin，修改了/etc/profile文件，给/usr/local/bin配置到了PATH中\nmv docker-compose /usr/local/binvi /etc/profile    # 添加内容：export PATH=$JAVA_HOME:/usr/local/bin:$PATHsource /etc/profile\n\n1.4测试# 在任意目录下输入docker-composedocker-compose down -vdocker-compose up -d\n\n2.Docker-Compose管理MySQL和Tomcat容器yml文件以key:value方式来指定配置信息\n多个配置信息以换行+缩进的方式来区分\n在docker-compose.yml文件中，不要使用制表符\nversion: &#x27;3.1&#x27;services:  mysql:           # 服务的名称    restart: always   # 代表只要docker启动，那么这个容器就跟着一起启动    image: daocloud.io/library/mysql:5.7.4  # 指定镜像路径    container_name: mysql  # 指定容器名称    ports:      - 3306:3306   #  指定端口号的映射    environment:      MYSQL_ROOT_PASSWORD: root   # 指定MySQL的ROOT用户登录密码      TZ: Asia/Shanghai        # 指定时区    volumes:     - /opt/docker_mysql_tomcat/mysql_data:/var/lib/mysql   # 映射数据卷  tomcat:    restart: always    image: daocloud.io/library/tomcat:8.5.15-jre8    container_name: tomcat    ports:      - 8080:8080    environment:      TZ: Asia/Shanghai    volumes:      - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps      - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs\n\n3.使用docker-compose命令管理容器在使用docker-compose的命令时，默认会在当前目录下找docker-compose.yml文件\n# 1.基于docker-compose.yml启动管理的容器docker-compose up -d # 2.关闭并删除容器docker-compose down # 3.开启|关闭|重启已经存在的由docker-compose维护的容器docker-compose start|stop|restart # 4.查看由docker-compose管理的容器docker-compose ps # 5.查看日志docker-compose logs -f\n\n4.docker-compose配合Dockerfile使用使用docker-compose.yml文件以及Dockerfile文件在生成自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器 \n4.1docker-compose文件编写docker-compose文件  yml文件\nversion: &#x27;3.1&#x27;services:  ssm:    restart: always    build:            # 构建自定义镜像      context: ../      # 指定dockerfile文件的所在路径      dockerfile: Dockerfile   # 指定Dockerfile文件名称    image: ssm:1.0.1    container_name: ssm    ports:      - 8081:8080    environment:      TZ: Asia/Shanghai\n\n4.2 Dockerfile文件编写Dockerfile文件\nfrom daocloud.io/library/tomcat:8.5.15-jre8copy ssm.war /usr/local/tomcat/webapps\n\n4.3 运行可以直接基于docker-compose.yml以及Dockerfile文件构建的自定义镜像\ndocker-compose up -d\n\n如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已经存在，会直接运行这个自定义镜像;\n重新构建自定义镜像\ndocker-compose build\n运行当前内容，并重新构建\ndocker-compose up -d --build\n\n九、CI、CD介绍及准备1.CI、CD引言 项目部署\n\n1.将项目通过maven进行编译打包\n2.将文件上传到指定的服务器中\n3.将war包放到tomcat的目录中\n4.通过Dockerfile将Tomcat和war包转成一个镜像，由DockerCompose去运行容器\n\n项目更新后，需要将上述流程再次的从头到尾的执行一次，如果每次更新一次都执行一次上述操作，很费时，费力。我们就可以通过CI、CD帮助我们实现持续集成，持续交付和部署\n2.CI介绍CI（continuous intergration）持续集成 持续集成：编写代码时，完成了一个功能后，立即提交代码到Git仓库中，将项目重新的构建并且测试。\n\n1.快速发现错误。\n2.防止代码偏离主分支。\n\n3.搭建Gitlab服务器3.1准备环境实现CI，需要使用到Gitlab远程仓库，先通过Docker搭建Gitlab 创建一个全新的虚拟机，并且至少指定4G的运行内存，4G运行内存是Gitlab推荐的内存大小。 并且安装Docker以及Docker-Compose\n3.2 修改ssh的22端口将ssh的默认22端口，修改为60022端口，因为Gitlab需要占用22端口\nvi /etc/ssh/sshd_config  PORT 22 -&gt; 60022\n重启sshd服务\nsystemctl restart sshd\n\n3.3 编写docker-compose.yml docker-compose.yml文件去安装gitlab（下载和运行的时间比较长的）\nversion: &#x27;3.1&#x27;services: gitlab:  image: &#x27;twang2218/gitlab-ce-zh:11.1.4&#x27;  container_name: &quot;gitlab&quot;  restart: always  privileged: true  hostname: &#x27;gitlab&#x27;  environment:   TZ: &#x27;Asia/Shanghai&#x27;   GITLAB_OMNIBUS_CONFIG: |    external_url &#x27;http://192.168.199.110&#x27;    gitlab_rails[&#x27;time_zone&#x27;] = &#x27;Asia/Shanghai&#x27;    gitlab_rails[&#x27;smtp_enable&#x27;] = true    gitlab_rails[&#x27;gitlab_shell_ssh_port&#x27;] = 22  ports:   - &#x27;80:80&#x27;   - &#x27;443:443&#x27;   - &#x27;22:22&#x27;  volumes:   - /opt/docker_gitlab/config:/etc/gitlab   - /opt/docker_gitlab/data:/var/opt/gitlab   - /opt/docker_gitlab/logs:/var/log/gitlab\n\n十、搭建GitlabRunner1.准备文件daemon.json\n&#123;&quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;],&quot;insecure-registries&quot;: [ip:ports]&#125;\n文件夹 environment里面准备maven安装包，jdk1.8安装包，Dockerfile，daemon.json以及docker-compose\n2.开始搭建 创建工作目录 /usr/local/docker_gitlab-runner\n将docker-compose.yml文件以及environment目录全部复制到上述目录中 在宿主机启动docker程序后先执行 sudo chown root:root /var/run/docker.sock (如果重启过 docker,重新执行)\n在/usr/local/docker_gitlab-runner 目录中执行docker-compose up -d –build 启动容器\n添加容器权限，保证容器可以使用宿主机的dockerdocker exec -it gitlab-runner usermod -aG root gitlab-runner 注册Runner信息到gitlab\n3.进入后续步骤docker exec -it gitlab-runner gitlab-runner register # 输入 GitLab 地址Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/):http://192.168.199.109/ # 输入 GitLab TokenPlease enter the gitlab-ci token for this runner:1Lxq_f1NRfCfeNbE5WRh # 输入 Runner 的说明Please enter the gitlab-ci description for this runner:可以为空 # 设置 Tag，可以用于指定在构建规定的 tag 时触发 ciPlease enter the gitlab-ci tags for this runner (comma separated):deploy # 这里选择 true ，可以用于代码上传后直接执行（根据版本，也会没有次选项）Whether to run untagged builds [true/false]:true # 这里选择 false，可以直接回车，默认为 false（根据版本，也会没有次选项）Whether to lock Runner to current project [true/false]:false # 选择 runner 执行器，这里我们选择的是 shellPlease enter the executor: virtualbox, docker+machine, parallels, shell, ssh, docker-ssh+machine, kubernetes, docker, docker-ssh:shell\n\n十一、整合项目入门测试1.创建项目# 创建maven工程，添加web.xml文件，编写HTML页面\n\n\n\n2.编写.gitlab-ci.yml文件stages:  - test test:  stage: test  script:    - echo first test ci   # 输入的命令\n\n3.将maven工程推送到gitlab中执行git命令推送到Gitlab\ngit push origin master\n\n4.查看效果可以在gitlab中查看到gitlab-ci.yml编写的内容\n十二、完善项目配置添加Dockerfile以及docker-compose.yml， 并修改.gitlab-ci.yml文件 \n1.创建DockerfileDockerfile\nFROM daocloud.io/library/tomcat:8.5.15-jre8COPY testci.war /usr/local/tomcat/webapps\n\n2.创建docker-compose.yml# docker-compose.ymlversion: &quot;3.1&quot;services:  testci:    build: docker    restart: always    container_name: testci    ports:      - 8080:8080\n\n3.修改.gitlab-ci.yml# ci.ymlstages:  - test test:  stage: test  script:    - echo first test ci    - /usr/local/maven/apache-maven-3.6.3/bin/mvn package    - cp target/testci-1.0-SNAPSHOT.war docker/testci.war    - docker-compose down    - docker-compose up -d --build    - docker rmi $(docker images -qf dangling=true)\n","categories":["Docker"],"tags":["Docker"]},{"title":"docker容器化wordpress更改服务器地址","url":"/20241121/Docker/68f4bd567ffe/","content":"docker容器化wordpress更改服务器地址docker容器化wordpress更改服务器地址1.复制容器内文件到新服务器\n2.复制数据库内容到新服务器\n3.启动容器\n[root@VM-16-16-centos blog]# ls -ltotal 12-rwxrwxr-x 1 root    root  588 Dec 27  2020 docker-compose.ymldrwxrwxrwx 6 root    root 4096 Oct 11 15:25 htmldrwxrwxr-x 6 polkitd root 4096 Oct 11 15:04 mysql\n\nversion: &#x27;3.1&#x27;services:  wordpress:    image: wordpress    restart: always    ports:      - 80:80    volumes:      - /data/blog/html:/var/www/html    environment:      WORDPRESS_DB_HOST: db      WORDPRESS_DB_USER: wordpress      WORDPRESS_DB_PASSWORD: mypassword      WORDPRESS_DB_NAME: wordpress  db:    image: mysql:5.7    restart: always    volumes:      - /data/blog/mysql:/var/lib/mysql    environment:      MYSQL_DATABASE: wordpress      MYSQL_USER: wordpress      MYSQL_PASSWORD: mypassword      MYSQL_RANDOM_ROOT_PASSWORD: &#x27;1&#x27;volumes:  html:  mysql: \n\n#启动容器docker-compose -f docker-compose.yml up -d\n\n4.登录mysql容器更改服务器地址\ndocker exec -it 容器ID sh\n\nupdate wp_options set option_value&#x3D;replace(option_value,’旧服务器IP’,’新服务器IP’) where option_value LIKE ‘%旧服务器IP%’;\nmysql -uwordpress -p mypassworduse wordpress;update wp_optionsset option_value=replace(option_value,&#x27;39.106.81.231&#x27;,&#x27;101.42.100.165&#x27;) where option_value LIKE &#x27;%39.106.81.231%&#x27;;\n\n5.更改域名解析\n完成\n","categories":["Docker"],"tags":["Docker"]},{"title":"docker配置本地镜像与容器的存储位置","url":"/20241121/Docker/5e7dd51ff188/","content":"docker配置本地镜像与容器的存储位置\n查看docker占用情况\ndocker system df# [root@master01 html]# docker system df# TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE# Images          135       29        21.64GB   19.06GB (88%)# Containers      50        43        18.73MB   0B (0%)# Local Volumes   9         9         48B       0B (0%)# Build Cache     0         0         0B        0B\n以下命令只能清除悬空镜像，未被使用的镜像不会被删除（未经测试，谨慎使用!!!）\ndocker system prune\n以下命令自动清理docker（未经测试，谨慎使用!!!）\ndocker system prune -a\n\n\n已停止的容器（container）\n未被任何容器所使用的卷（volume）\n未被任何容器所关联的网络（network）\n所有悬空镜像（image）\n\n所以选择docker system prune -a自动清理docker。\n1.软链接默认情况下Docker的存放位置为：/var/lib/docker可以通过下面命令查看具体位置：\ndocker info | grep &quot;Docker Root Dir&quot;\n\n首先停掉Docker服务：\n\nservice docker stop\n\n\n然后移动整个&#x2F;var&#x2F;lib&#x2F;docker目录到目的路径：\n\nmv /var/lib/docker /root/data/dockerln -s /root/data/docker /var/lib/docker\n这时候启动Docker时发现存储目录依旧是/var/lib/docker，但是实际上是存储在数据盘的，你可以在数据盘上看到容量变化。\n2.可扩展逻辑卷默认情况下docker的存放位置为: /var/lib/docker\n挂载大分区到/var/lib/docker; 一般选择建立逻辑分区lvm，方便后期扩展\n\n(1)建立新分区，并格式化\n\nPS: 以下操作建设你已经有现成的卷组，直接可以划逻辑卷。或者你可以自己创建逻辑卷，或者不适用逻辑卷直接使用分区\nlvcreate -L 300G lv_docker vg_homemkfs.ext4 /dev/vg_home/lv__docker\n\n\n(2)挂载新分区到临时挂载点\n\nmkdir /mnt/dockermount /dev/vg_home/lv_docker /mnt/docker/\n\n\n(3)停掉docker后拷贝&#x2F;var&#x2F;lib&#x2F;docker下数据到临时挂载点\n\nservice docker stopcp -r /var/lib/docker/* /mtn/docker\n\n(4)备份&#x2F;var&#x2F;lib&#x2F;docker 为&#x2F;var&#x2F;lib&#x2F;docker.bak，并创建新的&#x2F;var&#x2F;lib&#x2F;docker目录\n\nmv /var/lib/docker /var/lib/docker.bakmkdir /var/lib/docker\n\n(5)挂载新分区到&#x2F;var&#x2F;lib&#x2F;docker，并设置开机自动挂载\n\nmount /dev/vg_home/lv_docker /var/lib/dockervim /etc/fstab--- /dev/vg_home/lv_docker /docker_data   ext4    defaults 0 0----\n\n\n(6)检测docker是否可用，数据是否完整\n\ndocker imagesdocker ps -a\n\n\n(7)确认无误后卸载临时挂载点，删除&#x2F;var&#x2F;lib&#x2F;docker.bak\n\numount /mnt/dockerrm -rf /var/lib/docker.bak\n\n3.修改镜像和容器的存放路径指定镜像和容器存放路径的参数是–graph=/var/lib/docker，我们只需要修改配置文件指定启动参数即可。\nDocker 的配置文件可以设置大部分的后台进程参数，在各个操作系统中的存放位置不一致，在 Ubuntu 中的位置是：/etc/default/docker，在 CentOS 中的位置是：/etc/sysconfig/docker\n\n如果是 CentOS 则添加下面这行\n\nOPTIONS=--graph=&quot;/root/data/docker&quot; --selinux-enabled -H fd://\n\n如果是 Ubuntu 则添加下面这行（因为 Ubuntu 默认没开启 selinux）：\n\nOPTIONS=--graph=&quot;/root/data/docker&quot; -H fd://# 或者DOCKER_OPTS=&quot;-g /root/data/docker&quot;\n\n最后重新启动，Docker 的路径就改成 /root/data/docker 了。\n如果没有生效，按如下操作\ncat &gt;&gt;/etc/default/docker &lt;&lt;EOFDOCKER_OPTS=&quot;--graph=/root/data/docker&quot;EOFsystemctl restart docker\n\n发现配置并没有生效, 解决方案：\nmkdir -p /etc/systemd/system/docker.service.dcat &gt;&gt; /etc/systemd/system/docker.service.d/Using_Environment_File.conf &lt;&lt;EOF[Service]EnvironmentFile=-/etc/default/dockerExecStart=ExecStart=/usr/bin/docker daemon -H fd:// $DOCKER_OPTSEOF\n重启服务\nsystemctl daemon-reloadservice docker restart\n查看配置是否生效\n","categories":["Docker"],"tags":["Docker"]},{"title":"filebeat配置详解","url":"/20241121/ELK/29d153bc5d47/","content":"filebeat配置详解Filebeat是本地文件的日志数据采集器。 作为服务器上的代理安装，Filebeat监视日志目录或特定日志文件，tail file，并将它们转发给Elasticsearch或Logstash进行索引、kafka 等。\n工作原理：Filebeat由两个主要组件组成：prospector 和harvester。这些组件一起工作来读取文件（tail file）并将事件数据发送到您指定的输出\n启动Filebeat时，它会启动一个或多个查找器，查看您为日志文件指定的本地路径。 对于prospector 所在的每个日志文件，prospector 启动harvester。 每个harvester都会为新内容读取单个日志文件，并将新日志数据发送到libbeat，后者将聚合事件并将聚合数据发送到您为Filebeat配置的输出。\nharvesterharvester :负责读取单个文件的内容。读取每个文件，并将内容发送到 the output每个文件启动一个harvester, harvester 负责打开和关闭文件，这意味着在运行时文件描述符保持打开状态如果文件在读取时被删除或重命名，Filebeat将继续读取文件。这有副作用，即在harvester关闭之前，磁盘上的空间被保留。默认情况下，Filebeat将文件保持打开状态，直到达到close_inactive状态\n关闭harvester会产生以下结果：1）如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。2）文件的采集只会在scan_frequency过后重新开始。3）如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件。\n要控制收割机何时关闭，请使用close_ *配置选项\nprospectorprospector 负责管理harvester并找到所有要读取的文件来源。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个prospector都在自己的Go协程中运行。\nFilebeat目前支持两种prospector类型：log和stdin。每个prospector类型可以定义多次。日志prospector检查每个文件以查看harvester是否需要启动，是否已经运行，或者该文件是否可以被忽略（请参阅ignore_older）。只有在harvester关闭后文件的大小发生了变化，才会读取到新行。\n注：Filebeat prospector只能读取本地文件， 没有功能可以连接到远程主机来读取存储的文件或日志。\nFilebeat如何保持文件的状态？Filebeat 保存每个文件的状态并经常将状态刷新到磁盘上的注册文件中。该状态用于记住harvester正在读取的最后偏移量，并确保发送所有日志行。如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取。\n每个prospector为它找到的每个文件保留一个状态。由于文件可以被重命名或移动，因此文件名和路径不足以识别文件。对于每个文件，Filebeat存储唯一标识符以检测文件是否先前已采集过。\n如果您的使用案例涉及每天创建大量新文件，您可能会发现注册文件增长过大。请参阅注册表文件太大？编辑有关您可以设置以解决此问题的配置选项的详细信息。\nFilebeat如何确保至少一次交付Filebeat保证事件至少会被传送到配置的输出一次，并且不会丢失数据。 Filebeat能够实现此行为，因为它将每个事件的传递状态存储在注册文件中。\n在输出阻塞或未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到接收端确认已收到。\n如果Filebeat在发送事件的过程中关闭，它不会等待输出确认所有收到事件。发送到输出但在Filebeat关闭前未确认的任何事件在重新启动Filebeat时会再次发送。这可以确保每个事件至少发送一次，但最终会将重复事件发送到输出。也可以通过设置shutdown_timeout选项来配置Filebeat以在关闭之前等待特定时间。\n注意：Filebeat的至少一次交付保证包括日志轮换和删除旧文件的限制。如果将日志文件写入磁盘并且写入速度超过Filebeat可以处理的速度，或者在输出不可用时删除了文件，则可能会丢失数据。在Linux上，Filebeat也可能因inode重用而跳过行。\nfilebeat配置详解filebeat.yml的格式如下，我们主要了解从log中输入的相应配置\nfilebeat.inputs:- input_type: logpaths:- /var/log/apache/httpd-*.logdocument_type: apache- input_type: logpaths:- /var/log/messages- /var/log/*.log　\n\nFilebeat Optionsinput_type: log指定输入类型paths支持基本的正则，所有golang glob都支持,支持&#x2F;var&#x2F;log&#x2F;*&#x2F;*.logencoding\nplain, latin1, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk, hz-gb-2312,euc-kr, euc-jp, iso-2022-jp, shift-jis, and so onexclude_lines\n\n支持正则 排除匹配的行，如果有多行，合并成一个单一行来进行过滤include_lines支持正则 include_lines执行完毕之后会执行exclude_lines。exclude_files支持正则 排除匹配的文件exclude_files: [‘.gz$’]tags列表中添加标签，用过过滤filebeat.inputs:- paths: [“&#x2F;var&#x2F;log&#x2F;app&#x2F;*.json”]tags: [“json”]fields可选字段，选择额外的字段进行输出可以是标量值，元组，字典等嵌套类型默认在sub-dictionary 位置filebeat.inputs:- paths: [“&#x2F;var&#x2F;log&#x2F;app&#x2F;*.log”]fields:app_id: query_engine_12fields_under_root如果值为ture，那么fields存储在输出文档的顶级位置如果与filebeat中字段冲突，自定义字段会覆盖其他字段\nfields_under_root: truefields:instance_id: i-10a64379region: us-east-1ignore_older\n\n可以指定Filebeat忽略指定时间段以外修改的日志内容文件被忽略之前，确保文件不在被读取，必须设置ignore older时间范围大于close_inactive如果一个文件正在读取时候被设置忽略，它会取得到close_inactive后关闭文件，然后文件被忽略close_*close_ *配置选项用于在特定标准或时间之后关闭harvester。 关闭harvester意味着关闭文件处理程序。 如果在harvester关闭后文件被更新，则在scan_frequency过后，文件将被重新拾取。 但是，如果在harvester关闭时移动或删除文件，Filebeat将无法再次接收文件，并且harvester未读取的任何数据都将丢失。close_inactive启动选项时，如果在制定时间没有被读取，将关闭文件句柄读取的最后一条日志定义为下一次读取的起始点，而不是基于文件的修改时间如果关闭的文件发生变化，一个新的harverster将在scan_frequency运行后被启动建议至少设置一个大于读取日志频率的值，配置多个prospector来实现针对不同更新速度的日志文件使用内部时间戳机制，来反映记录日志的读取，每次读取到最后一行日志时开始倒计时使用2h 5m 来表示\nrecursive_glob.enabled 递归匹配日志文件，默认falseclose_rename当选项启动，如果文件被重命名和移动，filebeat关闭文件的处理读取close_removed当选项启动，文件被删除时，filebeat关闭文件的处理读取这个选项启动后，必须启动clean_removedclose_eof适合只写一次日志的文件，然后filebeat关闭文件的处理读取close_timeout当选项启动时，filebeat会给每个harvester设置预定义时间，不管这个文件是否被读取，达到设定时间后，将被关闭close_timeout 不能等于ignore_older,会导致文件更新时，不会被读取如果output一直没有输出日志事件，这个timeout是不会被启动的，至少要要有一个事件发送，然后haverter将被关闭设置0 表示不启动clean_inactived从注册表文件中删除先前收获的文件的状态设置必须大于ignore_older+scan_frequency，以确保在文件仍在收集时没有删除任何状态配置选项有助于减小注册表文件的大小，特别是如果每天都生成大量的新文件此配置选项也可用于防止在Linux上重用inode的Filebeat问题clean_removed启动选项后，如果文件在磁盘上找不到，将从注册表中清除filebeat如果关闭close removed 必须关闭clean removedscan_frequencyprospector　　检查指定用于收获的路径中的新文件的频率,默认10sdocument_type类型事件，被用于设置输出文档的type字段，默认是logharvester_buffer_size每次harvester读取文件缓冲字节数，默认是16384max_bytes对于多行日志信息，很有用，最大字节数json这些选项使Filebeat解码日志结构化为JSON消息逐行进行解码jsonkeys_under_root设置key为输出文档的顶级目录overwrite_keys覆盖其他字段add_error_key定一个json_errormessage_key指定json 关键建作为过滤和多行设置，与之关联的值必须是stringmultiline控制filebeat如何处理跨多行日志的选项，多行日志通常发生在java堆栈中multiline.pattern: ‘^\\[‘multiline.negate: truemultiline.match: after上面匹配是将多行日志所有不是以[符号开头的行合并成一行它可以将下面的多行日志进行合并成一行\nException in thread &quot;main&quot; java.lang.NullPointerExceptionat com.example.myproject.Book.getTitle(Book.java:16)at com.example.myproject.Author.getBookTitles(Author.java:25)at com.example.myproject.Bootstrap.main(Bootstrap.java:14)multiline.pattern\n\n指定匹配的正则表达式，filebeat支持的regexp模式与logstash支持的模式有所不同pattern regexpmultiline.negate定义上面的模式匹配条件的动作是 否定的，默认是false假如模式匹配条件’^b’，默认是false模式，表示讲按照模式匹配进行匹配 将不是以b开头的日志行进行合并如果是true，表示将不以b开头的日志行进行合并multiline.match指定Filebeat如何将匹配行组合成事件,在之前或者之后，取决于上面所指定的negatemultiline.max_lines可以组合成一个事件的最大行数，超过将丢弃，默认500multiline.timeout定义超时时间，如果开始一个新的事件在超时时间内没有发现匹配，也将发送日志，默认是5stail_files如果此选项设置为true，Filebeat将在每个文件的末尾开始读取新文件，而不是开头此选项适用于Filebeat尚未处理的文件symlinks符号链接选项允许Filebeat除常规文件外,可以收集符号链接。收集符号链接时，即使报告了符号链接的路径，Filebeat也会打开并读取原始文件。backoffbackoff选项指定Filebeat如何积极地抓取新文件进行更新。默认1sbackoff选项定义Filebeat在达到EOF之后再次检查文件之间等待的时间。max_backoff在达到EOF之后再次检查文件之前Filebeat等待的最长时间backoff_factor指定backoff尝试等待时间几次，默认是2harvester_limitharvester_limit选项限制一个prospector并行启动的harvester数量，直接影响文件打开数enabled控制prospector的启动和关闭filebeat globalspool_size事件发送的阀值，超过阀值，强制刷新网络连接filebeat.spool_size: 2048publish_async异步发送事件，实验性功能idle_timeout事件发送的超时时间，即使没有超过阀值，也会强制刷新网络连接filebeat.idle_timeout: 5sregistry_file注册表文件的名称，如果使用相对路径，则被认为是相对于数据路径有关详细信息，请参阅目录布局部分 默认值为${path.data}&#x2F;registryfilebeat.registry_file: registryconfig_dir包含额外的prospector配置文件的目录的完整路径每个配置文件必须以.yml结尾每个配置文件也必须指定完整的Filebeat配置层次结构，即使只处理文件的prospector部分。所有全局选项（如spool_size）将被忽略必须是绝对路径filebeat.config_dir: path&#x2F;to&#x2F;configsshutdown_timeoutFilebeat等待发布者在Filebeat关闭之前完成发送事件的时间。Filebeat Generalname设置名字，如果配置为空，则用该服务器的主机名name: “my-shipper”queue_size单个事件内部队列的长度 默认1000bulk_queue_size批量事件内部队列的长度max_procs设置最大使用cpu数量geoip.paths此配置选项目前仅由Packetbeat使用，它将在6.0版中删除要使GeoIP支持功能正常，GeoLite City数据库是必需的。\ngeoip:paths:  - &quot;/usr/share/GeoIP/GeoLiteCity.dat&quot;  - &quot;/usr/local/var/GeoIP/GeoLiteCity.dat&quot;\n\nFilebeat reload属于测试功能\npath\n\n定义要检查的配置路径\nreload.enabled\n设置为true时，启用动态配置重新加载。\nreload.period\n\n定义要检查的间隔时间\nfilebeat.config.inputs:path: configs/\\*.ymlreload.enabled: truereload.period: 10s\n\n一般配置：###################### Filebeat Configuration Example ########################## This file is an example configuration file highlighting only the most common# options. The filebeat.reference.yml file from the same directory contains all the# supported options with more comments. You can use it as a reference.## You can find the full configuration reference here:# https://www.elastic.co/guide/en/beats/filebeat/index.html# For more available modules and options, please see the filebeat.reference.yml sample# configuration file.#=========================== Filebeat inputs =============================#=========================== Filebeat 输入配置 ===========================filebeat.inputs:# Each - is an input. Most options can be set at the input level, so# you can use different inputs for various configurations.# Below are the input specific configurations.# 输入filebeat的类型，包括log(具体路径的日志),stdin(键盘输入),redis,udp,docker,tcp,syslog,可以同时配置多个(包括相同类型的)# 具体的每种类型的配置信息可以通过官网:https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html 了解- type: log  # Change to true to enable this input configuration.  # 配置是否生效  enabled: true  # Paths that should be crawled and fetched. Glob based paths.  # 指定要监控的日志，可以指定具体得文件或者目录  paths:    #- /var/log/*.log (这是默认的,自行可以修改）    - /usr/local/tomcat/logs/catalina.out  # Exclude lines. A list of regular expressions to match. It drops the lines that are  # matching any regular expression from the list.  # 在输入中排除符合正则表达式列表的那些行。  #exclude_lines: [&#x27;^DBG&#x27;]  # Include lines. A list of regular expressions to match. It exports the lines that are  # matching any regular expression from the list.  # 包含输入中符合正则表达式列表的那些行（默认包含所有行），include_lines执行完毕之后会执行exclude_lines  #include_lines: [&#x27;^ERR&#x27;, &#x27;^WARN&#x27;]  # Exclude files. A list of regular expressions to match. Filebeat drops the files that  # are matching any regular expression from the list. By default, no files are dropped.  # 忽略掉符合正则表达式列表的文件  #exclude_files: [&#x27;.gz$&#x27;]  # Optional additional fields. These fields can be freely picked  # to add additional information to the crawled log files for filtering  # 向输出的每一条日志添加额外的信息，比如“level:debug”，方便后续对日志进行分组统计。  # 默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录，例如fields.level  # 这个得意思就是会在es中多添加一个字段，格式为 &quot;filelds&quot;:&#123;&quot;level&quot;:&quot;debug&quot;&#125;  #fields:  #  level: debug  #  review: 1  #  module: mock   ### Multiline options  ### 日志中经常会出现多行日志在逻辑上属于同一条日志的情况，所以需要multiline参数来详细阐述。  # Multiline can be used for log messages spanning multiple lines. This is common  # for Java Stack Traces or C-Line Continuation  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [  # 多行匹配正则表达式，比如：用空格开头(^[[:space:]]),或者是否以[开头(^\\[)。正则表达式是非常复杂的，详细见filebeat的正则表达式官方链接：https://www.elastic.co/guide/en/beats/filebeat/current/regexp-support.html  multiline.pattern: ^\\[  # Defines if the pattern set under pattern should be negated or not. Default is false.  # 该参数意思是是否否定多行融入。  #multiline.negate: false  # Match can be set to &quot;after&quot; or &quot;before&quot;. It is used to define if lines should be append to a pattern  # that was (not) matched before or after or as long as a pattern is not matched based on negate.  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash  # 取值为after或before。该值与上面的pattern与negate值配合使用  # ----------------------------------------------------------------------------------------------------  #|multiline.pattern|multiline.negate|multiline.match|                      结论                      |  # ----------------------------------------------------------------------------------------------------  #|      true      |    true        |    before    |表示匹配行是结尾,和前面不匹配的组成一行完整的日志|  # ----------------------------------------------------------------------------------------------------  #|      true      |    true        |    after    |表示匹配行是开头,和后面不匹配的组成一行完整的日志|  # ----------------------------------------------------------------------------------------------------  #|      true      |    false      |    before    |表示匹配的行和后面不匹配的一行组成一行完整的日志 |  # ----------------------------------------------------------------------------------------------------  #|      true      |    false      |    after    |表示匹配的行和前面不匹配的一行组成一行完整的日志 |  # ----------------------------------------------------------------------------------------------------  multiline.match: after  # Specifies a regular expression, in which the current multiline will be flushed from memory, ending the multiline-message.  # 表示符合该正则表达式的，将从内存刷入硬盘。  #multiline.flush_pattern  # The maximum number of lines that can be combined into one event.  # If the multiline message contains more than max_lines, any additional lines are discarded. The default is 500.  # 表示如果多行信息的行数超过该数字，则多余的都会被丢弃。默认值为500行  #multiline.max_lines: 500  # After the specified timeout, Filebeat sends the multiline event even if no new pattern is found to start a new event. The default is 5s.  # 表示超过timeout的时间(秒)还没有新的一行日志产生，则自动结束当前的多行、形成一条日志发出去  #multiline.timeout: 5#============================= Filebeat modules ===============================# 引入filebeat的module配置filebeat.config.modules:  # Glob pattern for configuration loading  path: $&#123;path.config&#125;/modules.d/*.yml  # Set to true to enable config reloading  # 是否允许重新加载  reload.enabled: false  # Period on which files under path should be checked for changes  # 重新加载的时间间隔  #reload.period: 10s#==================== Elasticsearch template setting ==========================# Elasticsearch模板配置setup.template.settings:  # 数据分片数  index.number_of_shards: 3  # 数据分片备份数  #index.number_of_replicas: 1  #index.codec: best_compression  #_source.enabled: false#================================ General =====================================# The name of the shipper that publishes the network data. It can be used to group# all the transactions sent by a single shipper in the web interface.# 设置filebeat的名字，如果配置为空，则用该服务器的主机名#name:# The tags of the shipper are included in their own field with each# transaction published.# 额外添加的tag标签#tags: [&quot;service-X&quot;, &quot;web-tier&quot;]# Optional fields that you can specify to add additional information to the# output.# 额外添加的字段和值#fields:#  env: staging#============================== Dashboards =====================================# dashboards的相关配置# These settings control loading the sample dashboards to the Kibana index. Loading# the dashboards is disabled by default and can be enabled either by setting the# options here, or by using the `-setup` CLI flag or the `setup` command.# 是否启用仪表盘#setup.dashboards.enabled: false# The URL from where to download the dashboards archive. By default this URL# has a value which is computed based on the Beat name and version. For released# versions, this URL points to the dashboard archive on the artifacts.elastic.co# website.# 仪表盘地址#setup.dashboards.url:#============================== Kibana =====================================# kibana的相关配置# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.# This requires a Kibana endpoint configuration.setup.kibana:  # Kibana Host  # Scheme and port can be left out and will be set to the default (http and 5601)  # In case you specify and additional path, the scheme is required: http://localhost:5601/path  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601  # kibana地址  #host: &quot;localhost:5601&quot;  # Kibana Space ID  # ID of the Kibana Space into which the dashboards should be loaded. By default,  # the Default Space will be used.  # kibana的空间ID  #space.id:#============================= Elastic Cloud ==================================# These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/).# The cloud.id setting overwrites the `output.elasticsearch.hosts` and# `setup.kibana.host` options.# You can find the `cloud.id` in the Elastic Cloud web UI.#cloud.id:# The cloud.auth setting overwrites the `output.elasticsearch.username` and# `output.elasticsearch.password` settings. The format is `&lt;user&gt;:&lt;pass&gt;`.#cloud.auth:#================================ Outputs =====================================# 输出配置# Configure what output to use when sending the data collected by the beat.#-------------------------- Elasticsearch output ------------------------------# 输出到es#output.elasticsearch:  # Array of hosts to connect to.  # ES地址  # hosts: [&quot;localhost:9200&quot;]  # ES索引  # index: &quot;filebeat-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;&quot;  # Optional protocol and basic auth credentials.  # 协议  #protocol: &quot;https&quot;  # ES用户名  #username: &quot;elastic&quot;  # ES密码  #password: &quot;changeme&quot;#----------------------------- Logstash output --------------------------------# 输出到logstashoutput.logstash:  # The Logstash hosts  # logstash地址  hosts: [&quot;localhost:5044&quot;]  # Optional SSL. By default is off.  # List of root certificates for HTTPS server verifications  #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;]  # Certificate for SSL client authentication  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;  # Client Certificate Key  #ssl.key: &quot;/etc/pki/client/cert.key&quot;#================================ Procesors =====================================# Configure processors to enhance or manipulate events generated by the beat.processors:  #主机相关 信息  - add_host_metadata: ~# 云服务器的元数据信息,包括阿里云ECS 腾讯云QCloud AWS的EC2的相关信息   - add_cloud_metadata: ~  #k8s元数据采集  #- add_kubernetes_metadata: ~  # docker元数据采集  #- add_docker_metadata: ~  # 执行进程的相关数据  #- - add_process_metadata: ~#================================ Logging =====================================# Sets log level. The default log level is info.# Available log levels are: error, warning, info, debug#logging.level: debug# At debug level, you can selectively enable logging only for some components.# To enable all selectors use [&quot;*&quot;]. Examples of other selectors are &quot;beat&quot;,# &quot;publish&quot;, &quot;service&quot;.#logging.selectors: [&quot;*&quot;]#============================== Xpack Monitoring ===============================# filebeat can export internal metrics to a central Elasticsearch monitoring# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The# reporting is disabled by default.# Set to true to enable the monitoring reporter.#xpack.monitoring.enabled: false# Uncomment to send the metrics to Elasticsearch. Most settings from the# Elasticsearch output are accepted here as well. Any setting that is not set is# automatically inherited from the Elasticsearch output configuration, so if you# have the Elasticsearch output configured, you can simply uncomment the# following line.#xpack.monitoring.elasticsearch:\n","categories":["ELK"],"tags":["ELK"]},{"title":"hadoop集群搭建","url":"/20241121/Hadoop/d203e0d0df22/","content":"hadoop集群搭建集群列表\n\n\n服务器\n地址\n角色\n备注\n\n\n\nhadoop1\n192.168.11.81\nnamenode datanode\n32G 12C 800G\n\n\nhadoop2\n192.168.11.82\nsencondarynode datanode\n32G 12C 800G\n\n\nhadoop3\n192.168.11.83\ndatanode\n32G 12C 800G\n\n\nmysql8\n192.168.11.72\n元数据数据库\n\n\n\n一.基础环境设置关闭防火墙systemctl stop firewalldsystemctl disable firewalld\n\n关闭selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/configsetenforce 0\n\n创建用户和组useradd hadoopgroupadd hadoop\n\n添加hostsvi /etc/hosts192.168.11.81 hadoop1192.168.11.82 hadoop2192.168.11.83 hadoop3\n\n开启ssh免密登录ssh-keygen -t rsassh-copy-id hadoop1ssh-copy-id hadoop2ssh-copy-id hadoop3\n\n安装jdk1.8tar -xzvf jdk-8u271-linux-x64.tar.gz -C /cat &gt;&gt;/etc/profile &lt;&lt;EOF# java1.8export JAVA_HOME=/jdk1.8.0_271export JRE_HOME=/jdk1.8.0_271/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHEOF# 加载环境变量source /etc/profile# 验证javajava -version\n\n注:其它节点也需要安装jdk\n二.安装hadoop下载hadoop包http://archive.apache.org/dist/hadoop/core/https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop\n\n上传解压hadoop包tar -zxvf hadoop-2.6.4.tar.gz -C /data\n\n添加环境变量cat &gt;&gt;/etc/profile &lt;&lt;EOF# hadoopexport HADOOP_HOME=/data/hadoop-2.6.4export PATH=$HADOOP_HOME/bin:$PATHEOFsource /etc/profile\n\n配置hadoop参数配置文件放在$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;下\ncd $HADOOP_HOME/etc/hadoop/vi hadoop-env.shexport JAVA_HOME=/jdk1.8.0_271vi yarn-env.shexport JAVA_HOME=/jdk1.8.0_271\n\n配置工作节点\nvi slaveshadoop1hadoop2hadoop3\n\n配置hadoop参数\nvi core-site.xml\n\n &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/dfs/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;!-- HUE配置，没有可忽略 --&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;  &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;\n\nvi hdfs-site.xml\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;/dfs/nn/&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;/dfs/dn/&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;3&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.nameservices&lt;/name&gt;                &lt;value&gt;hadoop1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;hadoop2:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.permissions&lt;/name&gt;                &lt;value&gt;false&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;\n\ncp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml\n\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;                &lt;final&gt;true&lt;/final&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;                &lt;value&gt;hadoop3:50030&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;hadoop3:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;hadoop3:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapred.job.tracker&lt;/name&gt;                &lt;value&gt;hadoop3:9001&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;\n\nvi yarn-site.xml\n\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;hadoop1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;                &lt;value&gt;hadoop1:8032&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;                &lt;value&gt;hadoop1:8030&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;                &lt;value&gt;hadoop1:8031&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;                &lt;value&gt;hadoop1:8033&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;                &lt;value&gt;hadoop1:8088&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;                &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;        &lt;/property&gt;       &lt;property&gt;              &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;             &lt;value&gt;false&lt;/value&gt;      &lt;/property&gt;&lt;/configuration&gt;\n\n注:以上配置需要复制到其它节点中\n三.验证安装格式化文件系统注意:namenode第一次需要格式化文件系统，格式化后切忌不要再格式化!!!\nnamenode格式化文件系统\nhadoop namenode -format\n\n若没有hadoop命令可去$HADOOP_HOME&#x2F;bin中寻找\n启动服务启动namenode进程\ncd $HADOOP_HOME/sbinhadoop-daemon.sh start namenode\n\n启动datanode进程\ncd $HADOOP_HOME/sbinhadoop-daemon.sh start datanode\n\n或者一次性启动\ncd $HADOOP_HOME/sbin./start-all.sh\n\n检查节点配置情况[root@hadoop1 ~]# jps7616 NodeManager7074 DataNode7331 ResourceManager13703 Jps6939 NameNode\n\n关闭服务$HADOOP_HOME&#x2F;sbin有相应的停止脚本\n# 停止所有服务cd $HADOOP_HOME/sbin./stop-all.sh\n\n四.web可视化各个服务启动成功后会有相应的web界面\n# 节点管理http://192.168.11.81:8088# 资源管理http://192.168.11.81:50070 \n\n\n五.集群调优\n\n\t\n    \n\t\n\n\n\n","categories":["Hadoop"],"tags":["Hadoop"]},{"title":"kubeadm config 命令","url":"/20241121/Kubernetes/b56b635b029d/","content":"kubeadm config 命令◎　kubeadm config upload from-file：由配置文件上传到集群中生成ConfigMap。◎　kubeadm config upload from-flags：由配置参数生成ConfigMap。◎　kubeadm config view：查看当前集群中的配置值。◎　kubeadm config print init-defaults：输出kubeadm init默认参数文件的内容。◎　kubeadm config print join-defaults：输出kubeadm join默认参数文件的内容。◎　kubeadm config migrate：在新旧版本之间进行配置转换。◎　kubeadm config images list：列出所需的镜像列表。◎　kubeadm config images pull：拉取镜像到本地。\n例如，执行kubeadm config print init-defaults，可以取得默认的初始化参数文件：\nkubeadm config print init-defaults &gt; ini.default.yaml","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"kubelet修改PLEG的检查时间为30s","url":"/20241121/Kubernetes/df70045b3de2/","content":"kubelet修改PLEG的检查时间为30svi /etc/systemd/system/kubelet.service 增加--housekeeping-interval=30s\n[Unit]Description=kubelet: The Kubernetes Node AgentDocumentation=http://kubernetes.io/docs/[Service]ExecStart=/usr/bin/kubelet --housekeeping-interval=30s # 增加参数ExecStartPre=/usr/bin/kubelet-pre-start.shRestart=alwaysStartLimitInterval=0RestartSec=10[Install]WantedBy=multi-user.target","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"Kubernetes之调试DNS解析","url":"/20241121/Kubernetes/2c84e44bdf7d/","content":"Kubernetes之调试DNS解析创建一个简单的 Pod 来用作测试环境使用以下内容创建一个名为 busybox.yaml 的文件：\napiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - name: busybox    image: busybox    command:      - sleep      - &quot;3600&quot;    imagePullPolicy: IfNotPresent  restartPolicy: Always\n\n然后使用此文件创建一个 pod 并验证其状态：\nkubectl create -f busybox.yaml# pod &quot;busybox&quot; createdkubectl get pods busybox# NAME      READY     STATUS    RESTARTS   AGE# busybox   1/1       Running   0          &lt;some-time&gt;\n\n一旦该 pod 运行，您就可以在环境中执行 nslookup。如果您看到如下所示的内容，则 DNS 工作正常。\nkubectl exec -ti busybox -- nslookup kubernetes.default# Server:    10.0.0.10# Address 1: 10.0.0.10# Name:      kubernetes.default# Address 1: 10.0.0.1\n\n如果 nslookup 命令失败，请检查以下内容：\n首先检查本地 DNS 配置看一看 resolv.conf 文件\nkubectl exec busybox cat /etc/resolv.conf\n\n验证搜索路径和名称服务器是否设置如下（请注意，搜索路径可能因不同的云提供商而异）：\nsearch default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internalnameserver 10.0.0.10options ndots:5\n\n以下错误表明 kube-dns 附加组件或相关服务存在问题：\nkubectl exec -ti busybox -- nslookup kubernetes.default# Server:    10.0.0.10# Address 1: 10.0.0.10# nslookup: can&#x27;t resolve &#x27;kubernetes.default&#x27;\n或者\nkubectl exec -ti busybox -- nslookup kubernetes.default# Server:    10.0.0.10# Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local# nslookup: can&#x27;t resolve &#x27;kubernetes.default&#x27;\n\n检查 DNS pod 是否正在运行中使用 kubectl get pods 命令验证 DNS pod 是否正在运行中。\nkubectl get pods --namespace=kube-system -l k8s-app=kube-dns# NAME                    READY     STATUS    RESTARTS   AGE# ...# kube-dns-v19-ezo1y      3/3       Running   0           1h# ...\n\n如果您看到没有 pod 正在运行中，或者 pod 已失败&#x2F;已完成，那么在当前环境中，默认情况下可能不会部署 DNS 插件，您将不得不手动部署它。\n检查 DNS pod 中的错误使用 kubectl logs 命令查看 DNS 守护程序的日志。\nkubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c dnsmasq$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c sidecar\n\n看看有没有可疑的日志。字母 ‘W‘、’E‘、’F’ 表示警告、错误和失败。\nDNS服务起来了吗？通过使用 kubectl get service 命令验证 DNS 服务已启动。\nkubectl get svc --namespace=kube-system# NAME          CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE# ...# kube-dns      10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h# ...\n如果您已经创建了该服务，或者应该在默认情况下创建它，但它没有出现。\nDNS endpoints 是否暴露？您可以使用 kubectl get endpoints 命令验证是否暴露了了 DNS endpoints。\nkubectl get ep kube-dns --namespace=kube-system# NAME       ENDPOINTS                       AGE# kube-dns   10.180.3.17:53,10.180.3.17:53    1h","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"ntp时间同步","url":"/20241121/CentOS/04bb4545c9da/","content":"ntp时间同步1.安装ntp服务\nyum安装ntp服务\n\nyum install -y ntp\n2.设置自启动chkconfig ntpd onchkconfig --list ntpd# 或者systemctl enable ntpd\n3.配置内网NTP-Server192.168.11.155\nvim &#x2F;etc&#x2F;ntp.conf\n\n# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface.  This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1 restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#broadcast 192.168.1.255 autokey\t# broadcast server#broadcastclient\t\t\t# broadcast client#broadcast 224.0.1.1 autokey\t\t# multicast server#multicastclient 224.0.1.1\t\t# multicast client#manycastserver 239.255.254.254\t\t# manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitorrestrict 192.168.11.155 nomodify notrap noquery# 配置允许上游时间服务器主动修改本机的时间，你自己ntp server的ipserver 192.168.11.155fudge 192.168.11.155 stratum 5# 不允许来自公网上ipv4和ipv6客户端的访问restrict -4 default kod notrap nomodify nopeer noqueryrestrict -6 default kod notrap nomodify nopeer noquery# 外部时间服务器不可用时，以本地时间作为时间服务server  127.127.1.0     # local clockfudge   127.127.1.0 stratum 10  #(3-10)\n配置文件修改完成，保存退出，重新启动服务\nsystemctl restart ntpd\n查看服务连接和监听\nnetstat -tlunp | grep ntp\n\n无netstat命令 使用 yum install -y net-tools安装\n\n4.配置内网NTP-Clientsvim /etc/ntp.conf\n# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notraprestrict 192.168.204.111 nomodify notrap noquery# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# 配置时间服务器为本地的时间服务器server 192.168.11.155# Undisciplined Local Clock. This is a fake driver intended for backup# and when no outside source of synchronized time is available.# 外部时间服务器不可用时，以本地时间作为时间服务server  127.127.1.0     # local clockfudge   127.127.1.0 stratum 10  #(3-10)\n\n修改以上内容即可\n\n重新启动ntpd服务\nsystemctl restart ntpd\n\n5.ntpdate手动同步下时间ntpdate -u 192.168.11.155[root@hadoop1 hadoop_data]# ntpdate -u 192.168.11.155# 22 Jan 14:42:13 ntpdate[12880]: step time server 192.168.11.155 offset -0.544326 sec","categories":["CentOS"],"tags":["CentOS"]},{"title":"oracle下载地址","url":"/20241121/Oracle/88f8f57e2744/","content":"ORACLE下载地址ORACLE11GR2Oracle Database 11g Release 2 (11.2.0.1.0) for Microsoft Windows (64-bit)\nhttp://download.oracle.com/otn/nt/oracle11g/112010/win64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/nt/oracle11g/112010/win64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/nt/oracle11g/112010/win64_11gR2_client.ziphttp://download.oracle.com/otn/nt/oracle11g/112010/win64_11gR2_grid.zip\n\nOracle Database 11g Release 2 (11.2.0.1.0) for Microsoft Windows (32-bit)\nhttp://download.oracle.com/otn/nt/oracle11g/112010/win32_11gR2_database_1of2.ziphttp://download.oracle.com/otn/nt/oracle11g/112010/win32_11gR2_database_2of2.ziphttp://download.oracle.com/otn/nt/oracle11g/112010/win32_11gR2_client.zip\n\nOracle Database 11g Release 2 (11.2.0.1.0) for Linux x86\nhttp://download.oracle.com/otn/linux/oracle11g/R2/linux_11gR2_database_1of2.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux_11gR2_database_2of2.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux_11gR2_client.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux_11gR2_grid.zip\nOracle Database 11g Release 2 (11.2.0.1.0) for Linux x86-64\nhttp://download.oracle.com/otn/linux/oracle11g/R2/linux.x64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux.x64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux.x64_11gR2_client.ziphttp://download.oracle.com/otn/linux/oracle11g/R2/linux.x64_11gR2_grid.zip\nOracle Database 11g Release 2 (11.2.0.1.0) for Solaris Operating System (SPARC) (64-bit)\nhttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.sparc64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.sparc64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.sparc64_11gR2_client.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.sparc64_11gR2_client32.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.sparc64_11gR2_grid.zip\nOracle Database 11g Release 2 (11.2.0.1.0) for Solaris Operating System (x86-64)\nhttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.x64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.x64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.x64_11gR2_client.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.x86_11gR2_client.ziphttp://download.oracle.com/otn/solaris/oracle11g/R2/solaris.x64_11gR2_grid.zip\n\nOracle Database 11g Release 2 (11.2.0.1.0) for HP-UX Itanium\nhttp://download.oracle.com/otn/hp/oracle11g/R2/hpia64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpia64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpia64_11gR2_client.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpia64_11gR2_client32.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpia64_11gR2_grid.zip\nOracle Database 11g Release 2 (11.2.0.1.0) for HP-UX PA-RISC (64-bit)\nhttp://download.oracle.com/otn/hp/oracle11g/R2/hpux.parisc64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpux.parisc64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpux.parisc64_11gR2_client.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpux.parisc32_11gR2_client.ziphttp://download.oracle.com/otn/hp/oracle11g/R2/hpux.parisc64_11gR2_grid.zip\n\nOracle Database 11g Release 2 (11.2.0.1.0) for AIX (PPC64)\nhttp://download.oracle.com/otn/aix/oracle11g/R2/aix.ppc64_11gR2_database_1of2.ziphttp://download.oracle.com/otn/aix/oracle11g/R2/aix.ppc64_11gR2_database_2of2.ziphttp://download.oracle.com/otn/aix/oracle11g/R2/aix.ppc64_11gR2_client.ziphttp://download.oracle.com/otn/aix/oracle11g/R2/aix.ppc32_11gR2_client.ziphttp://download.oracle.com/otn/aix/oracle11g/R2/aix.ppc64_11gR2_grid.zip\n\nORACLE10GR2Oracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for Microsoft Windows (32-bit)\nhttp://download.oracle.com/otn/nt/oracle10g/10201/10201_database_win32.ziphttp://download.oracle.com/otn/nt/oracle10g/10201/10201_client_win32.ziphttp://download.oracle.com/otn/nt/oracle10g/10201/10201_clusterware_win32.ziphttp://download.oracle.com/otn/nt/oracle10g/10201/10201_gateways_win32.zip\nOracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for Microsoft Windows (x64)\nhttp://download.oracle.com/otn/nt/oracle10g/10201/102010_win64_x64_database.ziphttp://download.oracle.com/otn/nt/oracle10g/10201/102010_win64_x64_client.ziphttp://download.oracle.com/otn/nt/oracle10g/10201/102010_win64_x64_clusterware.zip\nOracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for Linux x86\nhttp://download.oracle.com/otn/linux/oracle10g/10201/10201_database_linux32.ziphttp://download.oracle.com/otn/linux/oracle10g/10201/10201_client_linux32.ziphttp://download.oracle.com/otn/linux/oracle10g/10201/10201_gateways_linux32.zip\nOracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for Linux x86-64\nhttp://download.oracle.com/otn/linux/oracle10g/10201/10201_database_linux_x86_64.cpio.gzhttp://download.oracle.com/otn/linux/oracle10g/10201/10201_client_linux_x86_64.cpio.gzhttp://download.oracle.com/otn/linux/oracle10g/10201/10201_clusterware_linux_x86_64.cpio.gzhttp://download.oracle.com/otn/linux/oracle10g/10201/10201_gateways_linux_x86_64.cpio.gz\nOracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for AIX5L\nhttp://download.oracle.com/otn/aix/oracle10g/10201/10gr2_aix5l64_database.cpio.gzhttp://download.oracle.com/otn/aix/oracle10g/10201/10gr2_aix5l64_client.cpio.gzhttp://download.oracle.com/otn/aix/oracle10g/10201/10gr2_aix5l64_cluster.cpio.gzhttp://download.oracle.com/otn/aix/oracle10g/10201/10gr2_aix5l64_gateways.cpio.gz\nOracle Database 10g Release 2 (10.2.0.2) Enterprise&#x2F;Standard Edition for Solaris Operating System (x86)\nhttp://download.oracle.com/otn/solaris/oracle10g/10202/10202_database_solx86.ziphttp://download.oracle.com/otn/solaris/oracle10g/10202/10202_client_solx86.ziphttp://download.oracle.com/otn/solaris/oracle10g/10202/10202_clusterware_solx86.zip\nOracle Database 10g Release 2 (10.2.0.1.0) Enterprise&#x2F;Standard Edition for Solaris Operating System (x86-64)\nhttp://download.oracle.com/otn/solaris/oracle10g/10201/x8664/10201_database_solx86_64.ziphttp://download.oracle.com/otn/solaris/oracle10g/10201/x8664/10201_client_solx86_64.ziphttp://download.oracle.com/otn/solaris/oracle10g/10201/x8664/10201_clusterware_solx86_64.zip\n\nEND\n","categories":["Oracle"],"tags":["Oracle"]},{"title":"python实现简单的ftp","url":"/20241121/Python/53856e82a5be/","content":"python实现简单的ftp会自动生成配置文件，下次启动会读取配置文件内的参数。\nini内容\n[FTP]#ftp用户user=ftp#密码password=1#端口号port=23#路径path=F:\\ftp\\\n\n\n\n代码如下:\n# -*- coding: utf-8 -*-from pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServerfrom pyftpdlib.authorizers import DummyAuthorizerimport configparser,os,sys########################################################################################## Variable needs to be modified# 需要修改的变量#########################################################################################def getVariable():  global user ,password ,path ,port  user = str(config.get(&#x27;FTP&#x27;, &#x27;user&#x27;))  password = str(config.get(&#x27;FTP&#x27;, &#x27;password&#x27;))  path = str(config.get(&#x27;FTP&#x27;, &#x27;path&#x27;))  port = config.get(&#x27;FTP&#x27;, &#x27;port&#x27;)  print(&quot;###############################################################&quot;)  print(&quot;# [FTP]&quot;)  print(&quot;# user=&quot;+user+&quot;\\n&quot;+&quot;# password=&quot;+password+&quot;\\n&quot;+&quot;# port&quot;+port+&quot;\\n&quot;+&quot;# path&quot;+path)  print(&quot;###############################################################\\n&quot;)########################################################################################## Reading configuration Files# 读取配置文件#########################################################################################global config# 生成ConfigParser对象config = configparser.ConfigParser()os.chdir(os.path.abspath(sys.path[0]))filename = &#x27;config.ini&#x27;#os.path.join(os.path.abspath(sys.path[0]),&#x27;config.ini&#x27;)iniExists=os.path.exists(os.path.join(os.path.abspath(sys.path[0]),&#x27;config.ini&#x27;))print(filename)print(iniExists)if iniExists == False:    with open(filename, mode=&#x27;w&#x27;) as file_ini:        file_ini.write(&quot;[FTP]\\n&quot;)        file_ini.write(&quot;#ftp用户\\n&quot;)        file_ini.write(&quot;user=ftp\\n&quot;)        file_ini.write(&quot;#密码\\n&quot;)        file_ini.write(&quot;password=1\\n&quot;)        file_ini.write(&quot;#端口号\\n&quot;)        file_ini.write(&quot;port=23\\n&quot;)        file_ini.write(&quot;#路径\\n&quot;)        file_ini.write(&quot;path=&quot;+os.path.dirname(__file__))#print(os.path.abspath(sys.path[0]))# 获取Variable变量config.read(filename, encoding=&#x27;utf-8&#x27;)getVariable()authorizer = DummyAuthorizer()authorizer.add_user(user, password, path, perm=&#x27;elradfmwM&#x27;)handler = FTPHandlerhandler.authorizer = authorizerserver = FTPServer((&#x27;0.0.0.0&#x27;, str(port)), handler)server.serve_forever()\n\n\n\n执行报错需要安装pyftpdlib与configparser模块\npip install pyftpdlibpip install configparser\n\n","categories":["Python"],"tags":["Python"]},{"title":"python连接elasticsearch并将数据传入kafka","url":"/20241121/Python/3800e8f7b389/","content":"python连接elasticsearch并将数据传入kafka\n脚本功能：\n\n配置本地配置config.ini(不存在运行时会自动创建),ptyhon连接本地sqlite数据库文件获取数据，校验数据在elasticsearch索引中是否存在，不存在则传入kafka，在由kafka自动传入logstash 最后传入elasticsearch中。\n代码如下\n# -*- coding:utf-8 -*-import sqlite3,json,socket,osimport sys # 1from datetime import datetimefrom elasticsearch import Elasticsearchfrom kafka import KafkaProducerimport configparser########################################################################################## Variable needs to be modified# 需要修改的变量#########################################################################################def getVariable():  # 集群地址  # host1=&quot;192.168.11.156&quot;  # host2=&quot;192.168.11.157&quot;  # host3=&quot;192.168.11.159&quot;  # kafka1=&quot;192.168.11.156:9092&quot;  # kafka2=&quot;192.168.11.157:9092&quot;  # kafka3=&quot;192.168.11.159:9092&quot;  # 索引名称  # indexName=&quot;syslogdebug&quot; + &quot;-&quot; + str(datetime.now().year) + &quot;.&quot; + str(datetime.now().month)  # indexName2=&quot;syslogerror&quot; + &quot;-&quot; + str(datetime.now().year) + &quot;.&quot; + str(datetime.now().month)    global host1 ,host2 ,host3 ,kafkaPort, kafka1, kafka2, kafka3, indexName, indexName2, dbPath  host1 = str(config.get(&#x27;variable&#x27;, &#x27;host1&#x27;))  host2 = str(config.get(&#x27;variable&#x27;, &#x27;host2&#x27;))  host3 = str(config.get(&#x27;variable&#x27;, &#x27;host3&#x27;))  kafkaPort = config.get(&#x27;variable&#x27;, &#x27;kafkaPort&#x27;)  kafka1 = config.get(&#x27;variable&#x27;, &#x27;kafka1&#x27;) + &quot;:&quot; + kafkaPort  kafka2 = config.get(&#x27;variable&#x27;, &#x27;kafka2&#x27;) + &quot;:&quot; + kafkaPort  kafka3 = config.get(&#x27;variable&#x27;, &#x27;kafka3&#x27;) + &quot;:&quot; + kafkaPort    # 索引名称  indexName = str(config.get(&#x27;variable&#x27;, &#x27;indexName&#x27;)) + &quot;-&quot; + str(datetime.now().year) + &quot;.&quot; + str(datetime.now().month)  indexName2 = str(config.get(&#x27;variable&#x27;, &#x27;indexName2&#x27;)) + &quot;-&quot; + str(datetime.now().year) + &quot;.&quot; + str(datetime.now().month)  # db文件位置  dbPath=str(config.get(&#x27;variable&#x27;, &#x27;dbPath&#x27;))  print(&quot;\\n[variable]&quot;)  print(&quot;host1=&quot;+host1+&quot;\\n&quot;+&quot;host2=&quot;+host2+&quot;\\n&quot;+&quot;host3&quot;+host3)  print(&quot;kafkaPort=&quot;+kafkaPort+&quot;\\n&quot;+&quot;kafka1=&quot;+kafka1+&quot;\\n&quot;+&quot;kafka2=&quot;+kafka2+&quot;\\n&quot;+&quot;kafka3=&quot;+kafka3)  print(&quot;indexName=&quot;+indexName+&quot;\\n&quot;+&quot;indexName=&quot;+indexName+&quot;&quot;)  print(&quot;dbPath=&quot;+dbPath+&quot;\\n&quot;)########################################################################################## Default variable# 默认的变量#########################################################################################def getDefault():  # es 端口 默认为9200  # port=&quot;9200&quot;  # es 默认用户  # user=&quot;elastic&quot;  # es 密码  # passwd=&quot;Root@123&quot;  global port, user, passwd  port = str(config.get(&#x27;default&#x27;, &#x27;port&#x27;)).strip(&#x27;&quot;&#x27;)  user = str(config.get(&#x27;default&#x27;, &#x27;user&#x27;)).strip(&#x27;&quot;&#x27;)  passwd = str(config.get(&#x27;default&#x27;, &#x27;passwd&#x27;)).strip(&#x27;&quot;&#x27;)  print(&quot;[default]&quot;)  print(&quot;port=&quot;+port+&quot;\\n&quot;+&quot;user=&quot;+user+&quot;\\n&quot;+&quot;passwd=&quot;+passwd+&quot;\\n&quot;)########################################################################################### function: select ip# 函数: 查询本机ip地址##########################################################################################def get_host_ip():    try:        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)        s.connect((&#x27;8.8.8.8&#x27;, 80))        ip = s.getsockname()[0]    finally:        s.close()    return ip########################################################################################### function: select index is or not exists# 函数: 查询索引是否存在##########################################################################################def indicesExists(my_index):  res = es.indices.exists(my_index)  return res########################################################################################### function: create index# 函数: 创建索引##########################################################################################def createIndex(my_index):  res = es.indices.create(index=my_index, ignore=400)  return resdef main():  #########################################################################################  # The system configuration  # 系统配置  #########################################################################################  reload(sys) # 2  sys.setdefaultencoding(&#x27;utf-8&#x27;) # 3  #########################################################################################  # Get IP   # 获取IP地址  #########################################################################################  global IP  IP=get_host_ip()if __name__ == &#x27;__main__&#x27;:    main()########################################################################################## Reading configuration Files# 读取配置文件#########################################################################################global config# 生成ConfigParser对象config = configparser.ConfigParser()# 读取配置文件filename = &#x27;config.ini&#x27;config.read(filename, encoding=&#x27;utf-8&#x27;)# 获取Variable变量getVariable()# 获取Default变量getDefault()########################################################################################## create Elasticsearch connection# 创建es连接#########################################################################################es = Elasticsearch(   [       &#123;&quot;host&quot;: host1, &quot;port&quot;: port&#125;,        &#123;&quot;host&quot;: host2, &quot;port&quot;: port&#125;,       &#123;&quot;host&quot;: host3, &quot;port&quot;: port&#125;   ],   http_auth=(user, passwd),    timeout=3600   )########################################################################################## create kafka connection# 创建kafka连接#########################################################################################producer = KafkaProducer(                            bootstrap_servers=[kafka1,kafka2,kafka3],                            value_serializer=lambda v: json.dumps(v).encode(&#x27;utf-8&#x27;)                         )########################################################################################## check index exists if not exists to create# 检查索引，不存在就创建#########################################################################################indexEx=indicesExists(indexName)if indexEx != True:    createIndex(indexName)########################################################################################## connect sqlite Datebase And select data# 连接sqlite数据库,查询数据#########################################################################################conn = sqlite3.connect(dbPath)c = conn.cursor()cursor = c.execute(&quot;SELECT * from SysLogDebug&quot;)########################################################################################## insert SysLogDebug data# 插入SysLogDebug表数据#########################################################################################for row in cursor:  data = dict([(&quot;ip&quot;,IP),(&quot;logid&quot;,str(row[0])),(&quot;date&quot;,str(row[1])),(&quot;assemblyname&quot;,str(row[2])),(&quot;namespacename&quot;,str(row[3])),(&quot;classname&quot;,str(row[4])),(&quot;methodname&quot;,str(row[5])),(&quot;elapsedtime&quot;,str(row[6])),(&quot;message&quot;,str(row[7]))])  logidEx=es.search(index=indexName, doc_type=&#x27;_doc&#x27;, body=&#123;&quot;query&quot;: &#123;&quot;match&quot;:&#123;&quot;logid&quot;: row[0]&#125;&#125;&#125;)  if str(logidEx).find(&quot;logid&quot;) == -1:    producer.send(&quot;syslogdebug&quot;, data)  # 测试  #if row[0] == 40:  #  break########################################################################################## check index exists if not exists to create# 检查索引，不存在就创建#########################################################################################indexEx=indicesExists(indexName2)if indexEx != True:    createIndex(indexName2)########################################################################################## select data# 查询数据#########################################################################################cursor2 = c.execute(&quot;SELECT * from SysLogError&quot;)########################################################################################## insert SysLogError data# 插入SysLogError表数据#########################################################################################for row in cursor2:  data2 = dict([(&quot;ip&quot;,IP),(&quot;logid&quot;,str(row[0])),(&quot;date&quot;,str(row[1])),(&quot;message&quot;,str(row[2]))])  logidEx2=es.search(index=indexName2, doc_type=&#x27;_doc&#x27;, body=&#123;&quot;query&quot;: &#123;&quot;match&quot;:&#123;&quot;logid&quot;: row[0]&#125;&#125;&#125;)  if str(logidEx2).find(&quot;logid&quot;) == -1:    producer.send(&quot;syslogerror&quot;, data2)  #测试  #if row[0] == 15:  #  break########################################################################################## Close the connection# 关闭连接##########################################################################################producer.flush()producer.close()conn.close()#python = sys.executable  # 获取当前执行python#os.execl(python, python, *sys.argv)  # 执行命令exit\n\n","categories":["ELK","Kafka","Python"],"tags":["ELK","Kafka","Python"]},{"title":"ssh服务安全加固","url":"/20241121/CentOS/8cf65c38841d/","content":"ssh服务安全加固\n1.修改端口号\n\nPort 22 -&gt; Port 222\n\n2.修改连接时间\n\nLoginGraceTime 2m -&gt; 20s\n\n3.禁止root直接登录\n\nPermintRootLogin yes -&gt; no\n\n4.密码验证(可选)\n\nPasswordAuthentication yes\n\n5.空密码登录(默认为no不允许)\n\nPerminEmptyPasswords no\n\n6.增加登录用户告警信息\n\n/etc/motd文件内\n\n7.增加密码强度\n\n8.只用密钥登录，禁止密码登录\n\n\nssh-copy-id -i IP地址\n\n9.安装fail2ban服务\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"typora破解激活","url":"/20241121/typora/2eb31aeebf0c/","content":"typora破解激活(仅供学习)Typora一款 Markdown 编辑器和阅读器\n风格极简 &#x2F; 多种主题 &#x2F; 支持 macOS，Windows 及 Linux实时预览 &#x2F; 图片与文字 &#x2F; 代码块 &#x2F; 数学公式 &#x2F; 图表目录大纲 &#x2F; 文件管理 &#x2F; 导入与导出 ……\n1、下载typora下载地址\n2、破解typora安装后\n将百度云盘下载的app.asar.txt 文件中的后缀.txt去掉，并拷贝到typora安装路径下替换\n我的路径是：D:\\Typora\\resources 根据自己的安装路径进行替换\n提示：若文件名中未展示扩展名(.txt)，请打开计算机，查看，勾选文件扩展名，使文件显示扩展名\n3、输入序列号激活1、打开 typora ，点击“输入序列号”：\n2、邮箱一栏中任意填写（但须保证邮箱地址格式正确），输入序列号(在key.txt文件中，任选一条)，点击“激活”。\n3、出现已激活即可\n\nThe End\n\n","categories":["typora"],"tags":["typora"]},{"title":"velero定期备份Kubernetes","url":"/20241121/Kubernetes/479323f6e6b8/","content":"velero定期备份Kubernetes一.velero 简介Velero 是一个云原生的灾难恢复和迁移工具，采用 Go 语言编写，可以安全的备份、恢复和迁移Kubernetes集群资源和持久卷。Velero 是西班牙语，意思是帆船，非常符合 Kubernetes 社区的命名风格。\nVelero目前包含以下特性：\n\n支持Kubernetes集群数据备份和恢复\n支持复制当前Kubernetes集群的资源到其它Kubernetes集群\n支持复制生产环境到开发以及测试环境\n\nVelero组件一共分两部分，分别是服务端和客户端。\n\n服务端：运行在Kubernetes集群中\n客户端：运行在本地的velero命令行工具，需要在机器上已配置好kubectl及集群kubeconfig\n\nvelero使用场景\n\n灾备场景：提供备份恢复k8s集群的能力\n迁移场景：提供拷贝集群资源到其他集群的能力（复制同步开发，测试，生产环境的集群配置，简化环境配置）\n\nvelero与etcd备份区别\n\n直接备份 Etcd 是将集群的全部资源备份起来，而 Velero 可以对 Kubernetes 集群内对象级别进行备份。\n除了对 Kubernetes 集群进行整体备份外，Velero 还可以通过对 Type、Namespace、Label等对象进行分类备份或者恢复。\n\n Velero 在 Kubernetes 集群中创建了很多 CRD 以及相关的控制器，进行备份恢复等操作实质上是对相关 CRD 的操作。 \n[root@master1 ~] kubectl -n velero get crds -l component=veleroNAME                                CREATED ATbackups.velero.io                   2021-08-06T08:29:34Zbackupstoragelocations.velero.io    2021-08-06T08:29:34Zdeletebackuprequests.velero.io      2021-08-06T08:29:34Zdownloadrequests.velero.io          2021-08-06T08:29:34Zpodvolumebackups.velero.io          2021-08-06T08:29:34Zpodvolumerestores.velero.io         2021-08-06T08:29:34Zresticrepositories.velero.io        2021-08-06T08:29:34Zrestores.velero.io                  2021-08-06T08:29:34Zschedules.velero.io                 2021-08-06T08:29:34Zserverstatusrequests.velero.io      2021-08-06T08:29:35Zvolumesnapshotlocations.velero.io   2021-08-06T08:29:35Z \n\n\n\nVelero 工作原理图如下图所示，当用户执行备份命令时，备份过程说明如下：\n\n调用自定义资源 API 创建备份对象（1）。\nBackupController 控制器检测到生成的备份对象时（2）执行备份操作（3）。\n将备份的集群资源和存储卷快照上传到 Velero 的后端存储（4）和（5）。\n\n二.velero 安装velero安装分为两部分，velero CLI命令行安装及服务端安装，前提条件：\n\n准备一个生产k8s集群cluster1，及一个灾备k8s集群cluster2，当然也可以在同一个集群备份恢复\n在灾备环境部署minio对象存储，当然也可以使用公有云对象存储\n\n1.minio对象存储安装velero依赖对象存储保存备份数据，这里部署minio替代公有云对象存储。灾备环境准备一个节点部署minio对象存储，两个集群velero服务端将指向同一个对象存储的同一个bucket，灾备时 cluster1向桶内备份数据，cluster2向桶内读取备份数据进行恢复。\n 方式1： \n可以在k8s集群内直接使用yaml或官方minio operator部署，这里在集群外使用docker安装minio对象存储 ：\ndocker run -d --name minio \\  --restart always \\  -p 9000:9000 \\  -p 9001:9001 \\  -e &quot;MINIO_ROOT_USER=minio&quot; \\  -e &quot;MINIO_ROOT_PASSWORD=minio123&quot; \\  -v minio-data:/data \\  minio/minio server /data --console-address &quot;:9001&quot;\n\n\n\n方式2：(此文章为方式2安装minio服务)\n1.搭建centos中的minio服务\n下载：https://dl.min.io/server/minio/release/linux-amd64/minio\nmkdir -p /opt/minio/datachmod 777 ./miniocp minio /usr/local/bin\n\n2.直接启动,替换下列地址为自己环境地址\nexport MINIO_ROOT_USER=minioexport MINIO_ROOT_PASSWORD=minio123minio server /data/minio --address 192.168.137.14:9000  --console-address 192.168.137.14:9001 \n\n3.确认没问题后将minio后台启动注册为服务开机自启\n创建&#x2F;opt&#x2F;minio&#x2F;minio.conf配置文件,替换下列地址为自己环境地址\ncd /opt/miniocat &gt; minio.conf &lt;&lt;EOF# Volume to be used for Minio server.MINIO_VOLUMES=&quot;/opt/minio/data&quot;# Use if you want to run Minio on a custom port.MINIO_OPTS=&quot;--address 192.168.11.159:9000 --console-address 192.168.11.159:9001&quot;# Access Key of the server.MINIO_ACCESS_KEY=minio# Secret key of the server.MINIO_SECRET_KEY=minio123EOF\n\n\nMINIO_VOLUMES为存储数据路径\nMINIO_OPTS为启动参数，需要替换成自己的地址\nMINIO_ACCESS_KEY为登录用户\nMINIO_SECRET_KEY为登录密码\n\n注册minio为centos系统服务，注意下列路径要对应\nvi &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;minio.service\n[Unit]Description=MinioDocumentation=https://docs.minio.ioWants=network-online.targetAfter=network-online.targetAssertFileIsExecutable=/usr/local/bin/minio[Service]WorkingDirectory=/opt/minio/dataEnvironmentFile=-/opt/minio/minio.confExecStartPre=/bin/bash -c &quot;if [ -z \\&quot;$&#123;MINIO_VOLUMES&#125;\\&quot; ]; then echo \\&quot;Variable MINIO_VOLUMES not set in /opt/minio/minio.conf\\&quot;; exit 1; fi&quot;ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES# Let systemd restart this service alwaysRestart=always# Specifies the maximum file descriptor number that can be opened by this processLimitNOFILE=65536# Disable timeout logic and wait until process is stoppedTimeoutStopSec=infinitySendSIGKILL=no[Install]WantedBy=multi-user.target\n\n4.启动服务\nsystemctl daemon-reloadsystemctl start miniosystemctl status miniosystemctl enable minio\n\n5.启动成功后直接访问http://ip:9001\n6.登录创建名为velero的Buckets\n2.安装velero CLI客户端wget https://github.com/vmware-tanzu/velero/releases/download/v1.5.2/velero-v1.5.2-linux-amd64.tar.gztar -zxvf velero-v1.5.2-linux-amd64.tar.gzcp velero-v1.5.2-linux-amd64 /usr/local/bin/\n\n 查看velero CLI版本 \n[root@master1 velero]# velero version\n\n3.安装velero服务端 1.本地创建minio对象存储访问凭证\ncat  &gt;credentials-velero &lt;&lt;EOF[default]aws_access_key_id = minioaws_secret_access_key = minio123EOF\n\n2.使用velero CLI在生产和灾备集群安装velero，指向同一个对象存储：(需要修改最后的s3Url)\nvelero install \\     --provider aws \\     --plugins velero/velero-plugin-for-aws:v1.2.0 \\     --bucket velero \\     --secret-file ./credentials-velero \\     --use-volume-snapshots=false \\     --backup-location-config region=minio,s3ForcePathStyle=&quot;true&quot;,s3Url=http://192.168.11.159:9000\n\n本地内网环境需要导入velero的镜像包velero/velerov:1.5.2\n --use-restic参数指定使用官方默认支持的restic将持久化卷备份到对象存储，而公有云厂商提供的k8s有对应的CSI实现和velero插件支持 (此次没有使用)\n3.查看集群中创建的pods\n[root@master1 velero]# kubectl -n velero get podsNAME                      READY   STATUS    RESTARTS   AGEvelero-67fdfbdf65-7gfn5   1/1     Running   0          31m\n\n4.查看velero版本 \n[root@master1 velero]# velero versionClient:\tVersion: v1.5.2\tGit commit: e115e5a191b1fdb5d379b62a35916115e77124a4Server:\tVersion: v1.5.2\n\nvelero安装完成\n3.卸载velero服务若需要重新安装请先卸载velero\nkubectl delete namespace/velero clusterrolebinding/velerokubectl delete crds -l component=velero\n\n三.velero备份在集群备份应用及pv卷，首先创建veleroCLI中自带的示例应用：\n[root@master1 velero]# cd velero-v1.5.2-linux-amd64/examples/nginx-app[root@master1 velero]# kubectl apply -f with-pv.yaml\n\n查看创建的应用及持久卷，注意这里使用storageclass动态申请持久卷，在恢复的目标集群，建议提供相同名称的storageclass:\n[root@master1 nginx-app]# kubectl -n nginx-example get podsNAME                                READY   STATUS    RESTARTS   AGEnginx-deployment-5ccc99bffb-swvr2   2/2     Running   0          6m59s[root@product-cluster nginx-app]# kubectl -n nginx-example get pvNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   REASON   AGEpvc-fd5eee69-ca7f-4e94-a749-7d4e04dc4a7c   50Mi       RWO            Delete           Bound    nginx-example/nginx-logs   longhorn                6m54s\n\n执行备份，备份nginx-example这个命名空间下的所有资源：\nvelero backup create mybackup-01 --include-namespaces=nginx-example\n\n 查看备份百分比进度 \nvelero backup describe mybackup-01\n\n查看备份的所有资源列表\nvelero backup describe mybackup-01 --details\n\n备份完成后可以查看备份日志\nvelero backup logs mybackup-01\n\n查看备份任务最终执行状态\nvelero backup get\n\n四.velero恢复 在集群中删除一些pod或者deploy后恢复集群（删除刚刚创建的nginx）\nvelero restore create --from-backup=mybackup-01\n\n查看恢复的应用及数据\nkubectl -n nginx-example get podsNAME                                READY   STATUS    RESTARTS   AGEnginx-deployment-5ccc99bffb-swvr2   2/2     Running   0          7m10skubectl -n nginx-example get pvNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   REASON   AGEpvc-c9713a25-a3f6-4179-89df-fd6e27ea5055   5Gi        RWO            Delete           Bound    ns-panda/mysql-pv-claim    longhorn                31h\n\n五.velero常用命令velero参数\nVelero is a tool for managing disaster recovery, specifically for Kubernetescluster resources. It provides a simple, configurable, and operationally robustway to back up your application state and associated data.If you&#x27;re familiar with kubectl, Velero supports a similar model, allowing you toexecute commands such as &#x27;velero get backup&#x27; and &#x27;velero create schedule&#x27;. The sameoperations can also be performed as &#x27;velero backup get&#x27; and &#x27;velero schedule create&#x27;.Usage:  velero [command]Available Commands:  backup            Work with backups  backup-location   Work with backup storage locations  bug               Report a Velero bug  client            Velero client related commands  completion        Output shell completion code for the specified shell (bash or zsh).  create            Create velero resources  delete            Delete velero resources  describe          Describe velero resources  get               Get velero resources  help              Help about any command  install           Install Velero  plugin            Work with plugins  restic            Work with restic  restore           Work with restores  schedule          Work with schedules  snapshot-location Work with snapshot locations  version           Print the velero version and associated imageFlags:      --add_dir_header                   If true, adds the file directory to the header      --alsologtostderr                  log to standard error as well as files      --features stringArray             Comma-separated list of features to enable for this Velero process. Combines with values from $HOME/.config/velero/config.json if present  -h, --help                             help for velero      --kubeconfig string                Path to the kubeconfig file to use to talk to the Kubernetes apiserver. If unset, try the environment variable KUBECONFIG, as well as in-cluster configuration      --kubecontext string               The context to use to talk to the Kubernetes apiserver. If unset defaults to whatever your current-context is (kubectl config current-context)      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)      --log_dir string                   If non-empty, write log files in this directory      --log_file string                  If non-empty, use this log file      --log_file_max_size uint           Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)      --logtostderr                      log to standard error instead of files (default true)      --master --kubeconfig              (Deprecated: switch to --kubeconfig) The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.  -n, --namespace string                 The namespace in which Velero should operate (default &quot;velero&quot;)      --skip_headers                     If true, avoid header prefixes in the log messages      --skip_log_headers                 If true, avoid headers when opening log files      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)  -v, --v Level                          number for the log level verbosity      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered loggingUse &quot;velero [command] --help&quot; for more information about a command.\n\n备份集群下的所有资源：\nvelero backup create mybackup-01\n\n备份单个namespace下的所有资源：\nvelero backup create mybackup-01 --include-namespaces=mynamespace\n\n备份多个namespace下的所有资源：\nvelero backup create mybackup --include-namespaces=mynamespaceA,mynamespaceB\n\n创建一小时后自动删除的备份\nvelero backup create mybackup --include-namespaces=mynamespace --ttl=1h\n\n获取备份列表\nvelero backup get\n\n获取mybackup这个备份的详情信息\nvelero backup describe mybackup\n\n删除mybackup这个备份\nvelero backup delete mybackup\n\n从某一个备份上恢复，如果备份有多个命名空间也可以只恢复其中一个\nvelero restore create --from-backup=mybackup-01 --include-namespaces=mynamespace\n\n每天1点创建一个72小时删除的mynamespace备份\nvelero schedule create myschedule --schedule=&quot;0 1 * * *&quot; --ttl 72h --include-namespaces=mynamespace\n\n查看备份位置配置\nkubectl -n velero get BackupStorageLocation -o yaml\n\n\n\n六.定时检查集群并备份定时执行脚本如下\n#!/bin/sh######################################################################################### 脚本功能: 检查k8s环境是否正常，进行集群备份# 作者: liuSw# 日期: 2021-12-08# 版本: v1.0######################################################################################### 写入定时任务函数function addCron &#123;# 文件路径me=$(cd &quot;`dirname $0`&quot;;pwd)/$(basename $0)# 定时任务文件cron=/var/spool/cron/$(whoami)# 创建定时任务，每小时执行if [ ! -f $&#123;cron&#125; ];then  echo &quot;00 23 * * * $&#123;me&#125; &quot; &gt;&gt;$&#123;cron&#125; fi# 添加定时任务if [ `grep -c &quot;$(basename $0)&quot; $&#123;cron&#125;` -eq 0 ];then  echo &quot;00 23 * * * $&#123;me&#125; &quot; &gt;&gt;$&#123;cron&#125;fi&#125;# 添加定时任务打开此函数注释#addCron# 内网或者外网邮件地址MailAddr=# 监控日志地址LogFile=/tmp/backup-k8s.log# 默认不清理日志,打开此选项即每次检查清理上次日志# [ -f /tmp/k8s-temp.log ] &amp;&amp; :&gt; $&#123;LogFile&#125;# 需要备份的集群namespace,默认为全部备份nameSpaces=&quot;kube-system,kubesphere-system&quot;# 备份时间Date=`date +%Y-%m-%d-%H-%M`# 巡检集群节点是否都为Ready状态if [ `kubectl get nodes |grep -wv Ready|grep -vc NAME`  -ne 0 ];then  echo -e &quot;\\n`date +%Y&quot;-&quot;%m&quot;-&quot;%d&quot; &quot;%H&quot;:&quot;%M` [Error]  K8s Node is not Ready.&quot; &gt;&gt;$&#123;LogFile&#125;  kubectl get nodes |grep -wv Ready | grep -v NAME &gt;&gt;$&#123;LogFile&#125;  #exit  # 发送邮件  #kubectl get nodes |grep -wv Ready | grep -v NAME | mail -s &quot;Subject&quot; $&#123;MailAddr&#125;fi# 巡检k8s基础组件是否为Running状态for p in kube-apiserver- kube-controller-manager- kube-proxy- kube-scheduler- etcd-;do  if  [ `kubectl get pod -n kube-system|grep &quot;$p&quot;|grep -vc &quot;Running&quot;` -ne 0 ];then    echo -e &quot;\\n`date +%Y&quot;-&quot;%m&quot;-&quot;%d&quot; &quot;%H&quot;:&quot;%M` [Error]  K8s kube-pod is not Running.&quot; &gt;&gt;$&#123;LogFile&#125;    kubectl get pod -n kube-system|grep &quot;$p&quot; | grep -v &quot;Running&quot; &gt;&gt;$&#123;LogFile&#125;    # 发送邮件    #kubectl get pod -n kube-system|grep &quot;$p&quot;  | mail -s &quot;Subject&quot; $&#123;MailAddr&#125;  fidone# 巡检正在运行的pod是否出现问题for i in OutOfcpu Evicted Terminating Error Pening;do  if [ `kubectl get pods -A |grep -c &quot;$&#123;i&#125;&quot;` -gt 3 ];then\techo -e &quot;\\n`date +%Y&quot;-&quot;%m&quot;-&quot;%d&quot; &quot;%H&quot;:&quot;%M` [Error]  K8s pod is Error.&quot; &gt;&gt;$&#123;LogFile&#125;    kubectl get pods -A | grep &quot;$&#123;i&#125;&quot; &gt;&gt;$&#123;LogFile&#125;    # 发送邮件    #kubectl get pods -A | grep &quot;$&#123;i&#125;&quot; | mail -s &quot;Subject&quot; $&#123;MailAddr&#125;  fidone# 备份集群 每天点执行velero backup create $&#123;Date&#125; --exclude-namespaces=&quot;velero&quot; &gt;&gt;$&#123;LogFile&#125;  # 等待备份完成获取状态sleep 60  # 获取备份状态if [ `velero backup get|grep -w $&#123;Date&#125;|awk &#x27;&#123;print $2&#125;&#x27;`&quot;x&quot; == &quot;Completedx&quot; ];then  echo &quot;`date +%Y&quot;-&quot;%m&quot;-&quot;%d&quot; &quot;%H&quot;:&quot;%M` [info] velero backup Successful.&quot; &gt;&gt;$&#123;LogFile&#125;else   echo &quot;`date +%Y&quot;-&quot;%m&quot;-&quot;%d&quot; &quot;%H&quot;:&quot;%M` [error] velero backup faild!&quot; &gt;&gt;$&#123;LogFile&#125;  # 发送邮件  #echo &quot;[error] $&#123;Date&#125; velero backup faild!&quot; | mail -s &quot;Subject&quot; $&#123;MailAddr&#125;fi# 查看邮件不提示cat /var/spool/mail/root &gt;/dev/null exit\n\n\n\n参考: https://blog.csdn.net/networken/article/details/120368546\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"windows文件系统挂载Linux虚拟机","url":"/20241121/CentOS/89649cab961a/","content":"windows文件系统挂载Linux虚拟机# 配置好/etc/exports后，执行mount -t cifs -o username=admin,password=boer //192.168.3.35/Yum /mnt/yum","categories":["CentOS"],"tags":["CentOS"]},{"title":"wordPress 忘记站点地址，修改站点地址后进不去了怎么办","url":"/20241121/WordPress/b76dfe6c0e59/","content":"wordPress 忘记站点地址，修改站点地址后进不去了怎么办1.登陆mysql\n\n若是docker搭建，需先登录mysql容器\n\ndocker exec -it `docker ps|grep mysql|grep 端口号|awk &#x27;&#123;print $1&#125;&#x27;`\n\n非容器直接登录mysql即可\n\nmysql -u博客用户 -p密码\n\n2.切至wordpress\nUSE wordpress;\n\n3.如下命令查看地址\nselect * from wp_options limit 1;-- 可以看到目前的地址如下mysql&gt; select * from wp_options limit 1;+-----------+-------------+----------------------------+----------+| option_id | option_name | option_value               | autoload |+-----------+-------------+----------------------------+----------+|         1 | siteurl     | http://192.168.16.105:8080 | yes      |+-----------+-------------+----------------------------+----------+\n4.通过如下命令修改地址\nUPDATE wp_options SET option_value=&quot;http://localhost:8080&quot; WHERE option_name=&quot;siteurl&quot;;\n","categories":["WordPress"],"tags":["WordPress"]},{"title":"yum自动下载RPM包及其所有依赖的包","url":"/20241121/CentOS/c457f0b0e4d0/","content":"yum自动下载RPM包及其所有依赖的包有两种方法实现，\n\n一种是使用yum命令\n另一种是使用yumdownloader\n\n1.使用yum(推荐使用)\n要使用--downloadonly选项，需要先安装yum-plugin-downloadonly，不安装该包的话，会报下面的错误信息：Command line error: no such option: --downloadonly\n\n安装yum-plugin-downloadonly\nyum install yum-plugin-downloadonly\n\n\n然后使用下面命令下载包\nyum install --downloadonly RPM_Name\n\n\n也可以指定下载的目录，参数--downloaddir，如下：\nyum install --downloadonly --downloaddir=/usr/package RPM_Name\n\n\n2.使用yumdownloader(不推荐使用)\n安装yum-utils\n\nyum install yum-utils\n\n\n下载包\nyumdownloader RPM_Name\n\n\n为了根据所有依赖性下载软件包，我们使用 --resolve 参数：\nyumdownloader --resolve httpd\n\n\n也可以指定下载的目录，参数--destdir，如下：\nyumdownloader --resolve --destdir=/root/mypackages/ httpd\n# 或者\nyumdownloader --resolve --destdir /root/mypackages/ httpd\n\n\nYumdownload 可以下载一组相关的软件包\nyumdownloader \"@Development Tools\" --resolve --destdir /root/mypackages/\n\n","categories":["CentOS"],"tags":["CentOS"]},{"title":"zabbix安装","url":"/20241121/Zabbix/9d07b75304b2/","content":"zabbix安装#!/bin/shprocess()&#123;install_date=&quot;zabbix_install_$(date +%Y-%m-%d_%H:%M:%S).log&quot;printf &quot;########################################################################                     欢迎使用Zabbix一键部署脚本                      ##                脚本适配环境CentOS7+/Radhat7+、内存1G+               ##                更多信息请访问 https://xxshell.com                   ########################################################################&quot;while :; do echo    read -p &quot;请输入Mysql数据库root密码: &quot; Database_Password     [ -n &quot;$Database_Password&quot; ] &amp;&amp; breakdoneecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                    正在安装常用组件 请稍等~                         #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;yum clean allyum install -y vim wget tarecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                  正在关闭SElinux策略 请稍等~                        #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;setenforce 0#临时关闭SElinuxsed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/&quot; /etc/selinux/config#永久关闭SElinuxecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                  正在配置Firewall策略 请稍等~                       #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;firewall-cmd --zone=public --add-port=80/tcp --permanentfirewall-cmd --zone=public --add-port=10050/tcp --permanentfirewall-cmd --zone=public --add-port=10051/tcp --permanent firewall-cmd --reloadfirewall-cmd --zone=public --list-ports#放行TCP80、10050、10051端口echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                 正在安装Mariadb数据库 请稍等~                       #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;yum install -y mariadb-server mariadb systemctl start mariadbsystemctl enable mariadbecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#         正在安装配置PHP环境及扩展  时间较长请稍等~                  #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpmyum install -y  php70w* --skip-brokensystemctl start php-fpmsystemctl enable php-fpmecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                   正在配置Apache服务 请稍等~                        #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;systemctl start httpdsystemctl enable httpdecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                   正在创建Zabbix用户 请稍等~                        #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;groupadd zabbixuseradd zabbix -g zabbix -s /sbin/nologinecho &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                   正在编译Zabbix软件 请稍等~                        #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;yum install -y gcc  libxml2-devel libevent-devel net-snmp net-snmp-devel  curl  curl-devel php  php-bcmath  php-mbstring mariadb mariadb-devel java-1.6.0-openjdk-devel --skip-broken#安装Zabbix编译的软件包 #去官网下载编译安装的Zabbix：https://www.zabbix.com/download_sourceswget https://www.xxshell.com/download/sh/zabbix/zabbix4.4/zabbix-4.4.1.tar.gztar -xzvf zabbix-4.4.1.tar.gzcd zabbix-4.4.1./configure  \\        --prefix=/usr/local/zabbix  \\        --enable-server  \\        --enable-agent  \\        --with-mysql=/usr/bin/mysql_config   \\        --with-net-snmp  \\        --with-libcurl  \\        --with-libxml2  \\        --enable-java   make -j 2 &amp;&amp; make install #编译安装Zabbixecho $?=&quot;Zabbix编译完成&quot;echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                   正在配置Mariadb数据库 请稍等~                     #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;mysqladmin -u root password &quot;$Database_Password&quot;echo &quot;---mysqladmin -u root password &quot;$Database_Password&quot;&quot;#修改数据库密码mysql -uroot -p$Database_Password -e &quot;CREATE DATABASE zabbix CHARACTER SET utf8 COLLATE utf8_general_ci;&quot;echo $?=&quot;正在创建zabbix数据库&quot;#将创建数据的命令重定向到数据库中mysql -uroot -p$Database_Password -e &quot;use zabbix;&quot;echo $?=&quot;对zabbix数据库进行操作&quot;#将选中的命令重定向到数据库中mysql -uroot -p$Database_Password  zabbix &lt; database/mysql/schema.sqlmysql -uroot -p$Database_Password  zabbix &lt; database/mysql/images.sqlmysql -uroot -p$Database_Password  zabbix &lt; database/mysql/data.sqlecho $?=&quot;对zabbix数据库进行操作&quot;#zabbix数据库导入echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                    正在配置Zabbix软件 请稍等~                       #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;cp misc/init.d/fedora/core/* /etc/init.d/#拷贝启动文件到/etc/init.d/下echo $?=&quot;拷贝启动文件到/etc/init.d/下&quot;sed -i &quot;s#BASEDIR=/usr/local#BASEDIR=/usr/local/zabbix#&quot; /etc/init.d/zabbix_serversed -i &quot;s#BASEDIR=/usr/local#BASEDIR=/usr/local/zabbix#&quot; /etc/init.d/zabbix_agentd#编辑启动模块下echo $?=&quot;编辑启动模块&quot;sed -i &quot;s|# DBHost=localhost|DBHost=localhost|&quot; /usr/local/zabbix/etc/zabbix_server.confsed -i &quot;s|DBUser=zabbix|DBUser=root|&quot; /usr/local/zabbix/etc/zabbix_server.confsed -i &quot;s|# DBPassword=|DBPassword=$Database_Password|&quot; /usr/local/zabbix/etc/zabbix_server.conf#编辑Zabbix配置配置文件echo $?=&quot;编辑Zabbix配置配置文件&quot;/etc/init.d/zabbix_server restart/etc/init.d/zabbix_agentd restart#启动zabbix服务 systemctl restart zabbix_server systemctl restart zabbix_agentd#重启验证服务#通过”netstat -an | grep LIS“查看10050、10051端口能否正常监听，如果不能正常监听可能数据库或配置文件有问题。 systemctl enable zabbix_serversystemctl enable zabbix_agentdecho $?=&quot;配置Zabbix完成&quot;echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                    正在配置PHP.ini 请稍等~                          #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;sed -i &quot;s/post_max_size = 8M/post_max_size = 32M/&quot; /etc/php.inised -i &quot;s/max_execution_time = 30/max_execution_time = 600/&quot; /etc/php.inised -i &quot;s/max_input_time = 60/max_input_time = 600/&quot; /etc/php.inised -i &quot;s#;date.timezone =#date.timezone = Asia/Shanghai#&quot; /etc/php.ini#修改PHP配置文件echo $?=&quot;PHP.inin配置完成完成&quot;echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#               正在配置Zabbix前台文件 请稍等~                        #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;rm -rf /var/www/html/*#清空网站根目录cp -r frontends/php/* /var/www/html/#复制PHP文件到网站根目录chown -R apache:apache  /var/www/html/chmod -R 777 /var/www/html/conf/#给网站目录添加属主echo $?=&quot;网页文件拷贝完成&quot;echo &quot;#######################################################################&quot;echo &quot;#                                                                     #&quot;echo &quot;#                       正在重启服务 请稍等~                          #&quot;echo &quot;#                                                                     #&quot;echo &quot;#######################################################################&quot;systemctl restart php-fpm httpd mariadb zabbix_server zabbix_agentdecho $?=&quot;服务启动完成&quot;echo &quot;--------------------------- 安装已完成 ---------------------------&quot;echo &quot; 数据库名     :zabbix&quot;echo &quot; 数据库用户名 :root&quot;echo &quot; 数据库密码   :&quot;$Database_Passwordecho &quot; 网站目录     :/var/www/html&quot;echo &quot; Zabbix登录   :http://主机IP&quot;echo &quot; 安装日志文件 :/var/log/&quot;$install_dateecho &quot;------------------------------------------------------------------&quot;echo &quot; 如果安装有问题请反馈安装日志文件。&quot;echo &quot; 使用有问题请在这里寻求帮助:https://www.xxshell.com&quot;echo &quot; 电子邮箱:admin@xxshell.com&quot;echo &quot;------------------------------------------------------------------&quot;&#125;LOGFILE=/var/log/&quot;zabbix_install_$(date +%Y-%m-%d_%H:%M:%S).log&quot;touch $LOGFILEtail -f $LOGFILE &amp;pid=$!exec 3&gt;&amp;1exec 4&gt;&amp;2exec &amp;&gt;$LOGFILEprocessret=$?exec 1&gt;&amp;3 3&gt;&amp;-exec 2&gt;&amp;4 4&gt;&amp;-\n","categories":["Zabbix"],"tags":["Zabbix"]},{"title":"书源仓库","url":"/20250409/Epub/8df7a529f328/","content":"\n仓库地址https://www.yckceo.com/yuedu/shuyuans/index.html\n","categories":["Epub"],"tags":["Epub"]},{"title":"删除kubeshpere集群脚本","url":"/20241121/Kubernetes/e924096f754b/","content":"删除kubeshpere集群脚本#!/usr/bin/env bash# 删除kubeshpere集群脚本# set -x: Print commands and their arguments as they are executed.# set -e: Exit immediately if a command exits with a non-zero status.# set -xe# delete ks-installkubectl delete deploy ks-installer -n kubesphere-system# delete helmfor namespaces in kubesphere-system kubesphere-devops-system kubesphere-monitoring-system kubesphere-logging-system istio-system kube-federation-system kube-system openpitrix-systemdo  helm list -n $namespaces | grep -v NAME | awk &#x27;&#123;print $1&#125;&#x27; | sort -u | xargs -r -L1 helm uninstall -n $namespacesdone# delete kubesphere deploymentkubectl delete deployment -n kubesphere-system `kubectl get deployment -n kubesphere-system -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`# delete monitor statefulsetkubectl delete statefulset -n kubesphere-monitoring-system `kubectl get statefulset -n kubesphere-monitoring-system -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`# delete pvcpvcs=&quot;kubesphere-system|openpitrix-system|kubesphere-monitoring-system|kubesphere-devops-system|kubesphere-logging-system&quot;kubectl --no-headers=true get pvc --all-namespaces -o custom-columns=:metadata.namespace,:metadata.name | grep -E $pvcs | xargs -n2 kubectl delete pvc -n# delete rolebindingsdelete_role_bindings() &#123;  for rolebinding in `kubectl -n $1 get rolebindings -l iam.kubesphere.io/user-ref -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`  do    kubectl -n $1 delete rolebinding $rolebinding  done&#125;# delete rolesdelete_roles() &#123;  kubectl -n $1 delete role admin  kubectl -n $1 delete role operator  kubectl -n $1 delete role viewer  for role in `kubectl -n $1 get roles -l iam.kubesphere.io/role-template -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`  do    kubectl -n $1 delete role $role  done&#125;# remove useless labels and finalizersfor ns in `kubectl get ns -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`do  kubectl label ns $ns kubesphere.io/workspace-  kubectl label ns $ns kubesphere.io/namespace-  kubectl patch ns $ns -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null,&quot;ownerReferences&quot;:null&#125;&#125;&#x27;  delete_role_bindings $ns  delete_roles $nsdone# delete workspacesfor ws in `kubectl get workspaces -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`do  kubectl patch workspace $ws -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27; --type=mergedonekubectl delete workspaces --all# delete clustersfor cluster in `kubectl get clusters -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`do  kubectl patch cluster $cluster -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27; --type=mergedone# delete validatingwebhookconfigurationsfor webhook in ks-events-admission-validate users.iam.kubesphere.io validating-webhook-configurationdo  kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io $webhookdone# delete mutatingwebhookconfigurationsfor webhook in ks-events-admission-mutate logsidecar-injector-admission-mutate mutating-webhook-configurationdo  kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io $webhookdone# delete usersfor user in `kubectl get users -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`do  kubectl patch user $user -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;&#x27; --type=mergedonekubectl delete users --all# delete crdsfor crd in `kubectl get crds -o jsonpath=&quot;&#123;.items[*].metadata.name&#125;&quot;`do  if [[ $crd == *kubesphere.io ]]; then kubectl delete crd $crd; fidone# delete relevance nsfor ns in kubesphere-alerting-system kubesphere-controls-system kubesphere-devops-system kubesphere-logging-system kubesphere-monitoring-system openpitrix-system istio-systemdo  kubectl delete ns $nsdone \n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"小说下载器","url":"/20250409/Epub/430bbe274db8/","content":"\n小说下载器地址https://github.com/freeok/so-novel/?tab=readme-ov-file\n","categories":["Epub"],"tags":["Epub"]},{"title":"拉取harbor相关镜像","url":"/20241121/Harbor/87d34fa201b1/","content":"拉取harbor相关镜像#!/usr/bin/env bash# creator: liusw# up-date: 2020/04/08# description: pull_harbor.shimages=&quot;chartmuseum-photon:v0.9.0-v1.10.1 \\ harbor-migrator:v1.10.1 \\ redis-photon:v1.10.1 \\ clair-adapter-photon:v1.0.1-v1.10.1 \\ clair-photon:v2.1.1-v1.10.1 \\ notary-server-photon:v0.6.1-v1.10.1 \\ notary-signer-photon:v0.6.1-v1.10.1 \\ harbor-registryctl:v1.10.1 \\ registry-photon:v2.7.1-patch-2819-2553-v1.10.1 \\ nginx-photon:v1.10.1 \\ harbor-log:v1.10.1 \\ harbor-jobservice:v1.10.1 \\ harbor-core:v1.10.1 \\ harbor-portal:v1.10.1 \\ harbor-db:v1.10.1 \\ prepare:v1.10.1&quot;for image in $&#123;images&#125;; do  imagefull=registry.cn-shanghai.aliyuncs.com/leozhanggg/goharbor/$&#123;image&#125;  echo &quot;docker pull $&#123;imagefull&#125;&quot;  docker pull $&#123;imagefull&#125;  docker tag $&#123;imagefull&#125; goharbor/$&#123;image&#125;  docker rmi $&#123;imagefull&#125;done\n","categories":["Harbor"],"tags":["Harbor"]},{"title":"罪全书-蜘蛛悬疑系列","url":"/20241121/PDF/000730dc97bb/","content":"\n文件过大 请下载到本地后观看\n\n罪全书-蜘蛛悬疑系列(共6册)\n\n\n\nThe End\n\n","categories":["PDF"],"tags":["PDF"]},{"title":"脚本-k8s加入集群节点","url":"/20241121/Kubernetes/9d9e1c2ee3de/","content":"脚本-k8s加入集群节点#!/usr/bin/env bash############################################################### 脚本名称: get_token.sh# 脚本功能: 获取加入节点命令信息################################################################ 生成token#kubeadm token create|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1# 获取token#Token=$(kubeadm token list|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1)Token=$(kubeadm token create|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1)# 获取masterUrlmasterUrl=$(kubectl cluster-info|grep master |awk -F&quot;https://&quot; &#x27;&#123;print $2&#125;&#x27;)# 显示tokenecho &quot;Token=$&#123;Token&#125;&quot;if [ ! -f /etc/kubernetes/pki/ca.crt ];then  echo &quot;not find /etc/kubernetes/pki/ca.crt , Please Check!&quot;  exitfi# 生成密钥Pubkey=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;)certificate_key=$(kubeadm init phase upload-certs --upload-certs|awk &#x27;&#123;print $1&#125;&#x27;|tail -n 1)# 返回信息# 返回信息echo &quot;添加control-plane命令&quot;echo &quot;kubeadm join $&#123;masterUrl&#125; --token $&#123;Token&#125; \\\\--discovery-token-ca-cert-hash sha256:$&#123;Pubkey&#125; \\\\--control-plane --certificate-key $&#123;certificate_key&#125;&quot;  echo &quot;添加node命令&quot;  echo &quot;kubeadm join $&#123;masterUrl&#125; --token $&#123;Token&#125; \\\\--discovery-token-ca-cert-hash sha256:$&#123;Pubkey&#125;&quot;exit","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"自动化部署流程","url":"/20241121/DevOps/c70cbdc30cc3/","content":"自动化部署流程\n\nThe End\n\n","categories":["DevOps"],"tags":["DevOps"]},{"title":"酷狗音乐概念版","url":"/20250410/Android/6101d54b6913/","content":"\n酷狗音乐概念版酷狗音乐概念版下载链接\n","categories":["Android"],"tags":["Android"]},{"title":"麒麟桌面版未插网线网口不工作","url":"/20250410/Kylin/3453b9805239/","content":"\n麒麟桌面版未插网线网口不工作你需要在&#x2F;etc&#x2F;NetworkManager&#x2F;NetworkManager.conf里面的[device]设置ignore-carrier&#x3D;true，见官方文档你把[ifupdown]下面的managed改成true，NetworkManager才去管，说明有过时的&#x2F;etc&#x2F;network&#x2F;interfaces文件在，建议删掉;\n具体解决办法：将&#x2F;etc&#x2F;NetworkManager&#x2F;NetworkManager.conf中device下的配置修改或增加ignore-carrier&#x3D;true\n[device]wifi.scan-rand-mac-address=noignore-carrier=true\n\n修改完成后重启NetworkManager服务\nsystemctl  restart NetworkManager\n\n引用【Ubuntu20.04】开机未插网线网口没有IP地址\n","categories":["Kylin"],"tags":["Kylin"]}]